# Lab 3: Advanced Azure AI Evaluation - Introduction

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](<https://colab.research.google.com/github/sachin0034/hands_on_AI_introduction_to_AI_evaluations-4038348/blob/main/Lab-3%28Building_AI_evaluators%29/Advanced_Azure_AI_Evaluation.ipynb>)



---

## ğŸ§  Overview

In this lab, we will use **predefined evaluators provided by Azure** to assess the quality of AI model outputs. 

While in **Lab 1**, we manually used an LLM to act as a judge by defining our own metrics and key terms, in this lab, we will explore how Azure simplifies this process by offering **built-in evaluation tools**. These tools allow you to automatically analyze and measure different aspects of generative AI outputs such as relevance, fluency, coherence, and safety.

---

## ğŸ“˜ What You Will Learn

- âœ… How to use Azure AI services to evaluate LLMs outputs
- âœ… How to assess model quality, safety, and bias using Azure tools
- âœ… How to build an end-to-end evaluation pipeline within the Azure ecosystem

---

## ğŸ–¼ï¸ Output Preview

Here's a preview of the Azure AI Evaluation dashboard/interface you will work with in this lab:

![Azure AI Evaluation Output](images/img-1.png)
---


