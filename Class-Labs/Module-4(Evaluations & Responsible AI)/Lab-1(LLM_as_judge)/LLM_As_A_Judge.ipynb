{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0516c151",
      "metadata": {
        "id": "0516c151"
      },
      "source": [
        "# LLM-as-a-Judge Simply Explained: A Complete Guide to Run LLM Evals\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](<https://colab.research.google.com/github/sachin0034/hands_on_AI_introduction_to_AI_evaluations-4038348/blob/main/Lab-1%28LLM_as_judge%29/LLM_As_A_Judge.ipynb>)\n",
        "\n",
        "\"LLM-as-a-judge\" is a technique where large language models — like GPT — are used to evaluate the quality of outputs generated by other AI models or systems. Instead of relying on human evaluators, which can be time-consuming and expensive, we use an LLM to act as the judge, scoring or ranking generated content based on factors like correctness, coherence, relevance, or even tone and style.\n",
        "\n",
        "This approach became popular because evaluating open-ended text (like summaries, chatbot replies, or creative writing) is inherently subjective. Traditional metrics like accuracy or BLEU scores often fall short since there’s no single 'right' answer. LLMs help fill that gap by providing nuanced judgments, often closer to how a human would interpret or assess the output.\n",
        "\n",
        "So in essence, LLM-as-a-judge is a scalable, cost-effective, and surprisingly reliable way to evaluate the quality of language model outputs — especially when human evaluation isn’t feasible at scale.\n",
        "\n",
        "---\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before you get started, please make sure you have the following ready:\n",
        "\n",
        "### 1. Sample Contract File for Testing\n",
        "\n",
        "To try out the contract analysis workflow, download the sample contract file provided below:\n",
        "\n",
        "- [Download Sample Contract (Google Drive)](https://drive.google.com/file/d/1E557kdNBZ5cDUvVDLNrEVRuKcRSYDG3Z/view?usp=sharing)\n",
        "\n",
        "### 2. OpenAI API Key\n",
        "\n",
        "You’ll need your own OpenAI API key to access the language models used for contract evaluation. If you don’t have one yet, follow this step-by-step guide to generate your API key:\n",
        "\n",
        "- [How to get your own OpenAI API key (Medium article)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61733e35",
      "metadata": {
        "id": "61733e35"
      },
      "source": [
        "## Step 1: Install the Dependencies\n",
        "Each dependency serves a specific purpose in the LLM Judge Lab:\n",
        "\n",
        "| Package        | Purpose / Use in Project                                                                                     |\n",
        "|----------------|--------------------------------------------------------------------------------------------------------------|\n",
        "| **gradio**     | Builds a web-based UI for interaction. Allows users to input text, upload files, and view model evaluations. |\n",
        "| **langchain**  | Manages the logic of LLM interactions — from document processing to chaining LLM calls.                      |\n",
        "| **openai**     | Connects the system to OpenAI’s models (e.g., GPT-4) for generating judgments or scores.                     |\n",
        "| **python-docx**| Parses and extracts content from `.docx` files for evaluation.                                               |\n",
        "| **PyPDF2**     | Extracts text from PDFs, enabling the model to assess uploaded PDF documents.                                |\n",
        "| **pandas**     | Structures and displays results in tables or dataframes for better analysis and comparison.                  |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78af67c4",
      "metadata": {
        "id": "78af67c4"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "! pip install gradio langchain openai python-docx PyPDF pandas langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2ca2b6a",
      "metadata": {
        "id": "c2ca2b6a"
      },
      "source": [
        "## Step 2 : Import Dependencies\n",
        "\n",
        "Now that you've installed all the necessary libraries, it's time to import them into your Python script or Jupyter notebook.\n",
        "\n",
        "- Start by importing **Gradio** to build the interactive web interface for your LLM-as-a-judge lab.\n",
        "\n",
        "- Next, bring in **document loaders from LangChain** — specifically for handling PDF, DOCX, and plain text files. These will help you extract content from user-uploaded documents.\n",
        "\n",
        "- Then, import the **OpenAI client**, which you'll use to connect to models like GPT-4 for analyzing and judging text.\n",
        "\n",
        "- You’ll also want **Pandas** to organize and display results in table formats, especially when dealing with comparisons or scores.\n",
        "\n",
        "- Finally, include Python’s built-in **os** and **tempfile** modules. These are useful for file path handling and safely working with temporary files during processing.\n",
        "\n",
        "Once these imports are in place, you're ready to move on to building the file processing and evaluation pipeline!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da24358",
      "metadata": {
        "id": "3da24358"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
        "from langchain.schema import Document\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "import tempfile\n",
        "import re\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "252947bb",
      "metadata": {
        "id": "252947bb"
      },
      "source": [
        "## Step 3: Key Terms and Evaluation Metrics\n",
        "\n",
        "### Here we are setting our key terms and evaluation metrics.Key terms are the specific pieces of information we need to extract from the contract and evaluation metrics are the criteria on which we will evaluate the llm responses.\n",
        "\n",
        "### Key Terms\n",
        "\n",
        "These are the critical contract clauses we want the LLM to extract and analyze:\n",
        "\n",
        "- **Product Name**  \n",
        "- **Limitation of Liability In Months**  \n",
        "- **Governing Law**  \n",
        "\n",
        "Each of these helps focus the LLM’s attention on high-priority legal elements.\n",
        "\n",
        "---\n",
        "```\n",
        "| **Category** | **Metric**                                                  | **What It Measures**                                             |\n",
        "|--------------|-------------------------------------------------------------|------------------------------------------------------------------|\n",
        "| **Helpful**  | Was the information extracted as per the question asked?    | Did the answer directly address the key term?                    |\n",
        "| **Helpful**  | Was the information complete?                               | Is all relevant information included?                            |\n",
        "| **Helpful**  | Was the information enough to make a conclusive decision?   | Is the answer sufficient for decision-making?                    |\n",
        "| **Helpful**  | Were associated red flags covered in the extracted output?  | Are potential issues or risks mentioned?                         |\n",
        "| **Honest**   | Was the information extracted from all relevant clauses?    | Are multiple relevant sections included if needed?               |\n",
        "| **Honest**   | Was the page number of extracted information correct?       | Are page references accurate?                                    |\n",
        "| **Honest**   | Was the AI reasoning discussing the relevant clause?        | Is the explanation focused on the right part?                    |\n",
        "| **Honest**   | Does the information stay within document scope?            | Is the answer limited to the uploaded contract?                  |\n",
        "| **Harmless** | Were results free from misleading claims?                   | Are there any false or misleading statements?                    |\n",
        "| **Harmless** | Does the tool avoid generic/non-contract answers?           | Is the answer specific to the contract, not generic?             |\n",
        "| **Harmless** | Did the AI avoid illegal or insensitive justifications?     | Are explanations appropriate and lawful?                         |\n",
        "| **Harmless** | Did the tool prevent false claims about people/entities?    | Are there any incorrect statements about parties?                |\n",
        "| **Harmless** | Did the tool context hateful/profane content?               | Is the output free from inappropriate language?                  |\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e9726237",
      "metadata": {
        "id": "e9726237"
      },
      "outputs": [],
      "source": [
        "KEY_TERMS = [\n",
        "    \"Product Name\",\n",
        "    \"Limitation of Liability In Months\",\n",
        "    \"Governing Law\"\n",
        "]\n",
        "\n",
        "EVALUATION_METRICS = [\n",
        "    \"Was the information extracted as per the question asked in the key term?\",\n",
        "    \"Was the information complete?\",\n",
        "    \"Was the information enough to make a conclusive decision?\",\n",
        "    \"Was the AI reasoning discussing the relevant clause?\",\n",
        "    \"Does the information stay within document scope?\",\n",
        "    \"Were results free from misleading claims?\",\n",
        "    \"Does the tool avoid generic/non-contract answers?\",\n",
        "    \"Did the tool prevent false claims about people/entities?\"\n",
        "]\n",
        "\n",
        "# Additional evaluation metrics you can use just add them to the above Evaluation metrics:\n",
        "# - Were associated red flags covered in the extracted output?\n",
        "# - Was the information extracted from all relevant clauses?\n",
        "# - Was the page number of extracted information correct?\n",
        "# - Did the AI avoid illegal or insensitive justifications?\n",
        "# - Did the tool context hateful/profane content?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c53a6a43",
      "metadata": {
        "id": "c53a6a43"
      },
      "source": [
        "## Step 4: Extract Text from Documents\n",
        "\n",
        "This step ensures that all files—no matter the format—are converted into a **standardized format** for the LLM to analyze.  \n",
        "It keeps the pipeline consistent, reliable, and ready for downstream tasks like key term extraction or clause classification.\n",
        "\n",
        "### What File Types Are Supported?\n",
        "\n",
        "```\n",
        "| File Type | Extensions     | Extracted Using      \n",
        "|-----------|----------------|-----------------------\n",
        "| 📄 PDF    | `.pdf`         | `PyPDFLoader`         \n",
        "| 📝 Word   | `.docx`, `.doc`| `Docx2txtLoader`      \n",
        "| 📃 Text   | `.txt`         | `TextLoader`          \n",
        "| 📊 CSV    | `.csv`         | `pandas`              \n",
        "\n",
        "```\n",
        "### What Does the Function Return?\n",
        "\n",
        "- `text`: Complete raw text from the document  \n",
        "- `docs`: Structured content, including page-wise segmentation (useful for referencing clauses by page)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7afea5dd",
      "metadata": {
        "id": "7afea5dd"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_file(file_path):\n",
        "    # Get the file extension and convert it to lowercase\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    # Handle PDF files\n",
        "    if ext == \".pdf\":\n",
        "        loader = PyPDFLoader(file_path)  # Use PyPDFLoader to read PDF\n",
        "        docs = loader.load()  # Load document into LangChain Document objects\n",
        "        text = \"\\n\".join([doc.page_content for doc in docs])  # Combine all page content\n",
        "\n",
        "    # Handle Word documents (.docx, .doc)\n",
        "    elif ext in [\".docx\", \".doc\"]:\n",
        "        loader = Docx2txtLoader(file_path)  # Use Docx2txtLoader for Word files\n",
        "        docs = loader.load()\n",
        "        text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Handle plain text files\n",
        "    elif ext in [\".txt\"]:\n",
        "        loader = TextLoader(file_path)  # Use TextLoader for .txt files\n",
        "        docs = loader.load()\n",
        "        text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Handle CSV files\n",
        "    elif ext == \".csv\":\n",
        "        df = pd.read_csv(file_path)  # Read CSV using pandas\n",
        "        text = df.to_string(index=False)  # Convert DataFrame to plain string\n",
        "        # Wrap in a dummy doc-like object to keep consistent structure\n",
        "        docs = [type('Doc', (object,), {'page_content': text})()]\n",
        "\n",
        "    # Unsupported file types\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type\")\n",
        "\n",
        "    # Return both raw text and structured docs for further processing\n",
        "    return text, docs  # docs may include page-level details\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63eb97e5",
      "metadata": {
        "id": "63eb97e5"
      },
      "source": [
        "## Step 5: Set Up OpenAI Client\n",
        "\n",
        "To use GPT models, you’ll need to set up access using your **OpenAI API key**.\n",
        "\n",
        "### Requirements\n",
        "\n",
        "- You **must** have a valid OpenAI API key to proceed.\n",
        "- **Keep your API key secure** — never commit it to public repositories or share it.\n",
        "\n",
        "### How to Get an API Key\n",
        "\n",
        "> **Don’t have one yet?**  \n",
        "> Follow this simple guide to create your own API key:  \n",
        "> 👉 [How to get your own OpenAI API key (Medium article)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
        "\n",
        "Once you have the key, you can place your key here in the code :\n",
        "\n",
        "```python\n",
        "api_key = '<Insert Your API Key>'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "730cfdf1",
      "metadata": {
        "collapsed": true,
        "id": "730cfdf1"
      },
      "outputs": [],
      "source": [
        "api_key = '<Insert Your API Key>'\n",
        "\n",
        "try:\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    # Minimal API call to check if the key is valid\n",
        "    client.models.list()\n",
        "    print(\"✅ OpenAI API key is valid.\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Invalid OpenAI API key or connection error:\", e)\n",
        "    raise RuntimeError(\"OpenAI API key check failed. Please provide a valid key.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca46ca6",
      "metadata": {
        "id": "2ca46ca6"
      },
      "source": [
        "## Step 6: Extract Key Terms from the Document\n",
        "\n",
        "The **`extract_key_terms(text, key_terms)`** function is used to pull out and summarize the most important legal clauses from your uploaded document.\n",
        "\n",
        "It takes two inputs:\n",
        "\n",
        "- `text`: The full contract content, extracted earlier using `extract_text_from_file()`\n",
        "- `key_terms`: A list of specific legal terms you defined earlier (e.g. \"Payment Terms\", \"Governing Law\")\n",
        "\n",
        "---\n",
        "\n",
        "### How the Function Works:\n",
        "\n",
        "Once the document is uploaded and text is extracted, this function:\n",
        "\n",
        "1. Loops through each key term from the list.\n",
        "2. For each term:\n",
        "   - It sends a carefully crafted prompt (along with the full text) to the OpenAI model.\n",
        "   - It asks the model to find the key terms, provide the value , and return it in a structured **JSON format**.\n",
        "3. Then, it:\n",
        "   - Parses the JSON response from the model.\n",
        "   - Extracts the **value** and **page number** (if mentioned).\n",
        "   - Stores the result in a dictionary.\n",
        "\n",
        "---\n",
        "\n",
        "✅ This helps transform lengthy legal documents into clear, structured insights you can quickly review and evaluate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35324c05",
      "metadata": {
        "id": "35324c05"
      },
      "outputs": [],
      "source": [
        "def extract_key_terms(text, key_terms):\n",
        "    import json\n",
        "    import re\n",
        "\n",
        "    def safe_json_parse(response_text):\n",
        "        \"\"\"Safely parse JSON from LLM response with fallback strategies.\"\"\"\n",
        "        # Strategy 1: Try direct JSON parsing\n",
        "        try:\n",
        "            return json.loads(response_text.strip())\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "        # Strategy 2: Extract from code blocks\n",
        "        json_patterns = [\n",
        "            r'```json\\s*(.*?)\\s*```',\n",
        "            r'```\\s*(.*?)\\s*```',\n",
        "            r'\\{.*\\}'\n",
        "        ]\n",
        "\n",
        "        for pattern in json_patterns:\n",
        "            json_match = re.search(pattern, response_text, re.DOTALL)\n",
        "            if json_match:\n",
        "                try:\n",
        "                    return json.loads(json_match.group(1).strip() if 'json' in pattern else json_match.group(0))\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "        # Fallback: return default structure\n",
        "        return {\"Value\": \"Not found\", \"Page Number\": None, \"Section\": None}\n",
        "\n",
        "    results = {}\n",
        "    for term in key_terms:\n",
        "        prompt = (\n",
        "            f\"Act as a legal expert. From this contract text, extract the value for '{term}'.\\n\\n\"\n",
        "            f\"Contract Text: {text}\\n\\n\"\n",
        "            f\"Instructions:\\n\"\n",
        "            f\"1. Provide a one-word answer for '{term}' if found\\n\"\n",
        "            f\"2. Include page number if available\\n\"\n",
        "            f\"3. If not found, use 'Not found'\\n\\n\"\n",
        "            f\"Return ONLY valid JSON in this exact format:\\n\"\n",
        "            f'{{\"Value\": \"your_answer\", \"Page Number\": \"page_number_or_null\", \"Section\": \"section_name\"}}'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",  # Fixed model name\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a legal contract analysis assistant. Always return valid JSON only.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.1  # Lower temperature for consistency\n",
        "            )\n",
        "            answer = completion.choices[0].message.content\n",
        "            print(\"****************LLM Answer*******************\")\n",
        "            print(answer)\n",
        "            print(\"*********************************************\")\n",
        "\n",
        "            # Use safe JSON parsing\n",
        "            parsed_response = safe_json_parse(answer)\n",
        "            print(f\"DEBUG: Parsed JSON successfully: {parsed_response}\")\n",
        "\n",
        "            value = parsed_response.get(\"Value\", \"Not found\")\n",
        "            print(f\"DEBUG: Extracted value: {value}\")\n",
        "\n",
        "            # Extract page number with multiple fallback options\n",
        "            page_number = (\n",
        "                parsed_response.get(\"Page Number\") or\n",
        "                parsed_response.get(\"PageNumber\") or\n",
        "                parsed_response.get(\"page_number\")\n",
        "            )\n",
        "            print(f\"DEBUG: Direct page number extraction: {page_number}\")\n",
        "\n",
        "            # If not found, try extracting from Section field\n",
        "            if not page_number:\n",
        "                section = parsed_response.get(\"Section\", \"\")\n",
        "                print(f\"DEBUG: Section field content: {section}\")\n",
        "                if section and \"Page\" in str(section):\n",
        "                    page_match = re.search(r'Page (\\d+)', str(section))\n",
        "                    if page_match:\n",
        "                        page_number = page_match.group(1)\n",
        "                        print(f\"DEBUG: Extracted page number from section: {page_number}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: API call or parsing failed: {e}\")\n",
        "            value = \"Error\"\n",
        "            page_number = None\n",
        "\n",
        "        final_result = {\"Value\": value, \"page_number\": page_number}\n",
        "        print(f\"DEBUG: Final result for term '{term}': {final_result}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        results[term] = final_result\n",
        "\n",
        "    print(f\"DEBUG: All results: {results}\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a418c07c",
      "metadata": {
        "id": "a418c07c"
      },
      "source": [
        "## Step 7: Judge the LLM's Response\n",
        "\n",
        "The `judge_llm()` function is responsible for **evaluating the quality of each extracted answer** provided by the LLM for the key terms.\n",
        "\n",
        "---\n",
        "\n",
        "### Inputs\n",
        "\n",
        "- `text`: The contract text (already extracted earlier)\n",
        "- `key_terms`: The list of legal terms you’re looking for\n",
        "- `extract_key_terms_response`: The previous step’s output (values + page numbers)\n",
        "- `metrics`: A list of evaluation metrics.\n",
        "\n",
        "---\n",
        "\n",
        "### How the Evaluation Works\n",
        "\n",
        "1. Loops through each key term.\n",
        "2. For each term and each evaluation metric:\n",
        "   - Sends a prompt to GPT to **judge the quality** of the LLM's extracted answer.\n",
        "   - GPT responds with:\n",
        "     - A **score between 0 to 5**\n",
        "     - A brief **justification**\n",
        "3. The score is interpreted as:\n",
        "   - **Score ≥ 3** → ✅ `LLM_Judge_Response = True` (Pass)\n",
        "   - **Score < 2** → ❌ `LLM_Judge_Response = False` (Fail)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7abb7fde",
      "metadata": {
        "id": "7abb7fde"
      },
      "outputs": [],
      "source": [
        "def judge_llm(key_terms, extract_key_terms_response, metrics):\n",
        "    results = []  # List to store evaluation results for each key term\n",
        "\n",
        "    print(f\"DEBUG: Starting evaluation for {len(key_terms)} key terms and {len(metrics)} metrics\")\n",
        "    print(f\"DEBUG: Key terms: {key_terms}\")\n",
        "    print(f\"DEBUG: Metrics: {metrics}\")\n",
        "    print(f\"DEBUG: Extract key terms response: {extract_key_terms_response}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    for term in key_terms:\n",
        "        print(f\"\\nDEBUG: Processing term: '{term}'\")\n",
        "\n",
        "        # Get the LLM's extracted answer for the current key term\n",
        "        llm_answer = extract_key_terms_response.get(term, {}).get(\"Value\", \"Not found\")\n",
        "        page_number = extract_key_terms_response.get(term, {}).get(\"page_number\", None)\n",
        "\n",
        "        print(f\"DEBUG: LLM Answer: {llm_answer}\")\n",
        "        print(f\"DEBUG: Page Number: {page_number}\")\n",
        "\n",
        "        for metric in metrics:\n",
        "            print(f\"\\n  DEBUG: Evaluating metric: '{metric}'\")\n",
        "\n",
        "            # Construct the evaluation prompt to ask the LLM to judge its own answer\n",
        "            prompt = (\n",
        "            f\"You are an expert contract lawyer. Evaluate the extracted answer for the key term '{term}' using the evaluation metrics provided.\\n\\n\"\n",
        "\n",
        "            f\"KEY TERM: {term}\\n\"\n",
        "            f\"EVALUATION METRICS:\\n{metrics}\\n\\n\"\n",
        "            f\"EXTRACTED ANSWER:\\n{llm_answer}\\n\\n\"\n",
        "\n",
        "            \"INSTRUCTIONS:\\n\"\n",
        "            \"- Check if the answer clearly addresses the key term and meets the evaluation metrics.\\n\"\n",
        "            \"- Assign a score from 0 to 5 using the criteria below:\\n\\n\"\n",
        "            \"- Provide a short justification why you score this metric a particular score.\\n\"\n",
        "\n",
        "            \"SCORING GUIDE:\\n\"\n",
        "            \"Score 0 : Key term not addressed at all.\\n\"\n",
        "            \"Score 1 : Answer is irrelevant or empty.\\n\"\n",
        "            \"Score 2 : Some relevant info, but fails to meet metrics or is incomplete.\\n\"\n",
        "            \"Score 3 : Adequate answer, meets around half of the metrics with acceptable accuracy.\\n\"\n",
        "            \"Score 4 : Strong answer, meets most metrics with good clarity and detail.\\n\"\n",
        "            \"Score 5 : Excellent answer, complete, accurate, and meets nearly all metrics with clear legal context.\\n\\n\"\n",
        "\n",
        "            \"RESPONSE FORMAT:\\n\"\n",
        "            \"Score: <number>\\n\"\n",
        "            \"Justification: <text>\\n\"\n",
        "        )\n",
        "            print(f\"  DEBUG: Prompt length: {len(prompt)} characters\")\n",
        "\n",
        "            # Call the LLM to get its evaluation response\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"gpt-4.1-nano\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a contract evaluation expert.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            content = completion.choices[0].message.content\n",
        "            print(\"  *********JUDGE EVALUATION ANSWER*********\")\n",
        "            print(content)\n",
        "            print(\"  *****************************************\")\n",
        "\n",
        "            import re  # Use regex to extract structured score and justification\n",
        "\n",
        "            # Extract score from the response\n",
        "            score_match = re.search(r\"Score:\\s*(\\d+)\", content)\n",
        "            score = int(score_match.group(1)) if score_match else None\n",
        "            print(f\"  DEBUG: Extracted score: {score}\")\n",
        "\n",
        "            if not score_match:\n",
        "                print(f\"  DEBUG: Score regex didn't match. Raw content: {repr(content)}\")\n",
        "\n",
        "            # Extract justification from the response\n",
        "            justification_match = re.search(r\"Justification:\\s*(.*)\", content, re.DOTALL)\n",
        "            justification = justification_match.group(1).strip() if justification_match else content\n",
        "            print(f\"  DEBUG: Extracted justification: {justification[:100]}...\")  # Show first 100 chars\n",
        "\n",
        "            if not justification_match:\n",
        "                print(f\"  DEBUG: Justification regex didn't match. Using full content as justification\")\n",
        "\n",
        "            # Determine pass/fail status based on score threshold\n",
        "            pass_fail = True if score is not None and score >= 3 else False\n",
        "            print(f\"  DEBUG: Pass/Fail (score >= 3): {pass_fail}\")\n",
        "\n",
        "            # Create result entry\n",
        "            result_entry = {\n",
        "                \"key_term_name\": term,\n",
        "                \"llm_extracted_ans_from_doc\": llm_answer,\n",
        "                \"page_number\": page_number,\n",
        "                \"evulation_metric_name\": metric,\n",
        "                \"LLM_Judge_Response\": pass_fail,\n",
        "                \"justification\": justification\n",
        "            }\n",
        "\n",
        "            print(f\"  DEBUG: Result entry: {result_entry}\")\n",
        "\n",
        "            # Append results for this term + metric\n",
        "            results.append(result_entry)\n",
        "            print(f\"  DEBUG: Added result. Total results so far: {len(results)}\")\n",
        "\n",
        "        print(f\"DEBUG: Completed all metrics for term '{term}'\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    print(f\"\\nDEBUG: Final results count: {len(results)}\")\n",
        "    print(f\"DEBUG: Sample result: {results[0] if results else 'No results'}\")\n",
        "    return results  # Return all evaluation results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebeeb4ba",
      "metadata": {
        "id": "ebeeb4ba"
      },
      "source": [
        "## Function: process_documents_with_progress\n",
        "\n",
        "Runs the full contract analysis pipeline with real-time progress using Gradio.\n",
        "\n",
        "---\n",
        "\n",
        "### Overview\n",
        "\n",
        "- Extracts text from a contract file.\n",
        "- Identifies key terms using `extract_key_terms()`.\n",
        "- Evaluates responses with `judge_llm()` against defined metrics.\n",
        "- Returns structured results as DataFrames.\n",
        "\n",
        "---\n",
        "\n",
        "### Parameters\n",
        "\n",
        "- `contract_file`: Uploaded contract file (PDF, DOCX, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### Flow Summary\n",
        "\n",
        "1. Extract text from the document.\n",
        "2. Pull key terms using the LLM.\n",
        "3. Score results against evaluation metrics.\n",
        "4. Organize and return results in DataFrames.\n",
        "\n",
        "---\n",
        "> ## **Note**  \n",
        "> - Relies on `extract_text_from_file`, `extract_key_terms`, and `judge_llm`.  \n",
        "> - Uses predefined key terms and evaluation metrics for analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a7cdc85",
      "metadata": {
        "id": "9a7cdc85"
      },
      "outputs": [],
      "source": [
        "def process_documents_with_progress(contract_file, progress=gr.Progress()):\n",
        "    \"\"\"\n",
        "    Process documents with progress updates\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Extract text from contract file\n",
        "        progress(0.1, desc=\"📄 Extracting text from contract file...\")\n",
        "        text, docs = extract_text_from_file(contract_file)\n",
        "        progress(0.2, desc=\"✅ Text extraction completed\")\n",
        "\n",
        "        # Step 2: Extract key terms\n",
        "        progress(0.3, desc=\"🔍 Extracting key terms from contract...\")\n",
        "        key_term_results = extract_key_terms(text, KEY_TERMS)\n",
        "        progress(0.5, desc=\"✅ Key terms extraction completed\")\n",
        "\n",
        "        # Step 3: Judge each key term\n",
        "        progress(0.6, desc=\"⚖️ Evaluating key terms with LLM judge...\")\n",
        "        evals = judge_llm(\n",
        "            KEY_TERMS,\n",
        "            key_term_results,\n",
        "            EVALUATION_METRICS\n",
        "        )\n",
        "        progress(0.8, desc=\"✅ Evaluation completed\")\n",
        "\n",
        "        # Step 4: Format results\n",
        "        progress(0.9, desc=\"📊 Formatting results for display...\")\n",
        "\n",
        "        # Format each evaluation result for display\n",
        "        for e in evals:\n",
        "            term = e[\"key_term_name\"]\n",
        "            llm_ans = e[\"llm_extracted_ans_from_doc\"]\n",
        "            # Extract only the text after 'Text:'\n",
        "            if llm_ans:\n",
        "                text_match = re.search(r'Text:\\s*(.*)', llm_ans, re.DOTALL)\n",
        "                e[\"llm_extracted_ans_from_doc\"] = text_match.group(1).strip() if text_match else llm_ans\n",
        "\n",
        "        # Prepare DataFrame with new columns in the correct order\n",
        "        df = pd.DataFrame(evals)\n",
        "        display_cols = [\n",
        "            \"key_term_name\",\n",
        "            \"llm_extracted_ans_from_doc\",\n",
        "            \"page_number\",\n",
        "            \"evulation_metric_name\",\n",
        "            \"LLM_Judge_Response\",\n",
        "            \"justification\"\n",
        "        ]\n",
        "        df = df[display_cols]\n",
        "\n",
        "        # Split DataFrame into three based on metric index\n",
        "        metric_groups = [EVALUATION_METRICS[:3], EVALUATION_METRICS[3:6], EVALUATION_METRICS[6:8]]\n",
        "        df1 = df[df[\"evulation_metric_name\"].isin(metric_groups[0])].reset_index(drop=True)\n",
        "        df2 = df[df[\"evulation_metric_name\"].isin(metric_groups[1])].reset_index(drop=True)\n",
        "        df3 = df[df[\"evulation_metric_name\"].isin(metric_groups[2])].reset_index(drop=True)\n",
        "\n",
        "        progress(1.0, desc=\"🎉 Processing completed successfully!\")\n",
        "\n",
        "        return text, df1, df2, df3, df\n",
        "\n",
        "    except Exception as e:\n",
        "        progress(1.0, desc=f\"❌ Error occurred: {str(e)}\")\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "240b4303",
      "metadata": {
        "id": "240b4303"
      },
      "source": [
        "## Gradio UI: LLM Contract Judge\n",
        "\n",
        "A simple web interface to upload contracts, extract key terms, evaluate them with an LLM, and view or download the results.\n",
        "\n",
        "---\n",
        "\n",
        "### What It Does\n",
        "\n",
        "- Accepts contract files (PDF, DOCX, TXT)\n",
        "- Extracts and evaluates key legal terms using GPT\n",
        "- Displays results grouped by evaluation metrics\n",
        "- Allows easy CSV export of final outputs\n",
        "\n",
        "---\n",
        "\n",
        "### Inputs\n",
        "\n",
        "- `contract_file`: Upload contract document\n",
        "- `start_btn`: Starts the evaluation process\n",
        "- `download_btn`: Exports all results as a CSV\n",
        "\n",
        "---\n",
        "\n",
        "### Flow Summary\n",
        "\n",
        "1. User uploads contract and clicks **Start Evaluating**\n",
        "2. `run_and_return_tables()` calls `process_documents_with_progress()`\n",
        "3. Extracted results appear in 3 separate tabs\n",
        "4. User clicks **Download** to export all results as CSV\n",
        "\n",
        "---\n",
        "\n",
        "> ## **Note:**  \n",
        "> - When you run the Gradio app, you'll see:  \n",
        ">   `* Running on local URL: http://127.0.0.1:7868`\n",
        "> - Click the URL to open the Gradio UI in a new browser tab.\n",
        "> - After processing, click the **Download CSV** button.\n",
        "> - A temporary file named `temp.csv` will be generated for download.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "024957e8",
      "metadata": {
        "id": "024957e8"
      },
      "outputs": [],
      "source": [
        "# Create the main Gradio interface using Blocks\n",
        "with gr.Blocks() as demo:\n",
        "    # Title Markdown\n",
        "    gr.Markdown(\"# 📄 LLM Contract Judge\\nUpload a contract, extract key terms, and evaluate with LLM.\")\n",
        "\n",
        "    # File upload component in a horizontal row layout\n",
        "    with gr.Row():\n",
        "        contract_file = gr.File(label=\"Upload Contract (PDF, DOCX, TXT)\")\n",
        "\n",
        "    # Button to trigger the evaluation process\n",
        "    start_btn = gr.Button(\"🚀 Start Evaluating\", variant=\"primary\")\n",
        "\n",
        "    # A non-editable textbox to show progress or status updates\n",
        "    progress_text = gr.Textbox(\n",
        "        label=\"Processing Status\",\n",
        "        value=\"Ready to start evaluation...\",\n",
        "        interactive=False\n",
        "    )\n",
        "\n",
        "    # Output box to display raw extracted contract text\n",
        "    extracted_text = gr.Textbox(label=\"Extracted Contract Text\", lines=10, interactive=False)\n",
        "\n",
        "    # Tabbed interface for displaying different metric evaluation results\n",
        "    with gr.Tabs():\n",
        "        # Helpful Metrics tab\n",
        "        with gr.TabItem(\"Helpful Metrics\"):\n",
        "            results_table1 = gr.Dataframe(headers=[\n",
        "                \"key_term_name\",\n",
        "                \"llm_extracted_ans_from_doc\",\n",
        "                \"page_number\",\n",
        "                \"evulation_metric_name\",\n",
        "                \"LLM_Judge_Response\",\n",
        "                \"justification\"\n",
        "            ], label=\"Evaluation Results (Helpful Metrics)\")\n",
        "\n",
        "        # Honest Metrics tab\n",
        "        with gr.TabItem(\"Honest Metrics\"):\n",
        "            results_table2 = gr.Dataframe(headers=[\n",
        "                \"key_term_name\",\n",
        "                \"llm_extracted_ans_from_doc\",\n",
        "                \"page_number\",\n",
        "                \"evulation_metric_name\",\n",
        "                \"LLM_Judge_Response\",\n",
        "                \"justification\"\n",
        "            ], label=\"Evaluation Results (Honest Metrics)\")\n",
        "\n",
        "        # Harmless Metrics tab\n",
        "        with gr.TabItem(\"Harmless Metrics\"):\n",
        "            results_table3 = gr.Dataframe(headers=[\n",
        "                \"key_term_name\",\n",
        "                \"llm_extracted_ans_from_doc\",\n",
        "                \"page_number\",\n",
        "                \"evulation_metric_name\",\n",
        "                \"LLM_Judge_Response\",\n",
        "                \"justification\"\n",
        "            ], label=\"Evaluation Results (Harmless Metrics)\")\n",
        "\n",
        "    # Button to download evaluation results as a CSV\n",
        "    download_btn = gr.Button(\"📥 Download All Results as CSV\")\n",
        "\n",
        "    # File component to show the downloadable CSV file\n",
        "    download_file = gr.File(label=\"Download CSV\")\n",
        "\n",
        "    # State variables to store DataFrames for use during download\n",
        "    state_df1 = gr.State()\n",
        "    state_df2 = gr.State()\n",
        "    state_df3 = gr.State()\n",
        "    state_df_all = gr.State()\n",
        "\n",
        "    # Main function to process contract and return data for all tables\n",
        "    def run_and_return_tables(contract_file, progress=gr.Progress()):\n",
        "        if not contract_file:\n",
        "            # If file not uploaded, return error message and clear outputs\n",
        "            return (\n",
        "                \"Please upload a contract file first.\",\n",
        "                gr.update(value=None),\n",
        "                gr.update(value=None),\n",
        "                gr.update(value=None),\n",
        "                gr.update(value=None),\n",
        "                None, None, None, None\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            # Update UI to show progress\n",
        "            progress_text = \"🔄 Starting document processing...\"\n",
        "\n",
        "            # Function processes the document and returns the text and 3 metrics tables\n",
        "            text, df1, df2, df3, df_all = process_documents_with_progress(contract_file, progress)\n",
        "\n",
        "            return (\n",
        "                text,\n",
        "                gr.update(value=df1),\n",
        "                gr.update(value=df2),\n",
        "                gr.update(value=df3),\n",
        "                df1, df2, df3, df_all\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            # Return error details in case of failure\n",
        "            error_msg = f\"❌ Error during processing: {str(e)}\"\n",
        "            return (\n",
        "                error_msg,\n",
        "                gr.update(value=None),\n",
        "                gr.update(value=None),\n",
        "                gr.update(value=None),\n",
        "                gr.update(value=None),\n",
        "                None, None, None, None\n",
        "            )\n",
        "\n",
        "    # Function to generate and return the downloadable CSV file from results\n",
        "    def download_csv(contract_file, df_all):\n",
        "        if df_all is None:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Define the columns we need from the original data\n",
        "            required_cols = [\n",
        "                \"key_term_name\",\n",
        "                \"llm_extracted_ans_from_doc\",\n",
        "                \"page_number\",\n",
        "                \"evulation_metric_name\",\n",
        "                \"LLM_Judge_Response\",\n",
        "                \"justification\"\n",
        "            ]\n",
        "\n",
        "            # Filter to only include the specified columns\n",
        "            df_filtered = df_all[required_cols].copy()\n",
        "\n",
        "            # Create the pivot table to transform data from long to wide format\n",
        "            # Each key_term will become a row, and each metric will become a column\n",
        "            pivot_df = df_filtered.pivot_table(\n",
        "                index=['key_term_name', 'llm_extracted_ans_from_doc', 'page_number'],\n",
        "                columns='evulation_metric_name',\n",
        "                values='LLM_Judge_Response',\n",
        "                aggfunc='first'  # Take first value if there are duplicates\n",
        "            ).reset_index()\n",
        "\n",
        "            # Flatten the column names (remove multi-level index)\n",
        "            pivot_df.columns.name = None\n",
        "\n",
        "            # Rename the basic columns to match your desired format\n",
        "            pivot_df = pivot_df.rename(columns={\n",
        "                'key_term_name': 'key_term_name',\n",
        "                'llm_extracted_ans_from_doc': 'value',\n",
        "                'page_number': 'page_number'\n",
        "            })\n",
        "\n",
        "            # Create a justification column by combining all justifications for each key term\n",
        "            justification_df = df_filtered.groupby(['key_term_name', 'llm_extracted_ans_from_doc', 'page_number'])['justification'].apply(\n",
        "                lambda x: ' | '.join(x.dropna().unique())  # Combine unique justifications with separator\n",
        "            ).reset_index()\n",
        "\n",
        "            # Merge the justification back to the pivot table\n",
        "            final_df = pivot_df.merge(\n",
        "                justification_df,\n",
        "                left_on=['key_term_name', 'value', 'page_number'],\n",
        "                right_on=['key_term_name', 'llm_extracted_ans_from_doc', 'page_number'],\n",
        "                how='left'\n",
        "            )\n",
        "\n",
        "            # Drop the duplicate column from merge\n",
        "            if 'llm_extracted_ans_from_doc' in final_df.columns:\n",
        "                final_df = final_df.drop('llm_extracted_ans_from_doc', axis=1)\n",
        "\n",
        "            # Reorder columns: basic info first, then metrics, then justification\n",
        "            basic_cols = ['key_term_name', 'value', 'page_number']\n",
        "            metric_cols = [col for col in final_df.columns if col not in basic_cols + ['justification']]\n",
        "            column_order = basic_cols + metric_cols + ['justification']\n",
        "\n",
        "            final_df = final_df[column_order]\n",
        "\n",
        "            # Write to a temporary file and return the path\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\", mode=\"w\", encoding=\"utf-8\") as tmp:\n",
        "                final_df.to_csv(tmp, index=False)\n",
        "                tmp_path = tmp.name\n",
        "\n",
        "            return tmp_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in download_csv: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    # Trigger processing function when 'Start Evaluating' is clicked\n",
        "    start_btn.click(\n",
        "        run_and_return_tables,\n",
        "        inputs=[contract_file],\n",
        "        outputs=[extracted_text, results_table1, results_table2, results_table3, state_df1, state_df2, state_df3, state_df_all]\n",
        "    )\n",
        "\n",
        "    # Trigger CSV download when 'Download' is clicked\n",
        "    download_btn.click(\n",
        "        download_csv,\n",
        "        inputs=[contract_file, state_df_all],\n",
        "        outputs=download_file\n",
        "    )\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
