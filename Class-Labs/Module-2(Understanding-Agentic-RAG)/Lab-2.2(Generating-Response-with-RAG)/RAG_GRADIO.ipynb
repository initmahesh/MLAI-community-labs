{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHYTrBEuso_I"
      },
      "source": [
        "# üöÄ RAG Lab: Build Your Own Retrieval Augmented Generation System\n",
        "\n",
        "## What is RAG?\n",
        "Retrieval Augmented Generation (RAG) is a technique that combines:\n",
        "- **Retrieval**: Finding relevant information from a knowledge base\n",
        "- **Generation**: Using an AI model to generate responses based on retrieved context\n",
        "\n",
        "## What You'll Learn:\n",
        "1. How to chunk documents for better retrieval\n",
        "2. Store document embeddings in ChromaDB\n",
        "3. Retrieve relevant chunks based on user queries\n",
        "4. Generate contextual responses using OpenAI\n",
        "5. Build an interactive interface with Gradio\n",
        "\n",
        "## Architecture:\n",
        "Upload File ‚Üí Chunk Text ‚Üí Create Embeddings ‚Üí Store in ChromaDB ‚Üí Query ‚Üí Retrieve ‚Üí Generate Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gdown\n",
        "from IPython.display import Image\n",
        "\n",
        "# URL format to download from Google Drive\n",
        "file_id = \"1v9eDyAFqlUv-7_SRdpVp9iOeNMEmBcxs\"\n",
        "url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "# Download the image\n",
        "gdown.download(url, \"image.png\", quiet=False)\n",
        "\n",
        "# Display the image\n",
        "Image(\"image.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALrkyjvMtCH_"
      },
      "source": [
        "\n",
        "# CELL 2: Install Required Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Qgm8vXbbskvt",
        "outputId": "d7d4f601-cd54-4b09-9282-678f8f6a5a7e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Installing all necessary packages for our RAG system\n",
        "\"\"\"\n",
        "! pip install gradio chromadb openai tiktoken python-docx PyPDF2 sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebYHQ_ehtNX1"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrB50nH7tM_V"
      },
      "source": [
        "\n",
        "# CELL 3: Import Libraries\n",
        "\n",
        "\n",
        "- `import gradio as gr`  \n",
        "  *Builds interactive web-based user interfaces for ML apps.*\n",
        "\n",
        "- `import chromadb`  \n",
        "  *Vector database for storing and querying embeddings.*\n",
        "\n",
        "- `import openai`  \n",
        "  *Accesses OpenAI models and APIs.*\n",
        "\n",
        "- `import os`  \n",
        "  *Interacts with the operating system (files, paths, env vars).*\n",
        "\n",
        "- `import io`  \n",
        "  *Handles streams and in-memory files.*\n",
        "\n",
        "- `import json`  \n",
        "  *Parses and generates JSON data.*\n",
        "\n",
        "- `from typing import List, Dict, Tuple`  \n",
        "  *Provides type hints for lists, dictionaries, and tuples.*\n",
        "\n",
        "- `import tiktoken`  \n",
        "  *Tokenizes text, often used with OpenAI models.*\n",
        "\n",
        "- `from sentence_transformers import SentenceTransformer`  \n",
        "  *Generates sentence embeddings with pre-trained models.*\n",
        "\n",
        "- `import PyPDF2`  \n",
        "  *Reads and manipulates PDF files.*\n",
        "\n",
        "- `import docx`  \n",
        "  *Reads and writes Microsoft Word (.docx) files.*\n",
        "\n",
        "- `import re`  \n",
        "  *Performs pattern matching and text manipulation with regular expressions.*\n",
        "\n",
        "- `from datetime import datetime`  \n",
        "  *Works with dates and times.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS427DqztIZx",
        "outputId": "893fd992-55fe-4f48-afe4-fd2855a2bd06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Importing all required libraries\n",
        "\"\"\"\n",
        "import gradio as gr\n",
        "import chromadb\n",
        "import openai\n",
        "import os\n",
        "import io\n",
        "import json\n",
        "from typing import List, Dict, Tuple\n",
        "import tiktoken\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import PyPDF2\n",
        "import docx\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU8Y3_lbtZD1"
      },
      "source": [
        "\n",
        "\n",
        "## Cell 4 - Configuration Settings for the RAG System\n",
        "\n",
        "This section defines and explains the main settings used to configure your Retrieval-Augmented Generation (RAG) system.\n",
        "\n",
        "---\n",
        "\n",
        "### Configuration Dictionary\n",
        "\n",
        "- **chunk_size**: The number of characters in each text chunk. This controls how much text is processed at a time.\n",
        "- **chunk_overlap**: The number of characters that overlap between consecutive chunks to ensure context is maintained.\n",
        "- **max_chunks_to_retrieve**: The maximum number of relevant text chunks to retrieve for answering each query.\n",
        "- **embedding_model**: The name of the sentence transformer model used to convert text into embeddings (numerical vector representations).\n",
        "- **openai_model**: The specific OpenAI model (such as GPT-4o) used for generating responses.\n",
        "- **collection_name**: The name of the collection where your documents are stored in the vector database.\n",
        "\n",
        "---\n",
        "\n",
        "### Embedding Model Initialization\n",
        "\n",
        "- Loads the specified sentence transformer embedding model (e.g., `all-MiniLM-L6-v2`).\n",
        "- This model is used to convert text into embeddings, which are essential for semantic search and retrieval in the RAG pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "### OpenAI API Key Setup\n",
        "\n",
        "- Stores your OpenAI API key, which is required to access OpenAI's language models.\n",
        "- Checks if the API key is set and provides feedback:\n",
        "  - If the key is present, it confirms successful loading.\n",
        "  - If the key is missing, it prompts you to set it and provides instructions.\n",
        "\n",
        "> # **Note:**  \n",
        "> # Make sure to replace the placeholder with your actual OpenAI API key.  \n",
        "> # You can also set the key as an environment variable using:  \n",
        "> # 'OPENAI_API_KEY' = 'your-api-key-here'`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEj7VGWVtWjk",
        "outputId": "a2b36e5f-107e-4b91-9d58-62fc61a9ba35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OpenAI API key loaded from environment\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Configuration settings for our RAG system\n",
        "\"\"\"\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"chunk_size\": 500,\n",
        "    \"chunk_overlap\": 50,\n",
        "    \"max_chunks_to_retrieve\": 5,\n",
        "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
        "    \"openai_model\": \"gpt-4o\",\n",
        "    \"collection_name\": \"rag_documents\"\n",
        "}\n",
        "\n",
        "# Initialize the sentence transformer model for embeddings\n",
        "def load_embedding_model():\n",
        "    return SentenceTransformer(CONFIG[\"embedding_model\"])\n",
        "\n",
        "embedding_model = load_embedding_model()\n",
        "\n",
        "# Note: You'll need to set your OpenAI API key\n",
        "# Either set it as an environment variable or replace the line below\n",
        "OPENAI_API_KEY = 'your-api-key-here'\n",
        "if OPENAI_API_KEY:\n",
        "    print(\"‚úÖ OpenAI API key loaded from environment\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Please set your OPENAI_API_KEY environment variable\")\n",
        "    print(\"You can do this by running: os.environ['OPENAI_API_KEY'] = 'your-api-key-here'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Euf6r-auB64"
      },
      "source": [
        "## Cell 5 - Functions to Extract Text from Different File Formats\n",
        "\n",
        "This section describes the logic and purpose of functions used to extract text from various document formats, making your RAG system flexible and file-type agnostic.\n",
        "\n",
        "---\n",
        "\n",
        "### PDF Extraction\n",
        "\n",
        "- Extracts text from PDF files by reading each page and concatenating the text.\n",
        "- Handles errors gracefully and notifies if extraction fails.\n",
        "\n",
        "---\n",
        "\n",
        "### DOCX Extraction\n",
        "\n",
        "- Extracts text from Microsoft Word (.docx) files by reading each paragraph and joining them.\n",
        "- Handles errors and prints a message if extraction fails.\n",
        "\n",
        "---\n",
        "\n",
        "### TXT Extraction\n",
        "\n",
        "- Reads plain text files (.txt) and returns their content.\n",
        "- Handles errors and notifies if extraction fails.\n",
        "\n",
        "---\n",
        "\n",
        "### General File Extraction\n",
        "\n",
        "- Determines file type based on the file extension.\n",
        "- Calls the appropriate extraction function for PDF, DOCX, or TXT files.\n",
        "- Returns a helpful message if the file format is unsupported.\n",
        "\n",
        "---\n",
        "\n",
        "> **Note:**  \n",
        "> These functions help your system automatically process and extract text from uploaded files, regardless of whether they're PDFs, Word documents, or plain text files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uPb_NqZytv5o"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Functions to extract text from different file formats\n",
        "\"\"\"\n",
        "def extract_text_from_pdf(file_path: str) -> str:\n",
        "    \"\"\"Extract text from PDF file\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF: {e}\")\n",
        "    return text\n",
        "\n",
        "def extract_text_from_docx(file_path: str) -> str:\n",
        "    \"\"\"Extract text from DOCX file\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        doc = docx.Document(file_path)\n",
        "        for paragraph in doc.paragraphs:\n",
        "            text += paragraph.text + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading DOCX: {e}\")\n",
        "    return text\n",
        "\n",
        "def extract_text_from_txt(file_path: str) -> str:\n",
        "    \"\"\"Extract text from TXT file\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return file.read()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading TXT: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_file(file_path: str) -> str:\n",
        "    \"\"\"Extract text based on file extension\"\"\"\n",
        "    file_extension = file_path.lower().split('.')[-1]\n",
        "\n",
        "    if file_extension == 'pdf':\n",
        "        return extract_text_from_pdf(file_path)\n",
        "    elif file_extension == 'docx':\n",
        "        return extract_text_from_docx(file_path)\n",
        "    elif file_extension == 'txt':\n",
        "        return extract_text_from_txt(file_path)\n",
        "    else:\n",
        "        return \"Unsupported file format. Please upload PDF, DOCX, or TXT files.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8-Y3zUauH0m"
      },
      "source": [
        "## Cell 6 - Functions to Split Text into Chunks for Better Retrieval\n",
        "\n",
        "This section explains the logic and purpose of functions that prepare long texts for efficient retrieval by splitting them into manageable, overlapping chunks.\n",
        "\n",
        "---\n",
        "\n",
        "### Text Cleaning\n",
        "\n",
        "- **Purpose:**  \n",
        "  Cleans and normalizes the input text by removing extra whitespace and trimming leading/trailing spaces.\n",
        "- **Benefit:**  \n",
        "  Ensures that the text is tidy and consistent before further processing.\n",
        "\n",
        "---\n",
        "\n",
        "### Splitting Text into Overlapping Chunks\n",
        "\n",
        "- **Purpose:**  \n",
        "  Splits the cleaned text into overlapping chunks based on a specified chunk size and overlap.\n",
        "- **How it works:**  \n",
        "  - The text is split into words.\n",
        "  - Chunks are created by moving a sliding window across the words, with each chunk overlapping the previous one by a set number of words.\n",
        "  - Each chunk is stored with metadata: the chunk text, a unique chunk ID, word count, and character count.\n",
        "- **Benefit:**  \n",
        "  Overlapping chunks help maintain context between segments, which improves retrieval accuracy in downstream tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### Chunk Statistics\n",
        "\n",
        "- **Purpose:**  \n",
        "  Calculates and returns useful statistics about the generated chunks, such as:\n",
        "  - Total number of chunks\n",
        "  - Average words and characters per chunk\n",
        "  - Minimum and maximum words per chunk\n",
        "\n",
        "---\n",
        "\n",
        "> **Note:**  \n",
        "> Chunking large texts into smaller, overlapping segments is a best practice for retrieval-augmented systems, as it improves both search relevance and model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I5_Mbij_uEf3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Functions to split text into chunks for better retrieval\n",
        "\"\"\"\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Clean and normalize text\"\"\"\n",
        "    # Remove extra whitespace and normalize\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def split_text_into_chunks(text: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:\n",
        "    \"\"\"Split text into overlapping chunks\"\"\"\n",
        "    text = clean_text(text)\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk_words = words[i:i + chunk_size]\n",
        "        chunk_text = ' '.join(chunk_words)\n",
        "\n",
        "        chunk_info = {\n",
        "            \"text\": chunk_text,\n",
        "            \"chunk_id\": f\"chunk_{len(chunks)}\",\n",
        "            \"word_count\": len(chunk_words),\n",
        "            \"char_count\": len(chunk_text)\n",
        "        }\n",
        "        chunks.append(chunk_info)\n",
        "\n",
        "        # Break if we've processed all words\n",
        "        if i + chunk_size >= len(words):\n",
        "            break\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def get_chunk_statistics(chunks: List[Dict]) -> Dict:\n",
        "    \"\"\"Get statistics about the chunks\"\"\"\n",
        "    if not chunks:\n",
        "        return {}\n",
        "\n",
        "    word_counts = [chunk[\"word_count\"] for chunk in chunks]\n",
        "    char_counts = [chunk[\"char_count\"] for chunk in chunks]\n",
        "\n",
        "    return {\n",
        "        \"total_chunks\": len(chunks),\n",
        "        \"avg_words_per_chunk\": sum(word_counts) / len(word_counts),\n",
        "        \"avg_chars_per_chunk\": sum(char_counts) / len(char_counts),\n",
        "        \"min_words\": min(word_counts),\n",
        "        \"max_words\": max(word_counts)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfcOKdbsuWNB"
      },
      "source": [
        "## Cell 7 - ChromaDB Vector Database Setup\n",
        "\n",
        "This section outlines the class and methods used to manage your vector database (ChromaDB) for storing and retrieving document embeddings in a RAG system.\n",
        "\n",
        "---\n",
        "\n",
        "### RAGVectorStore Class\n",
        "\n",
        "#### Initialization\n",
        "\n",
        "- **Purpose:**  \n",
        "  Sets up a ChromaDB client and initializes a collection for storing document chunks and their embeddings.\n",
        "- **Features:**  \n",
        "  - Deletes any existing collection with the same name for a fresh start.\n",
        "  - Creates a new collection with metadata describing its purpose.\n",
        "  - Prints a success or error message based on the outcome.\n",
        "\n",
        "#### Adding Chunks\n",
        "\n",
        "- **Purpose:**  \n",
        "  Adds processed text chunks and their corresponding embeddings to the vector store.\n",
        "- **Features:**  \n",
        "  - Prepares data (IDs, documents, metadata) for insertion.\n",
        "  - Stores each chunk with metadata such as word count, character count, and timestamp.\n",
        "  - Prints how many chunks were added or an error message if the process fails.\n",
        "\n",
        "#### Searching for Similar Chunks\n",
        "\n",
        "- **Purpose:**  \n",
        "  Searches the vector store for chunks most similar to a given query embedding.\n",
        "- **Features:**  \n",
        "  - Returns the top N most relevant chunks, along with their metadata and similarity distances.\n",
        "  - Handles errors gracefully and returns empty results if the search fails.\n",
        "\n",
        "#### Collection Information\n",
        "\n",
        "- **Purpose:**  \n",
        "  Provides information about the current collection.\n",
        "- **Features:**  \n",
        "  - Returns the collection's name, the number of stored documents, and its status (active or empty).\n",
        "  - Handles errors and reports the status accordingly.\n",
        "\n",
        "---\n",
        "\n",
        "### Usage\n",
        "\n",
        "- The `RAGVectorStore` class is initialized with the configured collection name, setting up your vector database for immediate use in retrieval-augmented workflows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pimUXkTLuTNS",
        "outputId": "b1055a9c-e330-44ef-d67b-52831e10cb65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ChromaDB collection 'rag_documents' created successfully\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "ChromaDB vector database setup and management\n",
        "\"\"\"\n",
        "class RAGVectorStore:\n",
        "    def __init__(self, collection_name: str = \"rag_documents\"):\n",
        "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
        "        try:\n",
        "            self.client = chromadb.Client()\n",
        "            self.collection_name = collection_name\n",
        "\n",
        "            # Delete existing collection if it exists (for fresh start)\n",
        "            try:\n",
        "                self.client.delete_collection(collection_name)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Create new collection\n",
        "            self.collection = self.client.create_collection(\n",
        "                name=collection_name,\n",
        "                metadata={\"description\": \"RAG document chunks\"}\n",
        "            )\n",
        "            print(f\"‚úÖ ChromaDB collection '{collection_name}' created successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error initializing ChromaDB: {e}\")\n",
        "            self.collection = None\n",
        "\n",
        "    def add_chunks(self, chunks: List[Dict], embeddings: List[List[float]]) -> bool:\n",
        "        \"\"\"Add chunks and their embeddings to the vector store\"\"\"\n",
        "        if not self.collection:\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Prepare data for ChromaDB\n",
        "            ids = [chunk[\"chunk_id\"] for chunk in chunks]\n",
        "            documents = [chunk[\"text\"] for chunk in chunks]\n",
        "            metadatas = [\n",
        "                {\n",
        "                    \"word_count\": chunk[\"word_count\"],\n",
        "                    \"char_count\": chunk[\"char_count\"],\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                }\n",
        "                for chunk in chunks\n",
        "            ]\n",
        "\n",
        "            # Add to collection\n",
        "            self.collection.add(\n",
        "                embeddings=embeddings,\n",
        "                documents=documents,\n",
        "                metadatas=metadatas,\n",
        "                ids=ids\n",
        "            )\n",
        "\n",
        "            print(f\"‚úÖ Added {len(chunks)} chunks to vector store\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error adding chunks to vector store: {e}\")\n",
        "            return False\n",
        "\n",
        "    def search(self, query: str, query_embedding: List[float], n_results: int = 5) -> Dict:\n",
        "        \"\"\"Search for similar chunks\"\"\"\n",
        "        if not self.collection:\n",
        "            return {\"chunks\": [], \"distances\": []}\n",
        "\n",
        "        try:\n",
        "            results = self.collection.query(\n",
        "                query_embeddings=[query_embedding],\n",
        "                n_results=n_results\n",
        "            )\n",
        "\n",
        "            # Format results\n",
        "            chunks = []\n",
        "            for i, doc in enumerate(results['documents'][0]):\n",
        "                chunk_info = {\n",
        "                    \"text\": doc,\n",
        "                    \"metadata\": results['metadatas'][0][i],\n",
        "                    \"distance\": results['distances'][0][i],\n",
        "                    \"id\": results['ids'][0][i]\n",
        "                }\n",
        "                chunks.append(chunk_info)\n",
        "\n",
        "            return {\n",
        "                \"chunks\": chunks,\n",
        "                \"distances\": results['distances'][0]\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error searching vector store: {e}\")\n",
        "            return {\"chunks\": [], \"distances\": []}\n",
        "\n",
        "    def get_collection_info(self) -> Dict:\n",
        "        \"\"\"Get information about the collection\"\"\"\n",
        "        if not self.collection:\n",
        "            return {}\n",
        "\n",
        "        try:\n",
        "            count = self.collection.count()\n",
        "            return {\n",
        "                \"name\": self.collection_name,\n",
        "                \"document_count\": count,\n",
        "                \"status\": \"active\" if count > 0 else \"empty\"\n",
        "            }\n",
        "        except:\n",
        "            return {\"status\": \"error\"}\n",
        "\n",
        "# Initialize vector store\n",
        "vector_store = RAGVectorStore(CONFIG[\"collection_name\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjoKPh2nueVq"
      },
      "source": [
        "## Cell 8 - Functions to Generate Embeddings for Text Chunks\n",
        "\n",
        "This section explains the functions responsible for converting text or text chunks into numerical vector representations (embeddings), which are essential for semantic search and retrieval in a RAG system.\n",
        "\n",
        "---\n",
        "\n",
        "### Batch Embedding Generation\n",
        "\n",
        "- **Purpose:**  \n",
        "  Generates embeddings for a list of text chunks at once.\n",
        "- **How it works:**  \n",
        "  - Uses the preloaded sentence transformer model to encode each text chunk.\n",
        "  - Returns a list of embeddings, where each embedding is a list of floating-point numbers.\n",
        "- **Benefit:**  \n",
        "  Efficiently processes multiple chunks in a single call, speeding up large-scale document processing.\n",
        "\n",
        "---\n",
        "\n",
        "### Single Embedding Generation\n",
        "\n",
        "- **Purpose:**  \n",
        "  Generates an embedding for a single text input.\n",
        "- **How it works:**  \n",
        "  - Encodes the input text using the same sentence transformer model.\n",
        "  - Returns the embedding as a list of floating-point numbers.\n",
        "- **Benefit:**  \n",
        "  Useful for generating an embedding for a user query or a single document segment.\n",
        "\n",
        "---\n",
        "\n",
        "> **Note:**  \n",
        "> Embeddings capture the semantic meaning of text, allowing for effective similarity search and retrieval in your vector database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KZgZtymRucaR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Functions to generate embeddings for text chunks\n",
        "\"\"\"\n",
        "def generate_embeddings(texts: List[str]) -> List[List[float]]:\n",
        "    \"\"\"Generate embeddings for a list of texts\"\"\"\n",
        "    try:\n",
        "        embeddings = embedding_model.encode(texts, convert_to_tensor=False)\n",
        "        return embeddings.tolist()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating embeddings: {e}\")\n",
        "        return []\n",
        "\n",
        "def generate_single_embedding(text: str) -> List[float]:\n",
        "    \"\"\"Generate embedding for a single text\"\"\"\n",
        "    try:\n",
        "        embedding = embedding_model.encode([text], convert_to_tensor=False)\n",
        "        return embedding[0].tolist()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating single embedding: {e}\")\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkXVB_H0umm0"
      },
      "source": [
        "## Cell 9 - OpenAI Integration for Response\n",
        "\n",
        "This section describes how the system integrates with OpenAI's Chat Completions API to generate answers based on retrieved context chunks.\n",
        "\n",
        "---\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- Generates a comprehensive, context-aware response to a user's query by leveraging the power of OpenAI's advanced language models (such as GPT-4o).\n",
        "\n",
        "---\n",
        "\n",
        "### How It Works\n",
        "\n",
        "- **API Key Check:**  \n",
        "  Ensures that the OpenAI API key is set before proceeding. If not, it returns an error message prompting the user to configure the key.\n",
        "- **Context Preparation:**  \n",
        "  Combines the retrieved context chunks into a formatted string, clearly labeling each chunk for reference.\n",
        "- **Prompt Construction:**  \n",
        "  - **System Prompt:** Instructs the model to act as a helpful assistant, use the provided context, and cite which chunks were used in the answer.\n",
        "  - **User Prompt:** Presents the combined context and the user's question, asking for a comprehensive answer.\n",
        "- **OpenAI Client Initialization:**  \n",
        "  Initializes the OpenAI client with the provided API key.\n",
        "- **API Call:**  \n",
        "  Sends the constructed prompt to the Chat Completions API, specifying the model, message structure, maximum tokens, and temperature for response creativity.\n",
        "- **Response Extraction:**  \n",
        "  Extracts and returns the generated answer from the API's response.\n",
        "\n",
        "---\n",
        "\n",
        "> **Note:**  \n",
        "> Make sure your OpenAI API key is set correctly to enable response generation. This integration allows your RAG system to deliver accurate, context-based answers by combining retrieval and generation capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PLlE-br5ukgi"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "OpenAI integration for response generation using Chat Completions API\n",
        "\"\"\"\n",
        "from openai import OpenAI\n",
        "\n",
        "def generate_response(query: str, context_chunks: List[Dict], model: str = \"gpt-4o\") -> str:\n",
        "    \"\"\"Generate response using OpenAI Chat Completions API with retrieved context.\"\"\"\n",
        "\n",
        "    if not OPENAI_API_KEY:\n",
        "        return \"‚ùå OpenAI API key not set. Please configure your API key.\"\n",
        "\n",
        "    try:\n",
        "        # Prepare context from retrieved chunks\n",
        "        context = \"\\n\\n\".join([f\"Chunk {i+1}:\\n{chunk['text']}\" for i, chunk in enumerate(context_chunks)])\n",
        "\n",
        "        # Create the prompt\n",
        "        system_prompt = (\n",
        "            \"You are a helpful AI assistant that answers questions based on the provided context. \"\n",
        "            \"Use the context information to provide accurate and relevant answers. \"\n",
        "            \"If the context doesn't contain enough information to answer the question, say so clearly. \"\n",
        "            \"Always cite which chunks you used in your response.\"\n",
        "        )\n",
        "\n",
        "        user_prompt = f\"\"\"Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Please provide a comprehensive answer based on the context above.\"\"\"\n",
        "\n",
        "        # Initialize OpenAI client\n",
        "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "        # Call the Chat Completions API (current standard API)\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,  # e.g., \"gpt-4o\", \"gpt-4\", \"gpt-3.5-turbo\"\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            max_tokens=1000,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        # Extract and return the model's answer\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error generating response: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdeHOBXvuvOD"
      },
      "source": [
        "## Cell 10 - Main RAG Pipeline\n",
        "\n",
        "This section describes the core pipeline class that ties together all components of the Retrieval-Augmented Generation (RAG) system, handling document processing, storage, retrieval, and response generation.\n",
        "\n",
        "---\n",
        "\n",
        "### RAGPipeline Class\n",
        "\n",
        "#### Initialization\n",
        "\n",
        "- **Purpose:**  \n",
        "  Sets up the pipeline with access to the vector store and initializes containers for document chunks and statistics.\n",
        "\n",
        "---\n",
        "\n",
        "#### Document Processing\n",
        "\n",
        "- **Purpose:**  \n",
        "  Handles the entire workflow for uploading and processing a document.\n",
        "- **Steps:**\n",
        "  1. **Extracts text** from the uploaded file using the appropriate extraction function.\n",
        "  2. **Splits the text into chunks** using the configured chunk size and overlap.\n",
        "  3. **Generates embeddings** for each chunk.\n",
        "  4. **Stores the chunks and embeddings** in the ChromaDB vector database.\n",
        "  5. **Calculates and stores chunk statistics** for reporting and optimization.\n",
        "- **Feedback:**  \n",
        "  Returns a success status, a detailed status message with statistics, and the chunk stats. If any step fails, it returns an error message.\n",
        "\n",
        "---\n",
        "\n",
        "#### Query Processing\n",
        "\n",
        "- **Purpose:**  \n",
        "  Handles user queries by retrieving relevant information from the stored document chunks and generating a context-aware response.\n",
        "- **Steps:**\n",
        "  1. **Generates an embedding** for the user's question.\n",
        "  2. **Searches the vector store** for the most relevant chunks using the query embedding.\n",
        "  3. **Generates a response** using the OpenAI model, providing the retrieved chunks as context.\n",
        "  4. **Formats the retrieved chunks** for display, including similarity scores and brief previews.\n",
        "  5. **Creates a summary** of the retrieved chunks for transparency.\n",
        "- **Feedback:**  \n",
        "  Returns the generated answer, a list of retrieved chunk details, and a formatted summary. Handles errors gracefully and provides clear messages if any step fails.\n",
        "\n",
        "---\n",
        "\n",
        "### Usage\n",
        "\n",
        "- The `RAGPipeline` class is instantiated and ready for use, enabling seamless document ingestion and intelligent, context-driven question answering.\n",
        "\n",
        "---\n",
        "\n",
        "> **Note:**  \n",
        "> This pipeline is the heart of your RAG system, ensuring smooth coordination between extraction, chunking, embedding, storage, retrieval, and generation for robust, explainable AI responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "z7EvAWatutJ1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Main RAG pipeline that orchestrates the entire process\n",
        "\"\"\"\n",
        "class RAGPipeline:\n",
        "    def __init__(self):\n",
        "        self.vector_store = vector_store\n",
        "        self.chunks = []\n",
        "        self.chunk_stats = {}\n",
        "\n",
        "    def process_document(self, file_path: str) -> Tuple[bool, str, Dict]:\n",
        "        \"\"\"Process uploaded document and store in vector database\"\"\"\n",
        "        try:\n",
        "            # Extract text\n",
        "            text = extract_text_from_file(file_path)\n",
        "            if not text or text.startswith(\"Unsupported\"):\n",
        "                return False, text, {}\n",
        "\n",
        "            # Create chunks\n",
        "            self.chunks = split_text_into_chunks(\n",
        "                text,\n",
        "                CONFIG[\"chunk_size\"],\n",
        "                CONFIG[\"chunk_overlap\"]\n",
        "            )\n",
        "\n",
        "            if not self.chunks:\n",
        "                return False, \"No chunks created from document\", {}\n",
        "\n",
        "            # Generate embeddings\n",
        "            chunk_texts = [chunk[\"text\"] for chunk in self.chunks]\n",
        "            embeddings = generate_embeddings(chunk_texts)\n",
        "\n",
        "            if not embeddings:\n",
        "                return False, \"Failed to generate embeddings\", {}\n",
        "\n",
        "            # Store in vector database\n",
        "            success = self.vector_store.add_chunks(self.chunks, embeddings)\n",
        "\n",
        "            if not success:\n",
        "                return False, \"Failed to store chunks in vector database\", {}\n",
        "\n",
        "            # Get statistics\n",
        "            self.chunk_stats = get_chunk_statistics(self.chunks)\n",
        "\n",
        "            status_message = f\"\"\"‚úÖ Document processed successfully!\n",
        "\n",
        "üìä **Processing Statistics:**\n",
        "- Total chunks created: {self.chunk_stats['total_chunks']}\n",
        "- Average words per chunk: {self.chunk_stats['avg_words_per_chunk']:.1f}\n",
        "- Average characters per chunk: {self.chunk_stats['avg_chars_per_chunk']:.1f}\n",
        "- Word count range: {self.chunk_stats['min_words']} - {self.chunk_stats['max_words']} words\n",
        "\n",
        "üóÉÔ∏è **Database Status:**\n",
        "- Chunks stored in ChromaDB: {len(self.chunks)}\n",
        "- Embeddings generated: ‚úÖ\n",
        "- Ready for queries: ‚úÖ\"\"\"\n",
        "\n",
        "            return True, status_message, self.chunk_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, f\"‚ùå Error processing document: {str(e)}\", {}\n",
        "\n",
        "    def query(self, question: str) -> Tuple[str, List[Dict], str]:\n",
        "        \"\"\"Process query and return response with retrieved chunks\"\"\"\n",
        "        if not self.chunks:\n",
        "            return \"‚ùå No document loaded. Please upload a document first.\", [], \"\"\n",
        "\n",
        "        try:\n",
        "            # Generate query embedding\n",
        "            query_embedding = generate_single_embedding(question)\n",
        "            if not query_embedding:\n",
        "                return \"‚ùå Failed to generate query embedding.\", [], \"\"\n",
        "\n",
        "            # Search for relevant chunks\n",
        "            search_results = self.vector_store.search(\n",
        "                question,\n",
        "                query_embedding,\n",
        "                CONFIG[\"max_chunks_to_retrieve\"]\n",
        "            )\n",
        "\n",
        "            retrieved_chunks = search_results[\"chunks\"]\n",
        "            if not retrieved_chunks:\n",
        "                return \"‚ùå No relevant chunks found.\", [], \"\"\n",
        "\n",
        "            # Generate response\n",
        "            response = generate_response(question, retrieved_chunks, CONFIG[\"openai_model\"])\n",
        "\n",
        "            # Format retrieved chunks for display\n",
        "            chunks_display = []\n",
        "            for i, chunk in enumerate(retrieved_chunks):\n",
        "                chunks_display.append({\n",
        "                    \"chunk_number\": i + 1,\n",
        "                    \"text\": chunk[\"text\"][:200] + \"...\" if len(chunk[\"text\"]) > 200 else chunk[\"text\"],\n",
        "                    \"full_text\": chunk[\"text\"],\n",
        "                    \"similarity_score\": f\"{1 - chunk['distance']:.3f}\",\n",
        "                    \"word_count\": chunk[\"metadata\"].get(\"word_count\", \"N/A\")\n",
        "                })\n",
        "\n",
        "            # Create chunks summary\n",
        "            chunks_summary = f\"\"\"üîç **Retrieved {len(retrieved_chunks)} relevant chunks:**\n",
        "\n",
        "\"\"\" + \"\\n\".join([\n",
        "    f\"**Chunk {chunk['chunk_number']}** (Similarity: {chunk['similarity_score']}):\\n{chunk['text']}\\n\"\n",
        "    for chunk in chunks_display\n",
        "])\n",
        "\n",
        "            return response, chunks_display, chunks_summary\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error processing query: {str(e)}\", [], \"\"\n",
        "\n",
        "# Initialize RAG pipeline\n",
        "rag_pipeline = RAGPipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUAqeHUau4UG"
      },
      "source": [
        "## Cell 11 - Gradio Interface Functions\n",
        "\n",
        "This section explains the functions that power the user interface for your RAG system using Gradio, enabling file uploads, question answering, and system status checks.\n",
        "\n",
        "---\n",
        "\n",
        "### File Upload and Processing\n",
        "\n",
        "- **Purpose:**  \n",
        "  Handles the uploading and processing of documents through the Gradio interface.\n",
        "- **How it works:**  \n",
        "  - Checks if a file is uploaded.\n",
        "  - Processes the document using the RAG pipeline.\n",
        "  - Formats and displays document processing statistics and a preview of the first few chunks.\n",
        "  - Provides feedback if an error occurs or if no file is uploaded.\n",
        "\n",
        "---\n",
        "\n",
        "### Query Processing\n",
        "\n",
        "- **Purpose:**  \n",
        "  Handles user-submitted questions.\n",
        "- **How it works:**  \n",
        "  - Checks if the question is non-empty.\n",
        "  - Passes the question to the RAG pipeline for retrieval and response generation.\n",
        "  - Displays the answer, a summary of the retrieved chunks, and a timestamp for when the query was processed.\n",
        "\n",
        "---\n",
        "\n",
        "### System Status\n",
        "\n",
        "- **Purpose:**  \n",
        "  Provides a real-time overview of the system's configuration and readiness.\n",
        "- **How it works:**  \n",
        "  - Retrieves information about the vector database, such as status and document count.\n",
        "  - Displays configuration details, including the embedding model, OpenAI model, chunk size, chunk overlap, and retrieval settings.\n",
        "  - Indicates whether the system is ready, based on the presence of a valid OpenAI API key.\n",
        "\n",
        "---\n",
        "\n",
        "> **Note:**  \n",
        "> These functions enable a smooth and interactive user experience, allowing users to upload documents, ask questions, and monitor the system's health directly from the Gradio web interface.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "t5G2LYP-u2fu"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Gradio interface functions\n",
        "\"\"\"\n",
        "def upload_and_process_file(file):\n",
        "    \"\"\"Handle file upload and processing\"\"\"\n",
        "    if file is None:\n",
        "        return \"‚ùå Please upload a file first.\", \"{}\", \"\"\n",
        "\n",
        "    try:\n",
        "        success, message, stats = rag_pipeline.process_document(file.name)\n",
        "\n",
        "        # Format stats for display\n",
        "        stats_json = json.dumps(stats, indent=2) if stats else \"{}\"\n",
        "\n",
        "        # Create chunks preview\n",
        "        chunks_preview = \"\"\n",
        "        if success and rag_pipeline.chunks:\n",
        "            chunks_preview = \"üìÑ **Document Chunks Preview:**\\n\\n\"\n",
        "            for i, chunk in enumerate(rag_pipeline.chunks[:3]):  # Show first 3 chunks\n",
        "                preview_text = chunk[\"text\"][:150] + \"...\" if len(chunk[\"text\"]) > 150 else chunk[\"text\"]\n",
        "                chunks_preview += f\"**Chunk {i+1}:**\\n{preview_text}\\n\\n\"\n",
        "\n",
        "            if len(rag_pipeline.chunks) > 3:\n",
        "                chunks_preview += f\"... and {len(rag_pipeline.chunks) - 3} more chunks\"\n",
        "\n",
        "        return message, stats_json, chunks_preview\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error: {str(e)}\", \"{}\", \"\"\n",
        "\n",
        "def process_query(question):\n",
        "    \"\"\"Handle user queries\"\"\"\n",
        "    if not question.strip():\n",
        "        return \"‚ùå Please enter a question.\", \"\", \"\"\n",
        "\n",
        "    response, chunks, chunks_summary = rag_pipeline.query(question.strip())\n",
        "\n",
        "    return response, chunks_summary, f\"Query processed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "\n",
        "def get_system_status():\n",
        "    \"\"\"Get current system status\"\"\"\n",
        "    db_info = vector_store.get_collection_info()\n",
        "\n",
        "    status = f\"\"\"üñ•Ô∏è **System Status:**\n",
        "\n",
        "üìä **Database:** {db_info.get('status', 'unknown').title()}\n",
        "üìÅ **Documents in DB:** {db_info.get('document_count', 0)}\n",
        "üß† **Embedding Model:** {CONFIG['embedding_model']}\n",
        "ü§ñ **OpenAI Model:** {CONFIG['openai_model']}\n",
        "üìè **Chunk Size:** {CONFIG['chunk_size']} words\n",
        "üîÑ **Chunk Overlap:** {CONFIG['chunk_overlap']} words\n",
        "üéØ **Max Retrieval:** {CONFIG['max_chunks_to_retrieve']} chunks\n",
        "\n",
        "‚öôÔ∏è **Configuration Ready:** {'‚úÖ' if OPENAI_API_KEY else '‚ùå (API Key Required)'}\"\"\"\n",
        "\n",
        "    return status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmXPQ4NMvCB_"
      },
      "source": [
        "## Cell 12 - Creating the Gradio Interface\n",
        "\n",
        "This section describes the layout and functionality of the Gradio-based web interface for your RAG system, enabling users to upload documents, process them, ask questions, and view results in an interactive, user-friendly way.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fbtYmrLyu_mS"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Create the Gradio interface\n",
        "\"\"\"\n",
        "def create_gradio_interface():\n",
        "    \"\"\"Create and configure the Gradio interface\"\"\"\n",
        "\n",
        "    with gr.Blocks(\n",
        "        title=\"üöÄ RAG Lab - Retrieval Augmented Generation\",\n",
        "        theme=gr.themes.Soft(),\n",
        "        css=\"\"\"\n",
        "        .gradio-container {\n",
        "            max-width: 1200px !important;\n",
        "        }\n",
        "        .panel {\n",
        "            border: 1px solid #ddd;\n",
        "            border-radius: 8px;\n",
        "            padding: 15px;\n",
        "            margin: 10px 0;\n",
        "        }\n",
        "        \"\"\"\n",
        "    ) as app:\n",
        "\n",
        "        # Header\n",
        "        gr.Markdown(\"\"\"\n",
        "        # üöÄ RAG Lab - Retrieval Augmented Generation System\n",
        "\n",
        "        **Learn and experiment with RAG technology!** Upload documents, explore chunking strategies, and see how retrieval-augmented generation works in real-time.\n",
        "\n",
        "        ---\n",
        "        \"\"\")\n",
        "\n",
        "        # System Status\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                status_display = gr.Markdown(get_system_status())\n",
        "                gr.Button(\"üîÑ Refresh Status\").click(\n",
        "                    fn=get_system_status,\n",
        "                    outputs=status_display\n",
        "                )\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        # Main Interface\n",
        "        with gr.Row():\n",
        "            # Left Column - Document Upload and Processing\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"## üì§ **Step 1: Upload Document**\")\n",
        "\n",
        "                file_upload = gr.File(\n",
        "                    label=\"Upload Document (PDF, DOCX, TXT)\",\n",
        "                    file_types=[\".pdf\", \".docx\", \".txt\"]\n",
        "                )\n",
        "\n",
        "                process_btn = gr.Button(\"üîÑ Process Document\", variant=\"primary\")\n",
        "\n",
        "                gr.Markdown(\"## üìä **Processing Results**\")\n",
        "\n",
        "                processing_status = gr.Markdown(\"Upload a document to see processing results...\")\n",
        "\n",
        "                with gr.Accordion(\"üìà Chunk Statistics\", open=False):\n",
        "                    chunk_stats = gr.Code(\n",
        "                        label=\"Statistics (JSON)\",\n",
        "                        language=\"json\",\n",
        "                        value=\"{}\"\n",
        "                    )\n",
        "\n",
        "                with gr.Accordion(\"üìÑ Chunks Preview\", open=False):\n",
        "                    chunks_preview = gr.Markdown(\"Process a document to see chunks preview...\")\n",
        "\n",
        "            # Right Column - Query Interface\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"## ü§î **Step 2: Ask Questions**\")\n",
        "\n",
        "                query_input = gr.Textbox(\n",
        "                    label=\"Enter your question\",\n",
        "                    placeholder=\"What would you like to know about the document?\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "                query_btn = gr.Button(\"üîç Search & Generate Answer\", variant=\"secondary\")\n",
        "\n",
        "                gr.Markdown(\"## üí¨ **AI Response**\")\n",
        "\n",
        "                response_output = gr.Markdown(\"Ask a question to get an AI-generated response...\")\n",
        "\n",
        "                gr.Markdown(\"## üîç **Retrieved Context**\")\n",
        "\n",
        "                with gr.Accordion(\"üìã Retrieved Chunks\", open=True):\n",
        "                    retrieved_chunks = gr.Markdown(\"Retrieved chunks will appear here...\")\n",
        "\n",
        "                query_timestamp = gr.Markdown(\"\")\n",
        "\n",
        "        # Instructions\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        ## üìö **How to Use This RAG Lab:**\n",
        "\n",
        "        1. **Upload a Document**: Choose a PDF, DOCX, or TXT file containing the information you want to query\n",
        "        2. **Process the Document**: Click \"Process Document\" to chunk the text and create embeddings\n",
        "        3. **Review the Results**: Check the statistics and preview the chunks created\n",
        "        4. **Ask Questions**: Enter questions about the document content\n",
        "        5. **Explore Results**: See both the AI response and the retrieved chunks that informed the answer\n",
        "\n",
        "        ## üîß **What's Happening Behind the Scenes:**\n",
        "\n",
        "        - **Text Chunking**: Documents are split into overlapping chunks for better retrieval\n",
        "        - **Embeddings**: Each chunk is converted to a vector representation using SentenceTransformers\n",
        "        - **Vector Storage**: Chunks and embeddings are stored in ChromaDB for fast similarity search\n",
        "        - **Retrieval**: User queries are embedded and matched against stored chunks\n",
        "        - **Generation**: OpenAI generates responses using retrieved chunks as context\n",
        "\n",
        "        ## ‚öôÔ∏è **Current Configuration:**\n",
        "        - Chunk Size: {chunk_size} words\n",
        "        - Overlap: {chunk_overlap} words\n",
        "        - Max Retrieved: {max_retrieve} chunks\n",
        "        - Embedding Model: {embed_model}\n",
        "        - Generation Model: {gen_model}\n",
        "        \"\"\".format(\n",
        "            chunk_size=CONFIG[\"chunk_size\"],\n",
        "            chunk_overlap=CONFIG[\"chunk_overlap\"],\n",
        "            max_retrieve=CONFIG[\"max_chunks_to_retrieve\"],\n",
        "            embed_model=CONFIG[\"embedding_model\"],\n",
        "            gen_model=CONFIG[\"openai_model\"]\n",
        "        ))\n",
        "\n",
        "        # Event Handlers\n",
        "        process_btn.click(\n",
        "            fn=upload_and_process_file,\n",
        "            inputs=[file_upload],\n",
        "            outputs=[processing_status, chunk_stats, chunks_preview]\n",
        "        )\n",
        "\n",
        "        query_btn.click(\n",
        "            fn=process_query,\n",
        "            inputs=[query_input],\n",
        "            outputs=[response_output, retrieved_chunks, query_timestamp]\n",
        "        )\n",
        "\n",
        "        # Allow Enter key to submit query\n",
        "        query_input.submit(\n",
        "            fn=process_query,\n",
        "            inputs=[query_input],\n",
        "            outputs=[response_output, retrieved_chunks, query_timestamp]\n",
        "        )\n",
        "\n",
        "    return app\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPcV7R7CvQIM"
      },
      "source": [
        "# ==========================================\n",
        "# CELL 13: Launch the Application\n",
        "# ==========================================\n",
        "\n",
        "# Test Document\n",
        "Download the test document from: [Test Document Link](https://drive.google.com/file/d/1izzVVeydCl3UAqZvIL8Vkx1DYM77ZjSp/view?usp=sharing)\n",
        "\n",
        "> **Note:** This is the same document that was used in RAG 2.1 lab where it failed to process correctly. This implementation successfully handles the document processing and retrieval that previously failed in Lab 2.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "pg2qrvervIQj",
        "outputId": "5d63cae4-2610-44d4-d5e7-98706641a4c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    üöÄ Starting RAG Lab...\n",
            "    \n",
            "    üìã **Pre-launch Checklist:**\n",
            "    ‚úÖ ChromaDB initialized\n",
            "    ‚úÖ Embedding model loaded\n",
            "    ‚úÖ Gradio interface created\n",
            "    ‚úÖ OpenAI API configured\n",
            "    \n",
            "    üì° **Launching Application...**\n",
            "    \n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d9d76c7a7cba7609f4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://d9d76c7a7cba7609f4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "Launch the Gradio application\n",
        "\"\"\"\n",
        "if __name__ == \"__main__\":\n",
        "    # Create and launch the interface\n",
        "    app = create_gradio_interface()\n",
        "\n",
        "    print(\"\"\"\n",
        "    üöÄ Starting RAG Lab...\n",
        "\n",
        "    üìã **Pre-launch Checklist:**\n",
        "    ‚úÖ ChromaDB initialized\n",
        "    ‚úÖ Embedding model loaded\n",
        "    ‚úÖ Gradio interface created\n",
        "    {} OpenAI API configured\n",
        "\n",
        "    üì° **Launching Application...**\n",
        "    \"\"\".format(\"‚úÖ\" if OPENAI_API_KEY else \"‚ö†Ô∏è\"))\n",
        "\n",
        "    # Launch with public sharing enabled for Colab\n",
        "    app.launch(\n",
        "        share=True,  # Creates public link for Colab\n",
        "        server_name=\"0.0.0.0\",  # Allow external connections\n",
        "        server_port=7860,  # Default Gradio port\n",
        "        show_error=True,  # Show detailed errors\n",
        "        quiet=False  # Show startup messages\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
