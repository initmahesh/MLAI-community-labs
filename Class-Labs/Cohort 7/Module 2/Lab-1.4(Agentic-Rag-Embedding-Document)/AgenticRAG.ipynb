{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJdEc5U1fZzL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WADfrUNwfjwB"
      },
      "source": [
        "# **Understanding Agentic RAG: Building Intelligent Document Assistants with ChromaDB(Vectore DB) & LangChain(Framework)**\n",
        "\n",
        "### **What You'll Achieve** üéØ\n",
        "\n",
        "By the end of this lab, you'll gain a deep understanding of **Agentic RAG (Retrieval-Augmented Generation)** and how it revolutionizes the way we interact with documents. Here's what you'll learn:\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Storing Documents Intelligently Using Vector Embeddings** üóÇÔ∏è\n",
        "- **What**: Learn how to convert documents into numerical representations (vector embeddings) that capture their meaning.\n",
        "- **How**: Use **ChromaDB**, a vector database, to store and organize these embeddings efficiently.\n",
        "- **Why**: This allows the system to understand and retrieve information based on semantic meaning, not just keywords.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Retrieving Information with Semantic Understanding** üîç\n",
        "- **What**: Discover how to fetch relevant information from a large document collection using semantic search.\n",
        "- **How**: Leverage **LangChain** to query ChromaDB and retrieve the most contextually relevant chunks.\n",
        "- **Why**: This ensures that the system understands the intent behind your questions, not just the literal words.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Generating Context-Aware Answers Using Agentic Decision-Making** ü§ñ\n",
        "- **What**: Explore how **Agentic RAG** makes smart decisions about how to answer questions.\n",
        "- **How**: Implement a decision-making agent that evaluates the confidence of retrieved information and chooses the best response strategy.\n",
        "- **Why**: This allows the system to provide accurate and contextually appropriate answers, even when the information is incomplete or ambiguous.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Optimizing Responses Through Knowledge Graph Enhancements** üß†\n",
        "- **What**: Learn how to enhance answers by connecting related concepts using a knowledge graph.\n",
        "- **How**: Build a knowledge graph that maps terms and relationships (e.g., \"Master Agreement\" ‚Üí \"Contract\") to improve understanding.\n",
        "- **Why**: This enables the system to provide more comprehensive and insightful answers by leveraging contextual connections.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Let's Get Started!** üöÄ\n",
        "Ready to dive in? Follow the steps in the lab to see how these concepts come to life in code. By the end, you'll not only understand **Agentic RAG** but also know how to implement it in real-world applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p1YBrA6HRnoG",
        "outputId": "dcd8c3ab-2f24-4f21-deb4-cf3e8a76fdbf"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install required libraries\n",
        "! pip install chromadb langchain pypdf2 sentence-transformers pyboxen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mURYH1rugGdk"
      },
      "source": [
        "### üåê **About LangChain (In Simple Terms)**\n",
        "LangChain is a framework that helps developers **connect AI models with external data sources** like databases or APIs. In this project, we use LangChain to:\n",
        "- Embed text for efficient search.\n",
        "- Retrieve relevant information from the database.\n",
        "- Generate answers using OpenAI‚Äôs model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WbLlwKiMmAQE",
        "outputId": "f9ae0e96-d222-434b-a72b-715fa1744aea"
      },
      "outputs": [],
      "source": [
        "# Step 1.1 Intsall this package as well\n",
        "! pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNfSJPCdnM3y"
      },
      "source": [
        "# Step 3: Setting Up the Lab üß™\n",
        "\n",
        "---\n",
        "## Import necessary modules\n",
        "Before we start, we need to install the necessary tools. These packages are like the ingredients for our recipe ‚Äì without them, the lab won't work!\n",
        "\n",
        "# üìå Explanation of Imported Modules  \n",
        "\n",
        "## üîπ UI Components  \n",
        "- **`ipywidgets`** ‚Üí Provides interactive widgets like buttons, text boxes, and dropdowns.  \n",
        "- **`IPython.display`** ‚Üí Used to display widgets, clear output, and update UI dynamically.  \n",
        "\n",
        "## üîπ Data Processing & Utility  \n",
        "- **`random`** ‚Üí Generates random numbers, useful for testing and sampling.  \n",
        "- **`typing (List, Dict)`** ‚Üí Provides type hints for better code readability and debugging.  \n",
        "- **`io.BytesIO`** ‚Üí Handles in-memory file operations without saving to disk.  \n",
        "- **`os`** ‚Üí Interacts with the operating system (e.g., file paths, environment variables).  \n",
        "\n",
        "## üîπ LangChain Components  \n",
        "- **`langchain.vectorstores.Chroma`** ‚Üí Stores and retrieves document embeddings efficiently.  \n",
        "- **`langchain.embeddings.HuggingFaceEmbeddings`** ‚Üí Uses Hugging Face models for text embeddings.  \n",
        "- **`langchain.text_splitter.RecursiveCharacterTextSplitter`** ‚Üí Splits text into manageable chunks for processing.  \n",
        "- **`langchain.llms.OpenAI`** ‚Üí Connects to OpenAI‚Äôs LLM for generating responses.  \n",
        "- **`langchain.agents.initialize_agent`** ‚Üí Creates an AI agent with tools for querying documents.  \n",
        "- **`langchain.tools.Tool`** ‚Üí Defines custom tools for AI agents.  \n",
        "- **`langchain.tools.StructuredTool`** ‚Üí Provides structured tools for better AI responses.  \n",
        "- **`langchain.prompts.ChatPromptTemplate`** ‚Üí Creates structured prompts for AI interactions.  \n",
        "- **`langchain.schema.runnable.RunnablePassthrough`** ‚Üí Allows passing data through AI models without modification.  \n",
        "\n",
        "## üîπ PDF Handling  \n",
        "- **`PyPDF2.PdfReader`** ‚Üí Reads and extracts text from PDF documents.  \n",
        "\n",
        "## üîπ Database & Storage  \n",
        "- **`chromadb`** ‚Üí A fast and scalable vector database for storing and retrieving embeddings.  \n",
        "\n",
        "## üîπ UI Styling  \n",
        "- **`pyboxen.boxen`** ‚Üí Formats text in visually appealing boxed outputs.  \n",
        "\n",
        "## üîπ Data Validation  \n",
        "- **`pydantic.BaseModel, Field`** ‚Üí Ensures structured and validated data inputs.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xt7L9rR0UrbR"
      },
      "outputs": [],
      "source": [
        "# Step 2: Import necessary modules\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import random\n",
        "from typing import List, Dict\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.tools import StructuredTool\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from pydantic import BaseModel, Field\n",
        "from PyPDF2 import PdfReader\n",
        "import chromadb\n",
        "from pyboxen import boxen\n",
        "import os\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xypvDMdbggis"
      },
      "source": [
        "### üìå **Step 3: Initializing Components - Setting Up AI-Powered Document Processing: Embeddings, Vector Storage, and LLM Integration**\n",
        "```python\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "llm = OpenAI(temperature=0)\n",
        "```\n",
        "- **Embeddings:** We use `HuggingFaceEmbeddings` to convert text into numerical format.\n",
        "- **ChromaDB Client:** Creates a database at `./chroma_db` to store text embeddings.\n",
        "- **OpenAI API:** We set the API key to access OpenAI‚Äôs language model.\n",
        "[Generate Your OpenAI API Key](https://youtu.be/CzO_AT9dkC8?si=6HsOpOGLoZH-kz-F)\n",
        "\n",
        "- **LLM Initialization:** We set the temperature to `0` for more deterministic responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-P9jwcSyUvdv"
      },
      "outputs": [],
      "source": [
        "# Step 3: Initialize components\n",
        "# Initialize embedding model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# Initialize LLM (Replace with your API key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "llm = OpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld1FlCazgsgP"
      },
      "source": [
        "# üìå Step 4: Upload & Process a PDF Document  \n",
        "\n",
        "This step allows users to upload a PDF file, extract its text, split it into smaller chunks, and store it in a vector database for AI processing.  \n",
        "\n",
        "## üîπ How It Works  \n",
        "1. **File Upload Widget**  \n",
        "   - A button appears that lets you upload a **PDF file**.  \n",
        "   - The system only accepts **one file at a time**.  \n",
        "\n",
        "2. **Processing the File**  \n",
        "   - When you click the **\"Process File\"** button:  \n",
        "     - The system checks if a file is uploaded.  \n",
        "     - If no file is found, it shows a message: **\"No file uploaded!\"**  \n",
        "     - If a file is uploaded, the system reads the PDF and extracts its text.  \n",
        "     - The text is split into smaller **chunks** (for efficient processing).  \n",
        "     - Each chunk is assigned a **random confidence score**.  \n",
        "     - The data is stored in **ChromaDB**, a special AI-ready database.  \n",
        "\n",
        "3. **Success Message**  \n",
        "   - If everything works, you‚Äôll see a **green notification box**:  \n",
        "     ```\n",
        "     File processed with confidence scores!\n",
        "     ```  \n",
        "   - This means your file was successfully processed and stored.  \n",
        "\n",
        "## üîπ Steps to Use  \n",
        "‚úÖ **Step 1** ‚Üí Click on **\"Upload Document\"** and select a PDF file.  \n",
        "‚úÖ **Step 2** ‚Üí Click **\"Process File\"** to analyze and store it.  \n",
        "‚úÖ **Step 3** ‚Üí Wait for the **success message** confirming the file is processed.  \n",
        "\n",
        "**‚úÖ NOTE** : [üìÑ **Reference Document You Can Use**](https://drive.google.com/file/d/1WWa_TgI49HIAGFuXTNvMLtkFBU6ZduHq/view?usp=sharing)\n",
        "\n",
        "This setup makes it easy to upload and process PDFs with AI-powered storage. üöÄ  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155,
          "referenced_widgets": [
            "61fa487e09f146df858e5ff852852817",
            "fcd2c04e736747f9a4aaefd2e3a8e5d8",
            "42009fd475574ef48de40ebe36f4961f",
            "234000427b2041f0a625cdc99ed45372",
            "208e2d02c0774f04bb470be2d81e1bfa",
            "3701175e40864740989bb733401f6ca4",
            "643332e54133406ea09ec5c7a36b3c16",
            "99a9dcae2e874181ab7cea927ec48a11"
          ]
        },
        "id": "SPaM1w24U1tF",
        "outputId": "3d1eb7e8-50e6-4eb0-e2fd-de236fb160b1"
      },
      "outputs": [],
      "source": [
        "# Step 4: File upload widget with value storage\n",
        "import random\n",
        "uploader = widgets.FileUpload(accept='.pdf', multiple=False)\n",
        "process_btn = widgets.Button(description=\"Process File\")\n",
        "process_output = widgets.Output()\n",
        "\n",
        "def process_file(b):\n",
        "    with process_output:\n",
        "        clear_output()\n",
        "        if not uploader.value:\n",
        "            print(\"No file uploaded!\")\n",
        "            return\n",
        "\n",
        "        for filename, file_info in uploader.value.items():\n",
        "            pdf = PdfReader(BytesIO(file_info['content']))\n",
        "            break\n",
        "\n",
        "        text = \"\\n\".join([page.extract_text() for page in pdf.pages])\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000, chunk_overlap=100\n",
        "        )\n",
        "        chunks = text_splitter.split_text(text)\n",
        "        metadatas = [{\"value\": random.uniform(0, 1)} for _ in chunks]\n",
        "\n",
        "        Chroma.from_texts(\n",
        "            chunks, embeddings,\n",
        "            client=chroma_client,\n",
        "            collection_name=\"doc_collection\",\n",
        "            metadatas=metadatas\n",
        "        )\n",
        "        print(boxen(\"File processed with confidence scores!\", title=\"Success\", color=\"green\"))\n",
        "\n",
        "display(uploader, process_btn, process_output)\n",
        "process_btn.on_click(process_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsnCbBlhg-sv"
      },
      "source": [
        "# üìù Step 5: Enhanced Query Processing with an Agent  \n",
        "\n",
        "This step enhances **query processing** by implementing an **AI agent** that decides how to answer user queries based on **confidence scores**. It retrieves relevant document chunks and determines whether the response should be **directly generated** or **refined using a knowledge graph** for better accuracy.  \n",
        "\n",
        "---\n",
        "## üîπ Overview of the Query Processing Flow  \n",
        "\n",
        "### 1Ô∏è‚É£ **User Query Retrieval**  \n",
        "- The user submits a **query**.\n",
        "- The system **retrieves relevant document chunks** using ChromaDB.\n",
        "- Each retrieved chunk has an **associated confidence score**.\n",
        "\n",
        "### 2Ô∏è‚É£ **Confidence Score Evaluation**  \n",
        "- If the confidence score is **high (‚â• 0.5)** ‚Üí Use a **direct answer** approach.\n",
        "- If the confidence score is **low (< 0.3)** ‚Üí Use a **knowledge-enhanced answer**.\n",
        "\n",
        "### 3Ô∏è‚É£ **Knowledge Graph Enhancement** (for low-confidence cases)  \n",
        "- Some terms in the query are mapped to **more meaningful alternatives** using a **knowledge graph**.\n",
        "- The query is **rewritten** with these enhancements.\n",
        "- A new document search is performed using the improved query.\n",
        "\n",
        "### 4Ô∏è‚É£ **Generating the Final Answer**  \n",
        "- The system **chooses the best response strategy** using an autonomous agent.\n",
        "- The **final answer is generated** based on the refined context and confidence evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç **Understanding Confidence Scores**  \n",
        "\n",
        "A **confidence score** is a measure of **how relevant** a retrieved document chunk is to the user's query.  \n",
        "- **Higher confidence (‚â• 0.5)** ‚Üí The retrieved text is **relevant and reliable**.  \n",
        "- **Lower confidence (< 0.3)** ‚Üí The retrieved text **may not be relevant or sufficient**.  \n",
        "\n",
        "### üìå **How is the Confidence Score Used?**  \n",
        "- If the retrieved document chunk has a **high confidence score**, the system can **directly answer the query**.  \n",
        "- If the retrieved document chunk has a **low confidence score**, the AI **enhances the query** using a **knowledge graph** and **retrieves better chunks**.\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ **Role of the AI Agent**  \n",
        "\n",
        "The **AI Agent** acts as a **decision-maker** that chooses the **best answering approach** based on **confidence scores**.  \n",
        "\n",
        "### üîπ **How the Agent Works**\n",
        "1. **User submits a query**.\n",
        "2. **Document chunks are retrieved** from the database.\n",
        "3. **Each chunk has an associated confidence score**.\n",
        "4. The AI agent **analyzes the confidence scores** and chooses:\n",
        "   - ‚úÖ **Direct Answer** ‚Üí If confidence is **high (‚â• 0.5)**.\n",
        "   - üîÑ **Enhanced Answer** ‚Üí If confidence is **low (< 0.3)**, it **rewrites the query** and **retrieves better chunks**.\n",
        "5. The chosen **answering tool is executed**, and the **final response** is provided.  \n",
        "\n",
        "---\n",
        "\n",
        "# What is Knowledge Graph\n",
        "\n",
        "A **Knowledge Graph** is a structured representation of information that connects data points using relationships. It organizes and links concepts, entities, and facts in a way that allows machines (like AI) to understand and reason about the data.\n",
        "\n",
        "## Key Components of a Knowledge Graph\n",
        "1. **Entities (Nodes)** ‚Äì Objects or concepts (e.g., a company, a contract, a product).\n",
        "2. **Relationships (Edges)** ‚Äì Connections between entities (e.g., \"Customer **has** a Master Agreement\").\n",
        "3. **Attributes (Properties)** ‚Äì Details about entities (e.g., \"Contract Start Date: Jan 2024\").\n",
        "\n",
        "## Example of a Simple Knowledge Graph\n",
        "\n",
        "### üìå Entities:\n",
        "- **Customer**: ABC Corp\n",
        "- **Master Agreement**: Contract\n",
        "- **Service Order**: Specific service agreement\n",
        "\n",
        "### üìå Relationships:\n",
        "- `ABC Corp ‚Üí has ‚Üí Master Agreement`\n",
        "- `Master Agreement ‚Üí includes ‚Üí Service Order`\n",
        "\n",
        "## Why Use a Knowledge Graph?\n",
        "- üöÄ Helps AI understand context better\n",
        "- üîç Improves search and query responses\n",
        "- üîó Connects scattered information for better decision-making\n",
        "\n",
        "# NOTE\n",
        "# We are using a dummy knowledge graph with respect to three terms:\n",
        "\n",
        "- **Customer**:\n",
        "- **Master Agreement**:\n",
        "- **Service Order**:\n",
        "\n",
        "# If you are using a query different from it, then you have to provide your context to the knowledge graph as well.\n",
        "---\n",
        "\n",
        "# üéØ **Final Outcome**\n",
        "- If the retrieved document is **relevant**, the AI answers immediately.  \n",
        "- If the information is **uncertain**, the AI **optimizes** the search before answering.  \n",
        "- The AI agent makes **independent decisions** for the best response. üöÄ  \n",
        "\n",
        "This **smart decision-making** process makes the AI **more intelligent and reliable** for answering document-based queries. üìñ‚ú®\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfeLdybzU5VW",
        "outputId": "c00e9e81-a00b-45f6-8547-902ad06e9dd3"
      },
      "outputs": [],
      "source": [
        "# Step 5: Enhanced query processing with autonomous agent\n",
        "class QueryInput(BaseModel):\n",
        "    query: str = Field(description=\"User's original question\")\n",
        "    chunks: List[Dict] = Field(description=\"Retrieved document chunks with confidence scores\")\n",
        "\n",
        "def retrieve_chunks(query: str) -> List[Dict]:\n",
        "    collection = chroma_client.get_collection(\"doc_collection\")\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=1,\n",
        "        include=['documents', 'metadatas']\n",
        "    )\n",
        "\n",
        "    # Ensure metadata is returned and handle cases where it's missing\n",
        "    documents = results['documents'][0]\n",
        "    metadatas = results.get('metadatas', [[{}] * len(documents)])[0]  # Default to empty dicts if metadata is missing\n",
        "\n",
        "    # Create chunks with actual metadata and confidence scores\n",
        "    chunks = [\n",
        "        {\"text\": doc, \"metadata\": meta}\n",
        "        for doc, meta in zip(documents, metadatas)\n",
        "    ]\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def knowledge_graph(query: str) -> str:\n",
        "    # Enhanced knowledge graph mappings\n",
        "    kg_mappings = {\n",
        "        \"customer\": \"Company Name Referenced in Master Agreement Only\",\n",
        "        \"Master Agreement\": \"Contract\",\n",
        "        \"Service Order\": \"Service Agreement\",\n",
        "        \"name of the Customer\": \"Company Name Referenced in Master Agreement Only\",\n",
        "        # Add case variations or use case-insensitive comparison\n",
        "        \"Name Of the Customer\": \"Company Name Referenced in Master Agreement Only\"\n",
        "    }\n",
        "\n",
        "    # Alternatively, use case-insensitive replacement:\n",
        "    enhanced_query = query\n",
        "    for term, replacement in kg_mappings.items():\n",
        "        pattern = re.compile(re.escape(term), re.IGNORECASE)\n",
        "        enhanced_query = pattern.sub(replacement, enhanced_query)\n",
        "\n",
        "    return enhanced_query\n",
        "\n",
        "def base_answer(input: QueryInput) -> str:\n",
        "    context = \"\\n\\n\".join([chunk[\"text\"] for chunk in input.chunks])\n",
        "    prompt = f\"Answer: {input.query}\\nContext:\\n{context}\"\n",
        "    answer = llm(prompt)\n",
        "    return f\"Answer: {answer}\"\n",
        "\n",
        "def enhanced_answer(input: QueryInput) -> str:\n",
        "    enhanced_query = knowledge_graph(input.query)\n",
        "    enhanced_chunks = retrieve_chunks(enhanced_query)\n",
        "    context = \"\\n\\n\".join([chunk[\"text\"] for chunk in enhanced_chunks])\n",
        "    prompt = f\"Answer: {enhanced_query}\\nContext:\\n{context}\"\n",
        "    answer = llm(prompt)\n",
        "    return f\"Optimized Answer: {answer}\"\n",
        "\n",
        "\n",
        "# Define Tools\n",
        "tools = [\n",
        "    StructuredTool.from_function(\n",
        "        name=\"DirectAnswer\",\n",
        "        description=\"Use this when the retrieved chunks have high confidence scores (>= 0.5) and sufficient context to directly answer the query.\",\n",
        "        func=base_answer\n",
        "    ),\n",
        "    StructuredTool.from_function(\n",
        "        name=\"EnhancedAnswer\",\n",
        "        description=\"Use this when the retrieved chunks have low confidence scores (< 0.3) or insufficient context, and the query needs to be enhanced using a knowledge graph.\",\n",
        "        func=enhanced_answer\n",
        "    )\n",
        "]\n",
        "\n",
        "# Initialize the agent with a custom prompt\n",
        "agent_prompt = \"\"\"You are an intelligent assistant that decides how to answer user queries based on the context and confidence scores of retrieved document chunks.\n",
        "\n",
        "Here are the tools available to you:\n",
        "1. DirectAnswer: Use this when the retrieved chunks have high confidence scores (>= 0.5) and sufficient context to directly answer the query.\n",
        "2. EnhancedAnswer: Use this when the retrieved chunks have low confidence scores (< 0.3) or insufficient context, and the query needs to be enhanced using a knowledge graph.\n",
        "\n",
        "Your task is to analyze the query, retrieved chunks, and their confidence scores, and decide which tool to use. Provide a reason for your decision.\n",
        "\n",
        "Query: {query}\n",
        "Retrieved Chunks: {chunks}\n",
        "\n",
        "Decision: Which tool should be used? Respond in the following format:\n",
        "Tool: [DirectAnswer or EnhancedAnswer]\n",
        "Reason: [Your reasoning]\"\"\"\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=\"structured-chat-zero-shot-react-description\",\n",
        "    verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aaAiW68hiqJ"
      },
      "source": [
        "# üìù **Step 6: Query Interface with Autonomous Decision-Making**  \n",
        "\n",
        "This step introduces an **interactive query interface** that allows users to input their questions.  \n",
        "The AI agent **analyzes** the query, **retrieves relevant document chunks**, and **decides how to answer**  \n",
        "based on confidence scores. The decision-making is fully **autonomous**, ensuring **optimized responses**.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **How the Query Interface Works**  \n",
        "\n",
        "### üîπ **User Input & Query Submission**  \n",
        "- You have to enters their query in the input field (`query_input`).\n",
        "- Clicking the \"Submit\" button (`submit_btn`) triggers the `handle_query` function.\n",
        "\n",
        "# üîç Step-by-Step Query Processing Flow\n",
        "**1Ô∏è‚É£ Step 1: Query Asked**\n",
        "- The user inputs a query.\n",
        "- The query is displayed in a notification box for reference.\n",
        "\n",
        "**2Ô∏è‚É£ Step 2: Retrieve Relevant Chunks**\n",
        "- The query is searched in the document database (ChromaDB).\n",
        "- The most relevant text chunks are retrieved, along with their confidence scores.\n",
        "- Retrieved chunks are displayed in a green notification box.\n",
        "\n",
        "**3Ô∏è‚É£ Step 3: Generate an Initial (Poor) Answer**\n",
        "- The system tries to generate an answer from the retrieved chunks.\n",
        "- This is an unoptimized response that might be incorrect due to low confidence scores.\n",
        "- The poor answer is displayed in a red notification box.\n",
        "\n",
        "**4Ô∏è‚É£ Step 4: AI Agent Decision**\n",
        "- The AI agent analyzes the retrieved chunks and confidence scores.\n",
        "- It decides whether to:\n",
        "  - Use the DirectAnswer tool (if confidence scores are high).\n",
        "  - Use the EnhancedAnswer tool (if confidence scores are low).\n",
        "- The agent's decision is displayed in a magenta notification box.\n",
        "\n",
        "**5Ô∏è‚É£ Step 5: Generate the Final Answer**\n",
        "\n",
        "‚úÖ If the agent chooses \"DirectAnswer\"\n",
        "- The initial answer is used without modifications.\n",
        "\n",
        "‚úÖ If the agent chooses \"EnhancedAnswer\"\n",
        "- The query is refined by agnet using a Knowledge Graph.\n",
        "-A new set of relevant document chunks is retrieved.\n",
        "- A more optimized answer is generated.\n",
        "\n",
        "# üèÅ How to Use the Query Interface?\n",
        "1Ô∏è‚É£ Enter your question in the input box.\n",
        "\n",
        "```text\n",
        "What is the name of the Customer ?\n",
        "```\n",
        "\n",
        "2Ô∏è‚É£ Click \"Submit\" to process the query.\n",
        "\n",
        "3Ô∏è‚É£ The system will retrieve relevant document chunks and evaluate confidence scores.\n",
        "\n",
        "4Ô∏è‚É£ The AI agent decides whether to give a direct answer or enhance the query using a Knowledge Graph.\n",
        "\n",
        "5Ô∏è‚É£ The final answer is displayed, either as a Direct Answer or an Optimized Answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1751d890cd77404384a73db788133cef",
            "a4632857da764d15a814b50ccba5b8f7",
            "371640502d514cfebf473c51ddcbb73a",
            "4170071a9f3149c8a18c0d25708d5558",
            "fe11a6ec32cb455bb5bd0f19e44a7f5d",
            "0e88e470f6634ddf9c928569ba30f0a8",
            "f6a68b0221b14c979183b2a4b0ef510a",
            "e851fb6833b74bf4bbec4cc23e72ae12"
          ]
        },
        "collapsed": true,
        "id": "JORFQMonVaFK",
        "outputId": "e881f7fa-6080-49ef-c2e5-a81d3f859b18"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# Step 6: Query interface with autonomous decision-making\n",
        "query_input = widgets.Text(placeholder=\"Enter your query\")\n",
        "submit_btn = widgets.Button(description=\"Submit\")\n",
        "query_output = widgets.Output()\n",
        "\n",
        "def handle_query(b):\n",
        "    with query_output:\n",
        "        clear_output()\n",
        "        query = query_input.value\n",
        "\n",
        "        # Step 1: Query Asked\n",
        "        print(boxen(f\"QUERY ASKED:\\n{query}\", title=\"Step 1: Query Asked\", color=\"blue\"))\n",
        "\n",
        "        # Step 2: Retrieve Chunks\n",
        "        chunks = retrieve_chunks(query)\n",
        "        print(boxen(\n",
        "            f\"RETRIEVED CHUNKS:\\n\" +\n",
        "            \"\\n\".join([f\"Chunk {i+1}: {chunk['text']}\"\n",
        "                       for i, chunk in enumerate(chunks)]),\n",
        "            title=\"Step 2: Retrieved Chunks\", color=\"green\"\n",
        "        ))\n",
        "\n",
        "        # Step 3: Generate and Display Poor Answer\n",
        "        poor_answer = base_answer(QueryInput(query=query, chunks=chunks))\n",
        "        print(boxen(\n",
        "            f\"POOR ANSWER (Initial Attempt):\\n{poor_answer}\",\n",
        "            title=\"Step 3: Poor Answer\", color=\"red\"\n",
        "        ))\n",
        "\n",
        "        # Step 4: Let the Agent Decide Which Tool to Use\n",
        "        decision_output = llm(agent_prompt.format(query=query, chunks=chunks))\n",
        "        decision = decision_output.split(\"Tool: \")[1].split(\"\\n\")[0].strip()  # Extract tool\n",
        "        reason = decision_output.split(\"Reason: \")[1].strip()  # Extract reason\n",
        "        print(boxen(\n",
        "            f\"AGENT DECISION:\\nTool: KNOWLEDGE GRAPH\\nReason: {reason}\",\n",
        "            title=\"Step 4: Agent Decision\", color=\"magenta\"\n",
        "        ))\n",
        "\n",
        "        # Step 5: Execute the Chosen Tool\n",
        "        if \"DirectAnswer\" in decision:\n",
        "            response = base_answer(QueryInput(query=query, chunks=chunks))\n",
        "            print(boxen(\n",
        "                f\"DIRECT ANSWER:\\n{response}\",\n",
        "                title=\"Step 5: Direct Answer\", color=\"yellow\"\n",
        "            ))\n",
        "        elif \"EnhancedAnswer\" in decision:\n",
        "            print(boxen(\n",
        "                \"USING KNOWLEDGE GRAPH TO ENHANCE QUERY...\",\n",
        "                title=\"Step 5: Knowledge Graph Enhancement\", color=\"cyan\"\n",
        "            ))\n",
        "            enhanced_query = knowledge_graph(query)\n",
        "            print(boxen(\n",
        "                f\"ENHANCED QUERY:\\n{enhanced_query}\",\n",
        "                title=\"Step 5: Enhanced Query\", color=\"cyan\"\n",
        "            ))\n",
        "            enhanced_chunks = retrieve_chunks(enhanced_query)\n",
        "            response = enhanced_answer(QueryInput(query=enhanced_query, chunks=enhanced_chunks))\n",
        "            print(boxen(\n",
        "                f\"FINAL OPTIMIZED ANSWER:\\n{response}\",\n",
        "                title=\"Step 5: Final Optimized Answer\", color=\"green\"\n",
        "            ))\n",
        "        else:\n",
        "            print(boxen(\n",
        "                \"Unable to determine the appropriate tool. Using default answer.\",\n",
        "                title=\"Step 5: Default Answer\", color=\"red\"\n",
        "            ))\n",
        "\n",
        "\n",
        "display(query_input, submit_btn, query_output)\n",
        "submit_btn.on_click(handle_query)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e88e470f6634ddf9c928569ba30f0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1751d890cd77404384a73db788133cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_a4632857da764d15a814b50ccba5b8f7",
            "placeholder": "Enter your query",
            "style": "IPY_MODEL_371640502d514cfebf473c51ddcbb73a",
            "value": "What is the name of the Customer ?"
          }
        },
        "208e2d02c0774f04bb470be2d81e1bfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "234000427b2041f0a625cdc99ed45372": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Process File",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_208e2d02c0774f04bb470be2d81e1bfa",
            "style": "IPY_MODEL_3701175e40864740989bb733401f6ca4",
            "tooltip": ""
          }
        },
        "3701175e40864740989bb733401f6ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "371640502d514cfebf473c51ddcbb73a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4170071a9f3149c8a18c0d25708d5558": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Submit",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_fe11a6ec32cb455bb5bd0f19e44a7f5d",
            "style": "IPY_MODEL_0e88e470f6634ddf9c928569ba30f0a8",
            "tooltip": ""
          }
        },
        "42009fd475574ef48de40ebe36f4961f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "61fa487e09f146df858e5ff852852817": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FileUploadModel",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".pdf",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_fcd2c04e736747f9a4aaefd2e3a8e5d8",
            "metadata": [
              {
                "lastModified": 1744443511642,
                "name": "Morningstar Inc MSA.pdf",
                "size": 248159,
                "type": "application/pdf"
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_42009fd475574ef48de40ebe36f4961f"
          }
        },
        "643332e54133406ea09ec5c7a36b3c16": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_99a9dcae2e874181ab7cea927ec48a11",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n",
                  "text/plain": ""
                },
                "metadata": {},
                "output_type": "display_data"
              },
              {
                "name": "stdout",
                "output_type": "stream",
                "text": [
                  "\u001b[32m‚ï≠‚îÄ\u001b[0m\u001b[32m Success \u001b[0m\u001b[32m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[32m‚îÄ‚ïÆ\u001b[0m                                                                           \n",
                  "\u001b[32m‚îÇ\u001b[0mFile processed with confidence scores!\u001b[32m‚îÇ\u001b[0m                                                                           \n",
                  "\u001b[32m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m                                                                           \n",
                  "\n"
                ]
              }
            ]
          }
        },
        "99a9dcae2e874181ab7cea927ec48a11": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4632857da764d15a814b50ccba5b8f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e851fb6833b74bf4bbec4cc23e72ae12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6a68b0221b14c979183b2a4b0ef510a": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_e851fb6833b74bf4bbec4cc23e72ae12",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n",
                  "text/plain": ""
                },
                "metadata": {},
                "output_type": "display_data"
              },
              {
                "name": "stdout",
                "output_type": "stream",
                "text": [
                  "\u001b[34m‚ï≠‚îÄ\u001b[0m\u001b[34m Step 1: Query Asked \u001b[0m\u001b[34m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[34m‚îÄ‚ïÆ\u001b[0m                                                                               \n",
                  "\u001b[34m‚îÇ\u001b[0mQUERY ASKED:                      \u001b[34m‚îÇ\u001b[0m                                                                               \n",
                  "\u001b[34m‚îÇ\u001b[0mWhat is the name of the Customer ?\u001b[34m‚îÇ\u001b[0m                                                                               \n",
                  "\u001b[34m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m                                                                               \n",
                  "\n"
                ]
              },
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n",
                  "text/plain": ""
                },
                "metadata": {},
                "output_type": "display_data"
              },
              {
                "name": "stdout",
                "output_type": "stream",
                "text": [
                  "\u001b[32m‚ï≠‚îÄ\u001b[0m\u001b[32m Step 2: Retrieved Chunks \u001b[0m\u001b[32m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[32m‚îÄ‚ïÆ\u001b[0m \n",
                  "\u001b[32m‚îÇ\u001b[0mRETRIEVED CHUNKS:                                                                                               \u001b[32m‚îÇ\u001b[0m \n",
                  "\u001b[32m‚îÇ\u001b[0mChunk 1: purpose described in this Master Agreement and that Product License Agreement; and/or                  \u001b[32m‚îÇ\u001b[0m \n",
                  "\u001b[32m‚îÇ\u001b[0m(2)Execute one or more Morningstar Service Orders (each a \"Service Order'') by which Morningstar Provider       \u001b[32m‚îÇ\u001b[0m \n",
                  "\u001b[32m‚îÇ\u001b[0mmakes available, and Company Recipient is allowed to use pursuant to the terms of that Service Order and        \u001b[32m‚îÇ\u001b[0m \n",
                  "\u001b[32m‚îÇ\u001b[0mthis Master Agreement, certain extra-cost customization, development, consulting, research  related             \u001b[32m‚îÇ\u001b[0m \n",
                  "\u001b[32m‚îÇ\u001b[0mservices and/01'.back office services (individually and collectively, \"Special Services\"), all as more fully    \u001b[32m‚îÇ\u001b[0m \n",
                  "\u001b[32m‚îÇ\u001b[0mdescribed in the applicable Service Order.                                                                      \u001b[32m‚îÇ\u001b[0m \n",
                  "\u001b[32m‚îÇ\u001b[0m(b)Attribution and Use: Subject to Section 1(e) below and any additional terms contained in the relevant Product\u001b[32m‚îÇ\u001b[0m \n",
                  "\u001b[32m‚îÇ\u001b[0mLicense Agreement/Service Order, Company Recipient will clearly reference the Morningstar Provider as the       \u001b[32m‚îÇ\u001b[0m \n",
                  "\u001b[32m‚îÇ\u001b[0mprovider of the Product/Special Services in all Company Recipient advertising and marketing materials and       \u001b[32m‚îÇ\u001b[0m \n",
                  "\u001b[32m‚îÇ\u001b[0mpublic communications referencing the Product/Special Services. If Company Recipient displays all or a          \u001b[32m‚îÇ\u001b[0m \n",
                  "\u001b[32m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m \n",
                  "\n"
                ]
              },
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n",
                  "text/plain": ""
                },
                "metadata": {},
                "output_type": "display_data"
              },
              {
                "name": "stdout",
                "output_type": "stream",
                "text": [
                  "\u001b[31m‚ï≠‚îÄ\u001b[0m\u001b[31m Step 3: Poor Answer \u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m‚îÄ‚ïÆ\u001b[0m       \n",
                  "\u001b[31m‚îÇ\u001b[0mPOOR ANSWER (Initial Attempt):                                                                            \u001b[31m‚îÇ\u001b[0m       \n",
                  "\u001b[31m‚îÇ\u001b[0mAnswer:                                                                                                   \u001b[31m‚îÇ\u001b[0m       \n",
                  "\u001b[31m‚îÇ\u001b[0mportion of the Product/Special Services on a Company Recipient website, Company Recipient will provide    \u001b[31m‚îÇ\u001b[0m       \n",
                  "\u001b[31m‚îÇ\u001b[0mappropriate attribution to Morningstar Provider on the same webpage(s) as the Product/Special Services are\u001b[31m‚îÇ\u001b[0m       \n",
                  "\u001b[31m‚îÇ\u001b[0mdisplayed.                                                                                                \u001b[31m‚îÇ\u001b[0m       \n",
                  "\u001b[31m‚îÇ\u001b[0m                                                                                                          \u001b[31m‚îÇ\u001b[0m       \n",
                  "\u001b[31m‚îÇ\u001b[0mWho is the Customer?                                                                                      \u001b[31m‚îÇ\u001b[0m       \n",
                  "\u001b[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m       \n",
                  "\n"
                ]
              },
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n",
                  "text/plain": ""
                },
                "metadata": {},
                "output_type": "display_data"
              },
              {
                "name": "stdout",
                "output_type": "stream",
                "text": [
                  "\u001b[35m‚ï≠‚îÄ\u001b[0m\u001b[35m Step 4: Agent Decision \u001b[0m\u001b[35m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[35m‚îÄ‚ïÆ\u001b[0m\n",
                  "\u001b[35m‚îÇ\u001b[0mAGENT DECISION:                                                                                                  \u001b[35m‚îÇ\u001b[0m\n",
                  "\u001b[35m‚îÇ\u001b[0mTool: KNOWLEDGE GRAPH                                                                                            \u001b[35m‚îÇ\u001b[0m\n",
                  "\u001b[35m‚îÇ\u001b[0mReason: The retrieved chunk has a low confidence score of 0.0415, indicating that it may not have enough context \u001b[35m‚îÇ\u001b[0m\n",
                  "\u001b[35m‚îÇ\u001b[0mto directly answer the query. Additionally, the chunk contains a lot of technical and legal jargon, making it    \u001b[35m‚îÇ\u001b[0m\n",
                  "\u001b[35m‚îÇ\u001b[0mdifficult for a direct answer to be extracted. Using a knowledge graph to enhance the query would provide more   \u001b[35m‚îÇ\u001b[0m\n",
                  "\u001b[35m‚îÇ\u001b[0maccurate and relevant information for the user.                                                                  \u001b[35m‚îÇ\u001b[0m\n",
                  "\u001b[35m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m\n",
                  "\n"
                ]
              },
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n",
                  "text/plain": ""
                },
                "metadata": {},
                "output_type": "display_data"
              },
              {
                "name": "stdout",
                "output_type": "stream",
                "text": [
                  "\u001b[36m‚ï≠‚îÄ\u001b[0m\u001b[36m Step 5: Knowledge Graph Enhancement \u001b[0m\u001b[36m‚îÄ‚îÄ\u001b[0m\u001b[36m‚îÄ‚ïÆ\u001b[0m                                                                        \n",
                  "\u001b[36m‚îÇ\u001b[0mUSING KNOWLEDGE GRAPH TO ENHANCE QUERY...\u001b[36m‚îÇ\u001b[0m                                                                        \n",
                  "\u001b[36m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m                                                                        \n",
                  "\n"
                ]
              },
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n",
                  "text/plain": ""
                },
                "metadata": {},
                "output_type": "display_data"
              },
              {
                "name": "stdout",
                "output_type": "stream",
                "text": [
                  "\u001b[36m‚ï≠‚îÄ\u001b[0m\u001b[36m Step 5: Enhanced Query \u001b[0m\u001b[36m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[36m‚îÄ‚ïÆ\u001b[0m                                               \n",
                  "\u001b[36m‚îÇ\u001b[0mENHANCED QUERY:                                                   \u001b[36m‚îÇ\u001b[0m                                               \n",
                  "\u001b[36m‚îÇ\u001b[0mWhat is the name of the Company Name Referenced in Contract Only ?\u001b[36m‚îÇ\u001b[0m                                               \n",
                  "\u001b[36m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m                                               \n",
                  "\n"
                ]
              },
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n",
                  "text/plain": ""
                },
                "metadata": {},
                "output_type": "display_data"
              },
              {
                "name": "stdout",
                "output_type": "stream",
                "text": [
                  "\u001b[32m‚ï≠‚îÄ\u001b[0m\u001b[32m Step 5: Final Optimized Answer \u001b[0m\u001b[32m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[32m‚îÄ‚ïÆ\u001b[0m                                                      \n",
                  "\u001b[32m‚îÇ\u001b[0mFINAL OPTIMIZED ANSWER:                                    \u001b[32m‚îÇ\u001b[0m                                                      \n",
                  "\u001b[32m‚îÇ\u001b[0mOptimized Answer:                                          \u001b[32m‚îÇ\u001b[0m                                                      \n",
                  "\u001b[32m‚îÇ\u001b[0m                                                           \u001b[32m‚îÇ\u001b[0m                                                      \n",
                  "\u001b[32m‚îÇ\u001b[0mThe company name referenced in the contract is Morningstar.\u001b[32m‚îÇ\u001b[0m                                                      \n",
                  "\u001b[32m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m                                                      \n",
                  "\n"
                ]
              }
            ]
          }
        },
        "fcd2c04e736747f9a4aaefd2e3a8e5d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe11a6ec32cb455bb5bd0f19e44a7f5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
