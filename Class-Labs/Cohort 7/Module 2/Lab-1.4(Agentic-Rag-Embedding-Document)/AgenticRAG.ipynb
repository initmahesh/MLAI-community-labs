{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJdEc5U1fZzL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WADfrUNwfjwB"
      },
      "source": [
        "# **Understanding Agentic RAG: Building Intelligent Document Assistants with ChromaDB(Vectore DB) & LangChain(Framework)**\n",
        "\n",
        "### **What You'll Achieve** üéØ\n",
        "\n",
        "By the end of this lab, you'll gain a deep understanding of **Agentic RAG (Retrieval-Augmented Generation)** and how it revolutionizes the way we interact with documents. Here's what you'll learn:\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Storing Documents Intelligently Using Vector Embeddings** üóÇÔ∏è\n",
        "- **What**: Learn how to convert documents into numerical representations (vector embeddings) that capture their meaning.\n",
        "- **How**: Use **ChromaDB**, a vector database, to store and organize these embeddings efficiently.\n",
        "- **Why**: This allows the system to understand and retrieve information based on semantic meaning, not just keywords.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Retrieving Information with Semantic Understanding** üîç\n",
        "- **What**: Discover how to fetch relevant information from a large document collection using semantic search.\n",
        "- **How**: Leverage **LangChain** to query ChromaDB and retrieve the most contextually relevant chunks.\n",
        "- **Why**: This ensures that the system understands the intent behind your questions, not just the literal words.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Generating Context-Aware Answers Using Agentic Decision-Making** ü§ñ\n",
        "- **What**: Explore how **Agentic RAG** makes smart decisions about how to answer questions.\n",
        "- **How**: Implement a decision-making agent that evaluates the confidence of retrieved information and chooses the best response strategy.\n",
        "- **Why**: This allows the system to provide accurate and contextually appropriate answers, even when the information is incomplete or ambiguous.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Optimizing Responses Through Knowledge Graph Enhancements** üß†\n",
        "- **What**: Learn how to enhance answers by connecting related concepts using a knowledge graph.\n",
        "- **How**: Build a knowledge graph that maps terms and relationships (e.g., \"Master Agreement\" ‚Üí \"Contract\") to improve understanding.\n",
        "- **Why**: This enables the system to provide more comprehensive and insightful answers by leveraging contextual connections.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Let's Get Started!** üöÄ\n",
        "Ready to dive in? Follow the steps in the lab to see how these concepts come to life in code. By the end, you'll not only understand **Agentic RAG** but also know how to implement it in real-world applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "p1YBrA6HRnoG"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install required libraries\n",
        "! pip install chromadb langchain pypdf2 sentence-transformers pyboxen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mURYH1rugGdk"
      },
      "source": [
        "### üåê **About LangChain (In Simple Terms)**\n",
        "LangChain is a framework that helps developers **connect AI models with external data sources** like databases or APIs. In this project, we use LangChain to:\n",
        "- Embed text for efficient search.\n",
        "- Retrieve relevant information from the database.\n",
        "- Generate answers using OpenAI‚Äôs model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WbLlwKiMmAQE"
      },
      "outputs": [],
      "source": [
        "# Step 1.1 Intsall this package as well\n",
        "! pip install -U langchain-community langchain-core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNfSJPCdnM3y"
      },
      "source": [
        "# Step 3: Setting Up the Lab üß™\n",
        "\n",
        "---\n",
        "## Import necessary modules\n",
        "Before we start, we need to install the necessary tools. These packages are like the ingredients for our recipe ‚Äì without them, the lab won't work!\n",
        "\n",
        "# üìå Explanation of Imported Modules  \n",
        "\n",
        "## üîπ UI Components  \n",
        "- **`ipywidgets`** ‚Üí Provides interactive widgets like buttons, text boxes, and dropdowns.  \n",
        "- **`IPython.display`** ‚Üí Used to display widgets, clear output, and update UI dynamically.  \n",
        "\n",
        "## üîπ Data Processing & Utility  \n",
        "- **`random`** ‚Üí Generates random numbers, useful for testing and sampling.  \n",
        "- **`typing (List, Dict)`** ‚Üí Provides type hints for better code readability and debugging.  \n",
        "- **`io.BytesIO`** ‚Üí Handles in-memory file operations without saving to disk.  \n",
        "- **`os`** ‚Üí Interacts with the operating system (e.g., file paths, environment variables).  \n",
        "\n",
        "## üîπ LangChain Components  \n",
        "- **`langchain.vectorstores.Chroma`** ‚Üí Stores and retrieves document embeddings efficiently.  \n",
        "- **`langchain.embeddings.HuggingFaceEmbeddings`** ‚Üí Uses Hugging Face models for text embeddings.  \n",
        "- **`langchain.text_splitter.RecursiveCharacterTextSplitter`** ‚Üí Splits text into manageable chunks for processing.  \n",
        "- **`langchain.llms.OpenAI`** ‚Üí Connects to OpenAI‚Äôs LLM for generating responses.  \n",
        "- **`langchain.agents.initialize_agent`** ‚Üí Creates an AI agent with tools for querying documents.  \n",
        "- **`langchain.tools.Tool`** ‚Üí Defines custom tools for AI agents.  \n",
        "- **`langchain.tools.StructuredTool`** ‚Üí Provides structured tools for better AI responses.  \n",
        "- **`langchain.prompts.ChatPromptTemplate`** ‚Üí Creates structured prompts for AI interactions.  \n",
        "- **`langchain.schema.runnable.RunnablePassthrough`** ‚Üí Allows passing data through AI models without modification.  \n",
        "\n",
        "## üîπ PDF Handling  \n",
        "- **`PyPDF2.PdfReader`** ‚Üí Reads and extracts text from PDF documents.  \n",
        "\n",
        "## üîπ Database & Storage  \n",
        "- **`chromadb`** ‚Üí A fast and scalable vector database for storing and retrieving embeddings.  \n",
        "\n",
        "## üîπ UI Styling  \n",
        "- **`pyboxen.boxen`** ‚Üí Formats text in visually appealing boxed outputs.  \n",
        "\n",
        "## üîπ Data Validation  \n",
        "- **`pydantic.BaseModel, Field`** ‚Üí Ensures structured and validated data inputs.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xt7L9rR0UrbR"
      },
      "outputs": [],
      "source": [
        "# Step 2: Import necessary modules (Simplified version)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import random\n",
        "from typing import List, Dict\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.llms import OpenAI\n",
        "from langchain_core.tools import Tool, StructuredTool\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from pydantic import BaseModel, Field\n",
        "from PyPDF2 import PdfReader\n",
        "import chromadb\n",
        "from pyboxen import boxen\n",
        "import os\n",
        "from io import BytesIO\n",
        "\n",
        "# Skip agent imports for now - we can implement custom logic instead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xypvDMdbggis"
      },
      "source": [
        "### üìå **Step 3: Initializing Components - Setting Up AI-Powered Document Processing: Embeddings, Vector Storage, and LLM Integration**\n",
        "```python\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "llm = OpenAI(temperature=0)\n",
        "```\n",
        "- **Embeddings:** We use `HuggingFaceEmbeddings` to convert text into numerical format.\n",
        "- **ChromaDB Client:** Creates a database at `./chroma_db` to store text embeddings.\n",
        "- **OpenAI API:** We set the API key to access OpenAI‚Äôs language model.\n",
        "[Generate Your OpenAI API Key](https://youtu.be/CzO_AT9dkC8?si=6HsOpOGLoZH-kz-F)\n",
        "\n",
        "- **LLM Initialization:** We set the temperature to `0` for more deterministic responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "id": "-P9jwcSyUvdv"
      },
      "outputs": [],
      "source": [
        "# Step 3: Initialize components\n",
        "# Initialize embedding model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# Initialize LLM (Replace with your API key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Add Your API Key\"\n",
        "llm = OpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld1FlCazgsgP"
      },
      "source": [
        "# üìå Step 4: Upload & Process a PDF Document  \n",
        "\n",
        "This step allows users to upload a PDF file, extract its text, split it into smaller chunks, and store it in a vector database for AI processing.  \n",
        "\n",
        "## üîπ How It Works  \n",
        "1. **File Upload Widget**  \n",
        "   - A button appears that lets you upload a **PDF file**.  \n",
        "   - The system only accepts **one file at a time**.  \n",
        "\n",
        "2. **Processing the File**  \n",
        "   - When you click the **\"Process File\"** button:  \n",
        "     - The system checks if a file is uploaded.  \n",
        "     - If no file is found, it shows a message: **\"No file uploaded!\"**  \n",
        "     - If a file is uploaded, the system reads the PDF and extracts its text.  \n",
        "     - The text is split into smaller **chunks** (for efficient processing).  \n",
        "     - Each chunk is assigned a **random confidence score**.  \n",
        "     - The data is stored in **ChromaDB**, a special AI-ready database.  \n",
        "\n",
        "3. **Success Message**  \n",
        "   - If everything works, you‚Äôll see a **green notification box**:  \n",
        "     ```\n",
        "     File processed with confidence scores!\n",
        "     ```  \n",
        "   - This means your file was successfully processed and stored.  \n",
        "\n",
        "## üîπ Steps to Use  \n",
        "‚úÖ **Step 1** ‚Üí Click on **\"Upload Document\"** and select a PDF file.  \n",
        "‚úÖ **Step 2** ‚Üí Click **\"Process File\"** to analyze and store it.  \n",
        "‚úÖ **Step 3** ‚Üí Wait for the **success message** confirming the file is processed.  \n",
        "\n",
        "**‚úÖ NOTE** : [üìÑ **Reference Document You Can Use**](https://drive.google.com/file/d/1WWa_TgI49HIAGFuXTNvMLtkFBU6ZduHq/view?usp=sharing)\n",
        "\n",
        "This setup makes it easy to upload and process PDFs with AI-powered storage. üöÄ  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154,
          "referenced_widgets": [
            "499bb7ca97e441cab66ba9ee0a36964c",
            "5ad4d69b621947d496110cb50a515554",
            "723e2d321c0a4cd6b35c8d96e34801b7",
            "9924cde225394254a295a2670985d796",
            "f8f439f9a3aa4715aff5e952656e0ad8",
            "79965054e08443c0b7eb5d526f9f2264",
            "98a38c5e05624220a6f915633d04d22a",
            "1e65e7b720914b0da668938d9ee79f3e"
          ]
        },
        "id": "SPaM1w24U1tF",
        "outputId": "99d00821-b079-4329-9f78-ce02bb3b4e87"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.pdf', description='Upload')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "499bb7ca97e441cab66ba9ee0a36964c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Process File', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9924cde225394254a295a2670985d796"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98a38c5e05624220a6f915633d04d22a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step 4: File upload widget with value storage\n",
        "import random\n",
        "uploader = widgets.FileUpload(accept='.pdf', multiple=False)\n",
        "process_btn = widgets.Button(description=\"Process File\")\n",
        "process_output = widgets.Output()\n",
        "\n",
        "def process_file(b):\n",
        "    with process_output:\n",
        "        clear_output()\n",
        "        if not uploader.value:\n",
        "            print(\"No file uploaded!\")\n",
        "            return\n",
        "\n",
        "        for filename, file_info in uploader.value.items():\n",
        "            pdf = PdfReader(BytesIO(file_info['content']))\n",
        "            break\n",
        "\n",
        "        text = \"\\n\".join([page.extract_text() for page in pdf.pages])\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000, chunk_overlap=100\n",
        "        )\n",
        "        chunks = text_splitter.split_text(text)\n",
        "        metadatas = [{\"value\": random.uniform(0, 1)} for _ in chunks]\n",
        "\n",
        "        Chroma.from_texts(\n",
        "            chunks, embeddings,\n",
        "            client=chroma_client,\n",
        "            collection_name=\"doc_collection\",\n",
        "            metadatas=metadatas\n",
        "        )\n",
        "        print(boxen(\"File processed with confidence scores!\", title=\"Success\", color=\"green\"))\n",
        "\n",
        "display(uploader, process_btn, process_output)\n",
        "process_btn.on_click(process_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsnCbBlhg-sv"
      },
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "LLM-BASED AUTONOMOUS QUERY PROCESSING SYSTEM\n",
        "================================================================================\n",
        "\n",
        "OVERVIEW:\n",
        "---------\n",
        "This system uses an LLM as an intelligent agent to autonomously decide whether\n",
        "a user query needs optimization through a Knowledge Graph or can be answered\n",
        "directly from initial retrieval results.\n",
        "\n",
        "HOW IT WORKS (5-STEP PROCESS):\n",
        "-------------------------------\n",
        "\n",
        "STEP 1: INITIAL QUERY SUBMISSION\n",
        "    - User submits a natural language query\n",
        "    - Example: \"What is the name of the Customer?\"\n",
        "\n",
        "STEP 2: INITIAL RETRIEVAL\n",
        "    - System retrieves relevant document chunks using semantic search (ChromaDB)\n",
        "    - Each chunk gets a confidence score (0.0 to 1.0)\n",
        "    - Higher confidence = better semantic match to the query\n",
        "\n",
        "STEP 3: GENERATE INITIAL ANSWER\n",
        "    - LLM generates an answer based on retrieved chunks\n",
        "    - This is our \"baseline\" answer before any optimization\n",
        "\n",
        "STEP 4: LLM AUTONOMOUS DECISION MAKING (THE KEY INNOVATION)\n",
        "    - LLM acts as an intelligent agent analyzing:\n",
        "      a) The original query\n",
        "      b) Retrieved chunks and their confidence scores\n",
        "      c) The initial answer quality\n",
        "    \n",
        "    - LLM decides between TWO TOOLS:\n",
        "      * DirectAnswer: Use initial results (good enough)\n",
        "      * EnhancedAnswer: Apply Knowledge Graph optimization (needs improvement)\n",
        "    \n",
        "    - Decision Criteria (LLM evaluates):\n",
        "      ‚úì Are retrieved chunks relevant?\n",
        "      ‚úì Is the initial answer complete and accurate?\n",
        "      ‚úì Would query enhancement help (e.g., mapping \"customer\" ‚Üí specific entity)?\n",
        "      ‚úì Is the confidence score too low?\n",
        "\n",
        "STEP 5: EXECUTE CHOSEN TOOL\n",
        "    - If DirectAnswer: Return initial answer\n",
        "    - If EnhancedAnswer:\n",
        "      a) Apply Knowledge Graph mappings to query\n",
        "      b) Re-retrieve chunks with enhanced query\n",
        "      c) Generate improved final answer\n",
        "\n",
        "WHY LLM-BASED DECISION MAKING?\n",
        "-------------------------------\n",
        "‚ùå Traditional Approach: Hard threshold (e.g., \"if confidence < 0.5, enhance\")\n",
        "‚úÖ LLM Approach: Semantic understanding of when optimization is truly needed\n",
        "\n",
        "Benefits:\n",
        "- Understands context and query intent\n",
        "- Adapts to different types of questions\n",
        "- No manual threshold tuning\n",
        "- Explains reasoning for transparency\n",
        "\n",
        "KNOWLEDGE GRAPH MAPPINGS:\n",
        "--------------------------\n",
        "Maps generic terms to domain-specific entities:\n",
        "- \"customer\" ‚Üí \"Company Name Referenced in Master Agreement Only\"\n",
        "- \"Master Agreement\" ‚Üí \"Contract\"\n",
        "- \"Service Order\" ‚Üí \"Service Agreement\"\n",
        "\n",
        "This helps retrieve more precise chunks when initial retrieval is vague.\n",
        "\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: DOCUMENT RETRIEVAL FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def retrieve_chunks(query: str, n_results: int = 3) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Retrieve document chunks using semantic search\n",
        "    \n",
        "    HOW IT WORKS:\n",
        "    1. Converts query to embeddings\n",
        "    2. Finds most similar document chunks in ChromaDB\n",
        "    3. Calculates confidence scores from distance metrics\n",
        "    \n",
        "    Args:\n",
        "        query: User's natural language question\n",
        "        n_results: Number of chunks to retrieve\n",
        "        \n",
        "    Returns:\n",
        "        List of chunks with text, metadata, and confidence scores\n",
        "        \n",
        "    CONFIDENCE CALCULATION:\n",
        "        - ChromaDB returns cosine distance (0 = identical, 2 = opposite)\n",
        "        - We convert to confidence: confidence = max(0, 1 - (distance / 2))\n",
        "        - Result: 0.0 (poor match) to 1.0 (perfect match)\n",
        "    \"\"\"\n",
        "    collection = chroma_client.get_collection(\"doc_collection\")\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=n_results,\n",
        "        include=['documents', 'metadatas', 'distances']\n",
        "    )\n",
        "\n",
        "    documents = results['documents'][0]\n",
        "    metadatas = results.get('metadatas', [[{}] * len(documents)])[0]\n",
        "    distances = results.get('distances', [[1.0] * len(documents)])[0]\n",
        "    \n",
        "    # Convert distances to confidence scores\n",
        "    chunks = [\n",
        "        {\n",
        "            \"text\": doc,\n",
        "            \"metadata\": meta,\n",
        "            \"confidence\": max(0, 1 - (dist / 2))  # Lower distance = higher confidence\n",
        "        }\n",
        "        for doc, meta, dist in zip(documents, metadatas, distances)\n",
        "    ]\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: KNOWLEDGE GRAPH QUERY ENHANCEMENT\n",
        "# ============================================================================\n",
        "\n",
        "def knowledge_graph(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Enhance query using Knowledge Graph mappings\n",
        "    \n",
        "    HOW IT WORKS:\n",
        "    - Replaces generic terms with domain-specific entities\n",
        "    - Uses case-insensitive pattern matching\n",
        "    - Example: \"customer\" ‚Üí \"Company Name Referenced in Master Agreement Only\"\n",
        "    \n",
        "    WHEN IS THIS USED?\n",
        "    - Only when LLM decides enhancement is needed (via llm_decision_maker)\n",
        "    - Helps retrieve more specific chunks when initial retrieval is too vague\n",
        "    \n",
        "    Args:\n",
        "        query: Original user query\n",
        "        \n",
        "    Returns:\n",
        "        Enhanced query with mapped terms\n",
        "        \n",
        "    EXAMPLE:\n",
        "        Input:  \"What is the name of the Customer?\"\n",
        "        Output: \"What is the name of the Company Name Referenced in Master Agreement Only?\"\n",
        "    \"\"\"\n",
        "    kg_mappings = {\n",
        "        \"customer\": \"Company Name Referenced in Master Agreement Only\",\n",
        "        \"Customer\": \"Company Name Referenced in Master Agreement Only\",\n",
        "        \"name of the customer\": \"Company Name Referenced in Master Agreement Only\",\n",
        "        \"name of the Customer\": \"Company Name Referenced in Master Agreement Only\",\n",
        "        \"Master Agreement\": \"Contract\",\n",
        "        \"Service Order\": \"Service Agreement\",\n",
        "    }\n",
        "\n",
        "    enhanced_query = query\n",
        "    for term, replacement in kg_mappings.items():\n",
        "        pattern = re.compile(re.escape(term), re.IGNORECASE)\n",
        "        enhanced_query = pattern.sub(replacement, enhanced_query)\n",
        "\n",
        "    return enhanced_query\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: ANSWER GENERATION FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def base_answer(query: str, chunks: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Generate answer using retrieved chunks\n",
        "    \n",
        "    HOW IT WORKS:\n",
        "    1. Combines all chunk texts into context\n",
        "    2. Sends query + context to LLM\n",
        "    3. LLM generates answer based on provided context\n",
        "    \n",
        "    Args:\n",
        "        query: User's question (original or enhanced)\n",
        "        chunks: Retrieved document chunks\n",
        "        \n",
        "    Returns:\n",
        "        Generated answer string\n",
        "        \n",
        "    NOTE: This function is used for both:\n",
        "        - Initial answer (before decision)\n",
        "        - Final answer (after enhancement, if needed)\n",
        "    \"\"\"\n",
        "    context = \"\\n\\n\".join([chunk[\"text\"] for chunk in chunks])\n",
        "    prompt = f\"\"\"Based on the following context, answer the question concisely.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    try:\n",
        "        answer = llm.invoke(prompt)\n",
        "    except AttributeError:\n",
        "        answer = llm.predict(prompt)\n",
        "    \n",
        "    return answer.strip()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: LLM AUTONOMOUS DECISION MAKER (CORE INTELLIGENCE)\n",
        "# ============================================================================\n",
        "\n",
        "def llm_decision_maker(query: str, chunks: List[Dict], initial_answer: str) -> Dict:\n",
        "    \"\"\"\n",
        "    *** THIS IS THE KEY COMPONENT ***\n",
        "    LLM acts as an intelligent agent to decide which tool to use\n",
        "    \n",
        "    HOW THE LLM DECIDES:\n",
        "    --------------------\n",
        "    The LLM analyzes three inputs:\n",
        "    1. QUERY: What is the user asking?\n",
        "    2. CHUNKS: What information was retrieved? (with confidence scores)\n",
        "    3. INITIAL_ANSWER: What answer was generated from these chunks?\n",
        "    \n",
        "    The LLM evaluates:\n",
        "    ‚úì Chunk Relevance: Do chunks actually relate to the question?\n",
        "    ‚úì Answer Quality: Is the answer complete, accurate, specific?\n",
        "    ‚úì Confidence Scores: Are the semantic matches strong enough?\n",
        "    ‚úì Enhancement Potential: Would Knowledge Graph mapping help?\n",
        "    \n",
        "    DECISION OPTIONS:\n",
        "    -----------------\n",
        "    Option 1: DirectAnswer\n",
        "        - Initial chunks are good enough\n",
        "        - Answer is satisfactory\n",
        "        - No optimization needed\n",
        "        \n",
        "    Option 2: EnhancedAnswer\n",
        "        - Chunks are vague or irrelevant\n",
        "        - Answer is unclear or incomplete\n",
        "        - Query contains terms that benefit from KG mapping\n",
        "        - Low confidence scores suggest poor retrieval\n",
        "    \n",
        "    WHY LLM INSTEAD OF RULES?\n",
        "    --------------------------\n",
        "    ‚ùå Rule-based: \"if confidence < 0.5, enhance\"\n",
        "       - Too rigid\n",
        "       - Doesn't understand context\n",
        "       - Misses nuanced cases\n",
        "       \n",
        "    ‚úÖ LLM-based: Semantic understanding\n",
        "       - Understands query intent\n",
        "       - Evaluates answer quality\n",
        "       - Adapts to different question types\n",
        "       - Provides reasoning (explainable AI)\n",
        "    \n",
        "    Args:\n",
        "        query: Original user query\n",
        "        chunks: Retrieved chunks with confidence scores\n",
        "        initial_answer: First answer generated\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with:\n",
        "        - decision: \"DirectAnswer\" or \"EnhancedAnswer\"\n",
        "        - reason: LLM's explanation for the decision\n",
        "        - confidence: LLM's confidence in its decision (0.0-1.0)\n",
        "        - avg_chunk_confidence: Average retrieval confidence\n",
        "        \n",
        "    EXAMPLE DECISION FLOW:\n",
        "    ----------------------\n",
        "    Query: \"What is the name of the Customer?\"\n",
        "    Chunks: [low confidence, vague content]\n",
        "    Initial Answer: \"Company Recipient\"\n",
        "    \n",
        "    LLM Analysis:\n",
        "    - Chunks don't clearly identify customer name\n",
        "    - Answer seems generic/uncertain\n",
        "    - Term \"customer\" could benefit from KG mapping\n",
        "    \n",
        "    Decision: EnhancedAnswer\n",
        "    Reason: \"Retrieved chunks lack specific customer identification;\n",
        "             knowledge graph mapping of 'customer' term will improve precision\"\n",
        "    \"\"\"\n",
        "    \n",
        "    # Prepare chunks summary for LLM analysis\n",
        "    chunks_summary = \"\\n\".join([\n",
        "        f\"Chunk {i+1} (Confidence: {chunk['confidence']:.2f}): {chunk['text'][:150]}...\"\n",
        "        for i, chunk in enumerate(chunks)\n",
        "    ])\n",
        "    \n",
        "    avg_confidence = sum(chunk[\"confidence\"] for chunk in chunks) / len(chunks) if chunks else 0\n",
        "    \n",
        "    # DECISION PROMPT: This is where we instruct the LLM to act as an intelligent agent\n",
        "    decision_prompt = f\"\"\"You are an intelligent query analysis agent. Your task is to decide whether the initial answer is satisfactory or if the query needs to be enhanced using a knowledge graph.\n",
        "\n",
        "Original Query: {query}\n",
        "\n",
        "Retrieved Chunks:\n",
        "{chunks_summary}\n",
        "\n",
        "Average Confidence Score: {avg_confidence:.2f}\n",
        "\n",
        "Initial Answer Generated: {initial_answer}\n",
        "\n",
        "Analyze the situation and decide:\n",
        "\n",
        "1. Is the initial answer accurate and complete based on the retrieved chunks?\n",
        "2. Do the retrieved chunks contain sufficient relevant information to answer the query?\n",
        "3. Does the query contain terms that might benefit from knowledge graph mapping (e.g., \"customer\", \"Master Agreement\")?\n",
        "\n",
        "Respond in this EXACT format:\n",
        "DECISION: [DirectAnswer OR EnhancedAnswer]\n",
        "REASON: [Your detailed reasoning in one sentence]\n",
        "CONFIDENCE_IN_DECISION: [0.0 to 1.0]\n",
        "\n",
        "Examples:\n",
        "- If chunks are highly relevant and answer is good: \"DECISION: DirectAnswer\"\n",
        "- If chunks are vague or answer is unclear: \"DECISION: EnhancedAnswer\"\n",
        "\"\"\"\n",
        "    \n",
        "    # Get LLM's decision\n",
        "    try:\n",
        "        decision_response = llm.invoke(decision_prompt)\n",
        "    except AttributeError:\n",
        "        decision_response = llm.predict(decision_prompt)\n",
        "    \n",
        "    # Parse LLM response to extract decision, reason, and confidence\n",
        "    decision_lines = decision_response.strip().split('\\n')\n",
        "    decision = \"DirectAnswer\"  # Default fallback\n",
        "    reason = \"Unable to parse decision\"\n",
        "    confidence = 0.5\n",
        "    \n",
        "    for line in decision_lines:\n",
        "        if line.startswith(\"DECISION:\"):\n",
        "            decision_value = line.split(\"DECISION:\")[1].strip()\n",
        "            if \"EnhancedAnswer\" in decision_value or \"Enhanced\" in decision_value:\n",
        "                decision = \"EnhancedAnswer\"\n",
        "            else:\n",
        "                decision = \"DirectAnswer\"\n",
        "        elif line.startswith(\"REASON:\"):\n",
        "            reason = line.split(\"REASON:\")[1].strip()\n",
        "        elif line.startswith(\"CONFIDENCE_IN_DECISION:\"):\n",
        "            try:\n",
        "                confidence = float(line.split(\"CONFIDENCE_IN_DECISION:\")[1].strip())\n",
        "            except:\n",
        "                confidence = 0.5\n",
        "    \n",
        "    return {\n",
        "        \"decision\": decision,\n",
        "        \"reason\": reason,\n",
        "        \"confidence\": confidence,\n",
        "        \"avg_chunk_confidence\": avg_confidence\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: ENHANCED ANSWER GENERATION (IF NEEDED)\n",
        "# ============================================================================\n",
        "\n",
        "def enhanced_answer(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate answer using Knowledge Graph enhanced query\n",
        "    \n",
        "    WHEN IS THIS CALLED?\n",
        "    - Only when llm_decision_maker returns \"EnhancedAnswer\"\n",
        "    \n",
        "    HOW IT WORKS:\n",
        "    1. Apply Knowledge Graph mappings to query\n",
        "    2. Re-retrieve chunks with enhanced query\n",
        "    3. Generate answer from new, hopefully better chunks\n",
        "    \n",
        "    Args:\n",
        "        query: Original user query\n",
        "        \n",
        "    Returns:\n",
        "        Enhanced answer string\n",
        "        \n",
        "    ENHANCEMENT FLOW:\n",
        "        Original Query: \"What is the name of the Customer?\"\n",
        "        ‚Üì (Knowledge Graph)\n",
        "        Enhanced Query: \"What is the name of the Company Name Referenced in Master Agreement Only?\"\n",
        "        ‚Üì (Re-retrieve)\n",
        "        Better Chunks: [chunks mentioning specific company names]\n",
        "        ‚Üì (Generate)\n",
        "        Better Answer: \"Morningstar\"\n",
        "    \"\"\"\n",
        "    # Step 1: Enhance query with Knowledge Graph\n",
        "    enhanced_query = knowledge_graph(query)\n",
        "    \n",
        "    # Step 2: Retrieve with enhanced query\n",
        "    enhanced_chunks = retrieve_chunks(enhanced_query)\n",
        "    \n",
        "    # Step 3: Generate answer from new chunks\n",
        "    context = \"\\n\\n\".join([chunk[\"text\"] for chunk in enhanced_chunks])\n",
        "    \n",
        "    prompt = f\"\"\"Based on the following context, answer the question concisely.\n",
        "\n",
        "Original Question: {query}\n",
        "Enhanced Question: {enhanced_query}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    try:\n",
        "        answer = llm.invoke(prompt)\n",
        "    except AttributeError:\n",
        "        answer = llm.predict(prompt)\n",
        "    \n",
        "    return answer.strip()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# COMPLETE WORKFLOW SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "COMPLETE QUERY PROCESSING FLOW:\n",
        "================================\n",
        "\n",
        "1. USER SUBMITS QUERY\n",
        "   ‚Üì\n",
        "2. RETRIEVE INITIAL CHUNKS (retrieve_chunks)\n",
        "   ‚Üì\n",
        "3. GENERATE INITIAL ANSWER (base_answer)\n",
        "   ‚Üì\n",
        "4. LLM ANALYZES & DECIDES (llm_decision_maker) ‚Üê THE INTELLIGENCE\n",
        "   ‚Üì\n",
        "   ‚îú‚îÄ‚Üí Decision: DirectAnswer\n",
        "   ‚îÇ   ‚îî‚îÄ‚Üí Return initial answer (done!)\n",
        "   ‚îÇ\n",
        "   ‚îî‚îÄ‚Üí Decision: EnhancedAnswer\n",
        "       ‚Üì\n",
        "       5a. APPLY KNOWLEDGE GRAPH (knowledge_graph)\n",
        "       ‚Üì\n",
        "       5b. RE-RETRIEVE CHUNKS (retrieve_chunks)\n",
        "       ‚Üì\n",
        "       5c. GENERATE FINAL ANSWER (base_answer)\n",
        "       ‚Üì\n",
        "       5d. Return enhanced answer (done!)\n",
        "\n",
        "\n",
        "KEY ADVANTAGES OF THIS APPROACH:\n",
        "=================================\n",
        "\n",
        "1. AUTONOMOUS: No manual threshold tuning\n",
        "2. ADAPTIVE: Works for different query types\n",
        "3. INTELLIGENT: Understands semantic context\n",
        "4. EXPLAINABLE: LLM provides reasoning\n",
        "5. EFFICIENT: Only enhances when truly needed\n",
        "6. TRANSPARENT: Users see decision-making process\n",
        "\n",
        "\n",
        "EXAMPLE USE CASES:\n",
        "==================\n",
        "\n",
        "Case 1: Good Initial Retrieval\n",
        "Query: \"What is the payment term?\"\n",
        "Chunks: High confidence, clear payment terms\n",
        "Initial Answer: \"Net 30 days\"\n",
        "LLM Decision: DirectAnswer ‚úì\n",
        "Result: Return \"Net 30 days\" (fast, efficient)\n",
        "\n",
        "Case 2: Poor Initial Retrieval  \n",
        "Query: \"What is the name of the Customer?\"\n",
        "Chunks: Low confidence, vague references\n",
        "Initial Answer: \"Company Recipient\"\n",
        "LLM Decision: EnhancedAnswer ‚úì\n",
        "Enhancement: \"customer\" ‚Üí \"Company Name Referenced in Master Agreement Only\"\n",
        "New Chunks: Better matches with company names\n",
        "Final Answer: \"Morningstar\" (accurate!)\n",
        "\n",
        "\n",
        "CONFIGURATION OPTIONS:\n",
        "======================\n",
        "\n",
        "1. Chunk Retrieval:\n",
        "   - n_results: Number of chunks to retrieve (default: 3)\n",
        "   - Adjust in retrieve_chunks() function\n",
        "\n",
        "2. Knowledge Graph:\n",
        "   - Add new mappings in knowledge_graph() dictionary\n",
        "   - Format: \"generic_term\": \"specific_entity\"\n",
        "\n",
        "3. LLM Decision Criteria:\n",
        "   - Modify decision_prompt in llm_decision_maker()\n",
        "   - Adjust analysis questions/criteria\n",
        "\n",
        "4. Confidence Thresholds:\n",
        "   - Optionally add backup logic if LLM fails\n",
        "   - Current: Pure LLM-based (no hardcoded thresholds)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZfeLdybzU5VW"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Step 5: LLM-based autonomous decision making\n",
        "def retrieve_chunks(query: str, n_results: int = 3) -> List[Dict]:\n",
        "    \"\"\"Retrieve document chunks with metadata\"\"\"\n",
        "    collection = chroma_client.get_collection(\"doc_collection\")\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=n_results,\n",
        "        include=['documents', 'metadatas', 'distances']\n",
        "    )\n",
        "\n",
        "    documents = results['documents'][0]\n",
        "    metadatas = results.get('metadatas', [[{}] * len(documents)])[0]\n",
        "    distances = results.get('distances', [[1.0] * len(documents)])[0]\n",
        "\n",
        "    chunks = [\n",
        "        {\n",
        "            \"text\": doc,\n",
        "            \"metadata\": meta,\n",
        "            \"confidence\": max(0, 1 - (dist / 2))\n",
        "        }\n",
        "        for doc, meta, dist in zip(documents, metadatas, distances)\n",
        "    ]\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def knowledge_graph(query: str) -> str:\n",
        "    \"\"\"Enhanced knowledge graph mappings\"\"\"\n",
        "    kg_mappings = {\n",
        "        \"customer\": \"Company Name Referenced in Master Agreement Only\",\n",
        "        \"Customer\": \"Company Name Referenced in Master Agreement Only\",\n",
        "        \"name of the customer\": \"Company Name Referenced in Master Agreement Only\",\n",
        "        \"name of the Customer\": \"Company Name Referenced in Master Agreement Only\",\n",
        "        \"Master Agreement\": \"Contract\",\n",
        "        \"Service Order\": \"Service Agreement\",\n",
        "    }\n",
        "\n",
        "    enhanced_query = query\n",
        "    for term, replacement in kg_mappings.items():\n",
        "        pattern = re.compile(re.escape(term), re.IGNORECASE)\n",
        "        enhanced_query = pattern.sub(replacement, enhanced_query)\n",
        "\n",
        "    return enhanced_query\n",
        "\n",
        "def base_answer(query: str, chunks: List[Dict]) -> str:\n",
        "    \"\"\"Generate answer using retrieved chunks directly\"\"\"\n",
        "    context = \"\\n\\n\".join([chunk[\"text\"] for chunk in chunks])\n",
        "    prompt = f\"\"\"Based on the following context, answer the question concisely.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        answer = llm.invoke(prompt)\n",
        "    except AttributeError:\n",
        "        answer = llm.predict(prompt)\n",
        "\n",
        "    return answer.strip()\n",
        "\n",
        "def llm_decision_maker(query: str, chunks: List[Dict], initial_answer: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Let the LLM decide if query optimization is needed\n",
        "    \"\"\"\n",
        "    # Prepare chunks summary for LLM\n",
        "    chunks_summary = \"\\n\".join([\n",
        "        f\"Chunk {i+1} (Confidence: {chunk['confidence']:.2f}): {chunk['text'][:150]}...\"\n",
        "        for i, chunk in enumerate(chunks)\n",
        "    ])\n",
        "\n",
        "    avg_confidence = sum(chunk[\"confidence\"] for chunk in chunks) / len(chunks) if chunks else 0\n",
        "\n",
        "    decision_prompt = f\"\"\"You are an intelligent query analysis agent. Your task is to decide whether the initial answer is satisfactory or if the query needs to be enhanced using a knowledge graph.\n",
        "\n",
        "Original Query: {query}\n",
        "\n",
        "Retrieved Chunks:\n",
        "{chunks_summary}\n",
        "\n",
        "Average Confidence Score: {avg_confidence:.2f}\n",
        "\n",
        "Initial Answer Generated: {initial_answer}\n",
        "\n",
        "Analyze the situation and decide:\n",
        "\n",
        "1. Is the initial answer accurate and complete based on the retrieved chunks?\n",
        "2. Do the retrieved chunks contain sufficient relevant information to answer the query?\n",
        "3. Does the query contain terms that might benefit from knowledge graph mapping (e.g., \"customer\", \"Master Agreement\")?\n",
        "\n",
        "Respond in this EXACT format:\n",
        "DECISION: [DirectAnswer OR EnhancedAnswer]\n",
        "REASON: [Your detailed reasoning in one sentence]\n",
        "CONFIDENCE_IN_DECISION: [0.0 to 1.0]\n",
        "\n",
        "Examples:\n",
        "- If chunks are highly relevant and answer is good: \"DECISION: DirectAnswer\"\n",
        "- If chunks are vague or answer is unclear: \"DECISION: EnhancedAnswer\"\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        decision_response = llm.invoke(decision_prompt)\n",
        "    except AttributeError:\n",
        "        decision_response = llm.predict(decision_prompt)\n",
        "\n",
        "    # Parse LLM response\n",
        "    decision_lines = decision_response.strip().split('\\n')\n",
        "    decision = \"DirectAnswer\"  # Default\n",
        "    reason = \"Unable to parse decision\"\n",
        "    confidence = 0.5\n",
        "\n",
        "    for line in decision_lines:\n",
        "        if line.startswith(\"DECISION:\"):\n",
        "            decision_value = line.split(\"DECISION:\")[1].strip()\n",
        "            if \"EnhancedAnswer\" in decision_value or \"Enhanced\" in decision_value:\n",
        "                decision = \"EnhancedAnswer\"\n",
        "            else:\n",
        "                decision = \"DirectAnswer\"\n",
        "        elif line.startswith(\"REASON:\"):\n",
        "            reason = line.split(\"REASON:\")[1].strip()\n",
        "        elif line.startswith(\"CONFIDENCE_IN_DECISION:\"):\n",
        "            try:\n",
        "                confidence = float(line.split(\"CONFIDENCE_IN_DECISION:\")[1].strip())\n",
        "            except:\n",
        "                confidence = 0.5\n",
        "\n",
        "    return {\n",
        "        \"decision\": decision,\n",
        "        \"reason\": reason,\n",
        "        \"confidence\": confidence,\n",
        "        \"avg_chunk_confidence\": avg_confidence\n",
        "    }\n",
        "\n",
        "def enhanced_answer(query: str) -> str:\n",
        "    \"\"\"Generate answer using enhanced query with knowledge graph\"\"\"\n",
        "    enhanced_query = knowledge_graph(query)\n",
        "    enhanced_chunks = retrieve_chunks(enhanced_query)\n",
        "    context = \"\\n\\n\".join([chunk[\"text\"] for chunk in enhanced_chunks])\n",
        "\n",
        "    prompt = f\"\"\"Based on the following context, answer the question concisely.\n",
        "\n",
        "Original Question: {query}\n",
        "Enhanced Question: {enhanced_query}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        answer = llm.invoke(prompt)\n",
        "    except AttributeError:\n",
        "        answer = llm.predict(prompt)\n",
        "\n",
        "    return answer.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aaAiW68hiqJ"
      },
      "source": [
        "# üìù **Step 6: Query Interface with Autonomous Decision-Making**  \n",
        "\n",
        "This step introduces an **interactive query interface** that allows users to input their questions.  \n",
        "The AI agent **analyzes** the query, **retrieves relevant document chunks**, and **decides how to answer**  \n",
        "based on confidence scores. The decision-making is fully **autonomous**, ensuring **optimized responses**.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **How the Query Interface Works**  \n",
        "\n",
        "### üîπ **User Input & Query Submission**  \n",
        "- You have to enters their query in the input field (`query_input`).\n",
        "- Clicking the \"Submit\" button (`submit_btn`) triggers the `handle_query` function.\n",
        "\n",
        "# üîç Step-by-Step Query Processing Flow\n",
        "**1Ô∏è‚É£ Step 1: Query Asked**\n",
        "- The user inputs a query.\n",
        "- The query is displayed in a notification box for reference.\n",
        "\n",
        "**2Ô∏è‚É£ Step 2: Retrieve Relevant Chunks**\n",
        "- The query is searched in the document database (ChromaDB).\n",
        "- The most relevant text chunks are retrieved, along with their confidence scores.\n",
        "- Retrieved chunks are displayed in a green notification box.\n",
        "\n",
        "**3Ô∏è‚É£ Step 3: Generate an Initial (Poor) Answer**\n",
        "- The system tries to generate an answer from the retrieved chunks.\n",
        "- This is an unoptimized response that might be incorrect due to low confidence scores.\n",
        "- The poor answer is displayed in a red notification box.\n",
        "\n",
        "**4Ô∏è‚É£ Step 4: AI Agent Decision**\n",
        "- The AI agent analyzes the retrieved chunks and confidence scores.\n",
        "- It decides whether to:\n",
        "  - Use the DirectAnswer tool (if confidence scores are high).\n",
        "  - Use the EnhancedAnswer tool (if confidence scores are low).\n",
        "- The agent's decision is displayed in a magenta notification box.\n",
        "\n",
        "**5Ô∏è‚É£ Step 5: Generate the Final Answer**\n",
        "\n",
        "‚úÖ If the agent chooses \"DirectAnswer\"\n",
        "- The initial answer is used without modifications.\n",
        "\n",
        "‚úÖ If the agent chooses \"EnhancedAnswer\"\n",
        "- The query is refined by agnet using a Knowledge Graph.\n",
        "-A new set of relevant document chunks is retrieved.\n",
        "- A more optimized answer is generated.\n",
        "\n",
        "# üèÅ How to Use the Query Interface?\n",
        "1Ô∏è‚É£ Enter your question in the input box.\n",
        "\n",
        "```text\n",
        "What is the name of the Customer ?\n",
        "```\n",
        "\n",
        "2Ô∏è‚É£ Click \"Submit\" to process the query.\n",
        "\n",
        "3Ô∏è‚É£ The system will retrieve relevant document chunks and evaluate confidence scores.\n",
        "\n",
        "4Ô∏è‚É£ The AI agent decides whether to give a direct answer or enhance the query using a Knowledge Graph.\n",
        "\n",
        "5Ô∏è‚É£ The final answer is displayed, either as a Direct Answer or an Optimized Answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "506ad967899e422bbae36525a6cf4678",
            "c5ebb3d7aa2e4a4e99873fe3b2ba6db9",
            "2ac12568ae4d44a98fb2c790ece6687e",
            "f3ecb3df448740968e78572445c89ba4",
            "f4f513f582a845b5a40eb9dbb6e1b693",
            "fc0da9dec853475a9a35f19211fe873d",
            "252b725370974b2694a06f15d74ed497",
            "3279f2d0bc0448e2a4e4195605ec7e28"
          ]
        },
        "collapsed": true,
        "id": "JORFQMonVaFK",
        "outputId": "2a478369-f178-407a-87ec-cf97b97760a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ü§ñ INTELLIGENT QUERY PROCESSING SYSTEM WITH LLM DECISION MAKING\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', layout=Layout(width='50%'), placeholder='Enter your query')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "506ad967899e422bbae36525a6cf4678"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='primary', description='Submit', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3ecb3df448740968e78572445c89ba4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "252b725370974b2694a06f15d74ed497"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Step 6: Query interface with LLM-based autonomous decision-making\n",
        "query_input = widgets.Text(placeholder=\"Enter your query\", layout=widgets.Layout(width='50%'))\n",
        "submit_btn = widgets.Button(description=\"Submit\", button_style='primary')\n",
        "query_output = widgets.Output()\n",
        "\n",
        "def handle_query(b):\n",
        "    with query_output:\n",
        "        clear_output()\n",
        "        query = query_input.value\n",
        "\n",
        "        if not query.strip():\n",
        "            print(boxen(\"Please enter a query!\", title=\"Error\", color=\"red\"))\n",
        "            return\n",
        "\n",
        "        # Step 1: Query Asked\n",
        "        print(boxen(f\"QUERY ASKED:\\n{query}\", title=\"Step 1: Query Asked\", color=\"blue\"))\n",
        "\n",
        "        # Step 2: Retrieve Initial Chunks\n",
        "        chunks = retrieve_chunks(query)\n",
        "        chunks_display = \"\\n\\n\".join([\n",
        "            f\"Chunk {i+1} (Confidence: {chunk.get('confidence', 0):.2f}):\\n{chunk['text'][:200]}...\"\n",
        "            for i, chunk in enumerate(chunks)\n",
        "        ])\n",
        "        print(boxen(\n",
        "            f\"RETRIEVED CHUNKS:\\n{chunks_display}\",\n",
        "            title=\"Step 2: Retrieved Chunks\", color=\"green\"\n",
        "        ))\n",
        "\n",
        "        # Step 3: Generate Initial Answer\n",
        "        initial_answer = base_answer(query, chunks)\n",
        "        avg_confidence = sum(chunk.get(\"confidence\", 0) for chunk in chunks) / len(chunks) if chunks else 0\n",
        "\n",
        "        print(boxen(\n",
        "            f\"INITIAL ANSWER:\\n{initial_answer}\\n\\nAverage Confidence: {avg_confidence:.2f}\",\n",
        "            title=\"Step 3: Initial Answer\", color=\"yellow\"\n",
        "        ))\n",
        "\n",
        "        # Step 4: LLM Makes Decision\n",
        "        print(boxen(\n",
        "            \"ü§ñ LLM ANALYZING RESULTS AND MAKING DECISION...\",\n",
        "            title=\"Step 4: LLM Decision Making\", color=\"cyan\"\n",
        "        ))\n",
        "\n",
        "        decision_result = llm_decision_maker(query, chunks, initial_answer)\n",
        "\n",
        "        decision_display = f\"\"\"DECISION: {decision_result['decision']}\n",
        "\n",
        "REASONING: {decision_result['reason']}\n",
        "\n",
        "LLM CONFIDENCE IN DECISION: {decision_result['confidence']:.2f}\n",
        "RETRIEVAL CONFIDENCE: {decision_result['avg_chunk_confidence']:.2f}\"\"\"\n",
        "\n",
        "        color = \"magenta\" if decision_result['decision'] == \"EnhancedAnswer\" else \"green\"\n",
        "        print(boxen(decision_display, title=\"Step 4: LLM Decision\", color=color))\n",
        "\n",
        "        # Step 5: Execute Based on LLM Decision\n",
        "        if decision_result['decision'] == \"DirectAnswer\":\n",
        "            print(boxen(\n",
        "                f\"‚úì LLM DETERMINED INITIAL ANSWER IS SATISFACTORY\\n\\nFINAL ANSWER:\\n{initial_answer}\",\n",
        "                title=\"Step 5: Final Answer (Direct)\", color=\"green\"\n",
        "            ))\n",
        "\n",
        "        elif decision_result['decision'] == \"EnhancedAnswer\":\n",
        "            print(boxen(\n",
        "                \"üîß LLM DETERMINED QUERY NEEDS OPTIMIZATION\\nAPPLYING KNOWLEDGE GRAPH...\",\n",
        "                title=\"Step 5a: Knowledge Graph Processing\", color=\"cyan\"\n",
        "            ))\n",
        "\n",
        "            # Show enhancement\n",
        "            enhanced_query = knowledge_graph(query)\n",
        "\n",
        "            if enhanced_query != query:\n",
        "                print(boxen(\n",
        "                    f\"ORIGINAL QUERY:\\n{query}\\n\\nENHANCED QUERY:\\n{enhanced_query}\",\n",
        "                    title=\"Step 5b: Query Enhancement\", color=\"cyan\"\n",
        "                ))\n",
        "            else:\n",
        "                print(boxen(\n",
        "                    \"No knowledge graph mappings applied.\",\n",
        "                    title=\"Step 5b: Query Enhancement\", color=\"cyan\"\n",
        "                ))\n",
        "\n",
        "            # Retrieve with enhanced query\n",
        "            enhanced_chunks = retrieve_chunks(enhanced_query)\n",
        "            enhanced_chunks_display = \"\\n\\n\".join([\n",
        "                f\"Chunk {i+1} (Confidence: {chunk.get('confidence', 0):.2f}):\\n{chunk['text'][:200]}...\"\n",
        "                for i, chunk in enumerate(enhanced_chunks)\n",
        "            ])\n",
        "\n",
        "            print(boxen(\n",
        "                f\"NEW RETRIEVED CHUNKS:\\n{enhanced_chunks_display}\",\n",
        "                title=\"Step 5c: Enhanced Retrieval\", color=\"cyan\"\n",
        "            ))\n",
        "\n",
        "            # Generate final answer\n",
        "            final_answer = base_answer(enhanced_query, enhanced_chunks)\n",
        "            new_confidence = sum(chunk.get(\"confidence\", 0) for chunk in enhanced_chunks) / len(enhanced_chunks) if enhanced_chunks else 0\n",
        "\n",
        "            print(boxen(\n",
        "                f\"FINAL OPTIMIZED ANSWER:\\n{final_answer}\\n\\nImproved Confidence: {new_confidence:.2f} (was {avg_confidence:.2f})\",\n",
        "                title=\"Step 5d: Final Optimized Answer\", color=\"green\"\n",
        "            ))\n",
        "\n",
        "submit_btn.on_click(handle_query)\n",
        "\n",
        "# Display interface\n",
        "print(\"=\" * 80)\n",
        "print(\"ü§ñ INTELLIGENT QUERY PROCESSING SYSTEM WITH LLM DECISION MAKING\")\n",
        "print(\"=\" * 80)\n",
        "display(query_input, submit_btn, query_output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "499bb7ca97e441cab66ba9ee0a36964c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".pdf",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_5ad4d69b621947d496110cb50a515554",
            "metadata": [
              {
                "name": "Morningstar Inc MSA (1).pdf",
                "type": "application/pdf",
                "size": 271641,
                "lastModified": 1762953854157
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_723e2d321c0a4cd6b35c8d96e34801b7"
          }
        },
        "5ad4d69b621947d496110cb50a515554": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "723e2d321c0a4cd6b35c8d96e34801b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "9924cde225394254a295a2670985d796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Process File",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_f8f439f9a3aa4715aff5e952656e0ad8",
            "style": "IPY_MODEL_79965054e08443c0b7eb5d526f9f2264",
            "tooltip": ""
          }
        },
        "f8f439f9a3aa4715aff5e952656e0ad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79965054e08443c0b7eb5d526f9f2264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "98a38c5e05624220a6f915633d04d22a": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_1e65e7b720914b0da668938d9ee79f3e",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                },
                "metadata": {}
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "\u001b[32m‚ï≠‚îÄ\u001b[0m\u001b[32m Success \u001b[0m\u001b[32m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[32m‚îÄ‚ïÆ\u001b[0m                                                                           \n",
                  "\u001b[32m‚îÇ\u001b[0mFile processed with confidence scores!\u001b[32m‚îÇ\u001b[0m                                                                           \n",
                  "\u001b[32m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m                                                                           \n",
                  "\n"
                ]
              }
            ]
          }
        },
        "1e65e7b720914b0da668938d9ee79f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "506ad967899e422bbae36525a6cf4678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c5ebb3d7aa2e4a4e99873fe3b2ba6db9",
            "placeholder": "Enter your query",
            "style": "IPY_MODEL_2ac12568ae4d44a98fb2c790ece6687e",
            "value": "What is the name of the Customer ?"
          }
        },
        "c5ebb3d7aa2e4a4e99873fe3b2ba6db9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "2ac12568ae4d44a98fb2c790ece6687e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3ecb3df448740968e78572445c89ba4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Submit",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_f4f513f582a845b5a40eb9dbb6e1b693",
            "style": "IPY_MODEL_fc0da9dec853475a9a35f19211fe873d",
            "tooltip": ""
          }
        },
        "f4f513f582a845b5a40eb9dbb6e1b693": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc0da9dec853475a9a35f19211fe873d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "252b725370974b2694a06f15d74ed497": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3279f2d0bc0448e2a4e4195605ec7e28",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                },
                "metadata": {}
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "\u001b[34m‚ï≠‚îÄ\u001b[0m\u001b[34m Step 1: Query Asked \u001b[0m\u001b[34m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[34m‚îÄ‚ïÆ\u001b[0m                                                                               \n",
                  "\u001b[34m‚îÇ\u001b[0mQUERY ASKED:                      \u001b[34m‚îÇ\u001b[0m                                                                               \n",
                  "\u001b[34m‚îÇ\u001b[0mWhat is the name of the Customer ?\u001b[34m‚îÇ\u001b[0m                                                                               \n",
                  "\u001b[34m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m                                                                               \n",
                  "\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                },
                "metadata": {}
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "\u001b[32m‚ï≠‚îÄ\u001b[0m\u001b[32m Step 2: Retrieved Chunks \u001b[0m\u001b[32m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[32m‚îÄ‚ïÆ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m RETRIEVED CHUNKS:                                                                                               \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m Chunk 1 (Confidence: 0.42):                                                                                     \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m purpose described in this Master Agreement and that Product License Agreement; and/or                           \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m (2)Execute one or more Morningstar Service Orders (each a \"Service Order'') by which Morningstar Provider       \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m makes av...                                                                                                     \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m                                                                                                                 \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m Chunk 2 (Confidence: 0.40):                                                                                     \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m and their respective successors and assigns, and their respective directors, officers and employees, from and   \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m against any                                                                                                     \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m and all claims, demands, suits, action and shall pay any and all Losses incurr...                               \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m                                                                                                                 \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m Chunk 3 (Confidence: 0.37):                                                                                     \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m Morningstar Provider's cost, the Product. the Special Services or any portion thereof, with a substitute        \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m product/service                                                                                                 \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚îÇ\u001b[0m that functions substantially in accordance with the Product's specifications; (...                              \u001b[32m‚îÇ\u001b[0m\n",
                  "\u001b[32m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m\n",
                  "\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                },
                "metadata": {}
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "\u001b[33m‚ï≠‚îÄ\u001b[0m\u001b[33m Step 3: Initial Answer \u001b[0m\u001b[33m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[33m‚îÄ‚ïÆ\u001b[0m                                                                   \n",
                  "\u001b[33m‚îÇ\u001b[0mINITIAL ANSWER:                               \u001b[33m‚îÇ\u001b[0m                                                                   \n",
                  "\u001b[33m‚îÇ\u001b[0mThe name of the Customer is Company Recipient.\u001b[33m‚îÇ\u001b[0m                                                                   \n",
                  "\u001b[33m‚îÇ\u001b[0m                                              \u001b[33m‚îÇ\u001b[0m                                                                   \n",
                  "\u001b[33m‚îÇ\u001b[0mAverage Confidence: 0.40                      \u001b[33m‚îÇ\u001b[0m                                                                   \n",
                  "\u001b[33m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m                                                                   \n",
                  "\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                },
                "metadata": {}
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "\u001b[36m‚ï≠‚îÄ\u001b[0m\u001b[36m Step 4: LLM Decision Making \u001b[0m\u001b[36m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[36m‚îÄ‚ïÆ\u001b[0m                                                                  \n",
                  "\u001b[36m‚îÇ\u001b[0mü§ñ LLM ANALYZING RESULTS AND MAKING DECISION...\u001b[36m‚îÇ\u001b[0m                                                                  \n",
                  "\u001b[36m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m                                                                  \n",
                  "\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                },
                "metadata": {}
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "\u001b[35m‚ï≠‚îÄ\u001b[0m\u001b[35m Step 4: LLM Decision \u001b[0m\u001b[35m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[35m‚îÄ‚ïÆ\u001b[0m\n",
                  "\u001b[35m‚îÇ\u001b[0mDECISION: EnhancedAnswer                                                                                         \u001b[35m‚îÇ\u001b[0m\n",
                  "\u001b[35m‚îÇ\u001b[0m                                                                                                                 \u001b[35m‚îÇ\u001b[0m\n",
                  "\u001b[35m‚îÇ\u001b[0mREASONING: The retrieved chunks do not directly answer the query and contain irrelevant information, indicating  \u001b[35m‚îÇ\u001b[0m\n",
                  "\u001b[35m‚îÇ\u001b[0mthat the query may benefit from knowledge graph mapping.                                                         \u001b[35m‚îÇ\u001b[0m\n",
                  "\u001b[35m‚îÇ\u001b[0m                                                                                                                 \u001b[35m‚îÇ\u001b[0m\n",
                  "\u001b[35m‚îÇ\u001b[0mLLM CONFIDENCE IN DECISION: 0.80                                                                                 \u001b[35m‚îÇ\u001b[0m\n",
                  "\u001b[35m‚îÇ\u001b[0mRETRIEVAL CONFIDENCE: 0.40                                                                                       \u001b[35m‚îÇ\u001b[0m\n",
                  "\u001b[35m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m\n",
                  "\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                },
                "metadata": {}
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "\u001b[36m‚ï≠‚îÄ\u001b[0m\u001b[36m Step 5a: Knowledge Graph Processing \u001b[0m\u001b[36m‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[36m‚îÄ‚ïÆ\u001b[0m                                                                       \n",
                  "\u001b[36m‚îÇ\u001b[0müîß LLM DETERMINED QUERY NEEDS OPTIMIZATION\u001b[36m‚îÇ\u001b[0m                                                                       \n",
                  "\u001b[36m‚îÇ\u001b[0mAPPLYING KNOWLEDGE GRAPH...               \u001b[36m‚îÇ\u001b[0m                                                                       \n",
                  "\u001b[36m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m                                                                       \n",
                  "\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                },
                "metadata": {}
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "\u001b[36m‚ï≠‚îÄ\u001b[0m\u001b[36m Step 5b: Query Enhancement \u001b[0m\u001b[36m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[36m‚îÄ‚ïÆ\u001b[0m                                               \n",
                  "\u001b[36m‚îÇ\u001b[0mORIGINAL QUERY:                                                   \u001b[36m‚îÇ\u001b[0m                                               \n",
                  "\u001b[36m‚îÇ\u001b[0mWhat is the name of the Customer ?                                \u001b[36m‚îÇ\u001b[0m                                               \n",
                  "\u001b[36m‚îÇ\u001b[0m                                                                  \u001b[36m‚îÇ\u001b[0m                                               \n",
                  "\u001b[36m‚îÇ\u001b[0mENHANCED QUERY:                                                   \u001b[36m‚îÇ\u001b[0m                                               \n",
                  "\u001b[36m‚îÇ\u001b[0mWhat is the name of the Company Name Referenced in Contract Only ?\u001b[36m‚îÇ\u001b[0m                                               \n",
                  "\u001b[36m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m                                               \n",
                  "\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                },
                "metadata": {}
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "\u001b[36m‚ï≠‚îÄ\u001b[0m\u001b[36m Step 5c: Enhanced Retrieval \u001b[0m\u001b[36m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[36m‚îÄ‚ïÆ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m NEW RETRIEVED CHUNKS:                                                                                           \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m Chunk 1 (Confidence: 0.45):                                                                                     \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m MUTUAL  OF AMERICA                                                                                              \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m 1. Provision of Products and/or Special Services                                                                \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m (a)From time to time, Morningstar or one of its majority-owned subsidiaries (such entity, hereafter, the        \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m \"Morningstar Provider\") and...                                                                                  \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m                                                                                                                 \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m Chunk 2 (Confidence: 0.43):                                                                                     \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m and their respective successors and assigns, and their respective directors, officers and employees, from and   \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m against any                                                                                                     \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m and all claims, demands, suits, action and shall pay any and all Losses incurr...                               \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m                                                                                                                 \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m Chunk 3 (Confidence: 0.43):                                                                                     \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m maximum extent provided by law. Neither this Master Agreement nor any Agreement executed under it creates a     \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚îÇ\u001b[0m partnership, joint venture, employment or other form of agency relationship between the part...                 \u001b[36m‚îÇ\u001b[0m\n",
                  "\u001b[36m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m\n",
                  "\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                },
                "metadata": {}
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "\u001b[32m‚ï≠‚îÄ\u001b[0m\u001b[32m Step 5d: Final Optimized Answer \u001b[0m\u001b[32m‚îÄ\u001b[0m\u001b[32m‚îÄ‚ïÆ\u001b[0m                                                                             \n",
                  "\u001b[32m‚îÇ\u001b[0mFINAL OPTIMIZED ANSWER:             \u001b[32m‚îÇ\u001b[0m                                                                             \n",
                  "\u001b[32m‚îÇ\u001b[0mMorningstar, Inc.                   \u001b[32m‚îÇ\u001b[0m                                                                             \n",
                  "\u001b[32m‚îÇ\u001b[0m                                    \u001b[32m‚îÇ\u001b[0m                                                                             \n",
                  "\u001b[32m‚îÇ\u001b[0mImproved Confidence: 0.44 (was 0.40)\u001b[32m‚îÇ\u001b[0m                                                                             \n",
                  "\u001b[32m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m                                                                             \n",
                  "\n"
                ]
              }
            ]
          }
        },
        "3279f2d0bc0448e2a4e4195605ec7e28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}