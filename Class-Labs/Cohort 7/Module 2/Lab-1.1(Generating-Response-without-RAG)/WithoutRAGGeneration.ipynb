{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6vMCaTzdu6a"
      },
      "source": [
        "# In this Notebook we will learn how we can use different documents and get them analysed by GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VROza_wGd4if"
      },
      "source": [
        "## First of all we will install the required packages that will help us along the way\n",
        "1. openai: This package will help us to call the chat completion method of openai to generate results using GPT.\n",
        "3. PyMuPDF: This package is used for easy PDF manipulation.\n",
        "4. tiktoken: This package is used to calculate the tokens in a text\n",
        "\n",
        "### üìå Prerequisites  \n",
        "\n",
        "Please download and review the following documents before proceeding:  \n",
        "\n",
        "1. **AWS1.pdf**  \n",
        "   [Download Link](https://drive.google.com/file/d/1XSe2pSsGN1ssAbif92rvb80AnHb_Ni0F/view?usp=sharing)  \n",
        "\n",
        "2. **PROFRAC HOLDINGS, LLC Credit Agreement.pdf**  \n",
        "   [Download Link](https://drive.google.com/file/d/1UyOxeaEQsK5TFxXHI63PshoKmj0yjTmW/view?usp=sharing)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "Cgny4E8V8NGU",
        "outputId": "7c3a95f1-e793-4fee-a745-ddcb4b2896a7"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "!pip install PyMuPDF==1.24.2 PyMuPDFb==1.24.1 tqdm tiktoken\n",
        "! pip install openai==1.55.3 httpx==0.27.2 --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pKRpv0MSNnln",
        "outputId": "d54d08e7-eafd-4656-8df9-ac1cc7fe4121"
      },
      "outputs": [],
      "source": [
        "! pip install openai httpx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20WwGnF0kLPB"
      },
      "source": [
        "# Make Sure to Place Your OPEN API KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "H6Z8_eO38qlS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import fitz\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import tiktoken\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(api_key=\"Place Your OpenAi API Key Here\")\n",
        "\n",
        "\n",
        "token_encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErbsjVzCgLcG"
      },
      "source": [
        "Here we have created a function that is called for generating response from OpenAI\n",
        "\n",
        "This loads the keys from the environment and uses it to call the `openai.chat.completions.create` method\n",
        "\n",
        "This method takes the system message and the user message as input for generating response.\n",
        "\n",
        "### The temperature parameter defines the randomness of the output,\n",
        "\n",
        "Higher the temperature the more creative the LLM will become with its answers, thus higher temperatures are used for poem generation, jokes etc.\n",
        "Lower temperature gives deterministic results and return the most probable next token.\n",
        "\n",
        "### Top P\n",
        "\n",
        "A sampling technique with temperature, called nucleus sampling, where you can control how deterministic the model is. If you are looking for exact and factual answers keep this low. If you are looking for more diverse responses, increase to a higher value. If you use Top P it means that only the tokens comprising the top_p probability mass are considered for responses, so a low top_p value selects the most confident responses. This means that a high top_p value will enable the model to look at more possible words, including less likely ones, leading to more diverse outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UXJBE_y19pCq"
      },
      "outputs": [],
      "source": [
        "def CallOpenAI(user,system):\n",
        "  response = client.chat.completions.create(\n",
        "              model= \"gpt-3.5-turbo\", # model = \"deployment_name\".\n",
        "              temperature= 0,\n",
        "              top_p= 0,\n",
        "              messages=[\n",
        "                  {\"role\": \"system\", \"content\": system},\n",
        "                  {\"role\": \"user\", \"content\": user}\n",
        "              ]\n",
        "          )\n",
        "  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2YvMRDm-Fx5"
      },
      "source": [
        "## Lets take a contract and try to analyse it without much instruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-JwKDwhgpvm"
      },
      "source": [
        "First we load the PDF and extract the texts from it and generate the token count of the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sI9uVaob_7Ie"
      },
      "outputs": [],
      "source": [
        "def extract_text(pdf_path):\n",
        "  pdf = fitz.open(pdf_path)\n",
        "  text = ''\n",
        "\n",
        "  for page in pdf:\n",
        "    text += page.get_text()\n",
        "\n",
        "  num_tokens = len(token_encoding.encode(text))\n",
        "  print(\"Number of tokens in the entire Document: \", num_tokens)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtScsyS8gz0r"
      },
      "source": [
        "Out here we can see the token count of the document is 11590 which is well withing the 16000 context limit of the GPT-3.5 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj_16PHCKS1R"
      },
      "source": [
        "**‚ö†Ô∏è Note:** **In the cell below, you need to upload a file named `AWS1.pdf`.**  \n",
        "**You can download the file from the link below.**\n",
        "[üì• Download AWS1.pdf](https://drive.google.com/file/d/1XSe2pSsGN1ssAbif92rvb80AnHb_Ni0F/view?usp=sharing)/Lab-2.1(Generating-Response-without-RAG)/AWS1.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aTg1NEj_H83Q"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-vel8SEHS_C",
        "outputId": "7e4963d5-b97d-4ff3-9541-df903ba42127"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens in the entire Document:  11590\n"
          ]
        }
      ],
      "source": [
        "short_document = extract_text(\"/content/AWS1.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtnxFpZyg9gr"
      },
      "source": [
        "## We concatenate the text from the PDF and the question that the user wants to ask to the GPT about the PDF and form a prompt that we will use to generate the response using `openai.chat.completion.create` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PJlC7zoDHmpV"
      },
      "outputs": [],
      "source": [
        "Question = \"What is the governing courts for Amazon Web Services South Africa ProprietaryLimited\"\n",
        "\n",
        "full_prompt_SD = \"<Context>\"+short_document+\"</Context>\" +\"\\n\\n\" +\"<Question>\"+Question+\"</Question>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "097gI1cFKWHu"
      },
      "outputs": [],
      "source": [
        "response = CallOpenAI(full_prompt_SD,\"You are a Professional lawyer who can analyse documents thorougly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT-ynR-nhSdx"
      },
      "source": [
        "### We can see that the GPT was able to generate the answer by refering to the prompt and give the correct result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZjJRGvhK_Ra",
        "outputId": "67c99ac6-122d-4ccb-bb0d-f71ac05d73d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The governing courts for Amazon Web Services South Africa Proprietary Limited are the South Gauteng High Court in Johannesburg, South Africa. This information is specified in the document under the \"AWS Contracting Party\" section for South Africa. The document outlines the laws and courts applicable to each AWS Contracting Party based on the Account Country associated with the AWS account.\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_rAndtLKzTq"
      },
      "source": [
        "## Now lets load up a document that has more than 16000 tokens, which is the limit of GPT-3.5-Turbo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0gbRdAnK9F3"
      },
      "source": [
        "**‚ö†Ô∏è Note:** **In the cell below, you need to upload a file named `PROFRAC HOLDINGS, LLC credit agreement.pdf`.**  \n",
        "**You can download the file from the link below.**\n",
        "[üì• Download PROFRAC HOLDINGS, LLC credit agreement.pdf ](https://drive.google.com/file/d/1UyOxeaEQsK5TFxXHI63PshoKmj0yjTmW/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwqltHhiK_gT"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja6FHWxeAiKo",
        "outputId": "65c1740c-9511-4d09-84b5-f251bf8080c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens in the entire Document:  163227\n"
          ]
        }
      ],
      "source": [
        "long_document = extract_text(\"/content/PROFRAC HOLDINGS, LLC credit agreement.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dfpmAVqmKCXM"
      },
      "outputs": [],
      "source": [
        "Question = \"What is the Acknowledgement Regarding Any Supported QFCs?\"\n",
        "\n",
        "full_prompt_LD = \"<Context>\"+long_document+\"</Context>\" +\"\\n\\n\" +\"<Question>\"+Question+\"</Question>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve7bc33tLygQ"
      },
      "source": [
        "## Here what you see is, when the message length exceeded the limit of GPT, it throws an error.\n",
        "### This problem will be fixed in the next lab where you see how Retrieval Augmented Generation(RAG) will fix this problem and enable us to analyse documents of any length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBWHfMohKxKw",
        "outputId": "f9ef3d6b-9528-4d3c-f38a-c74f1cd8f99c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error code: 401 - {'error': {'message': 'Incorrect API key provided: Place Yo******************Here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  response = CallOpenAI(full_prompt_LD,\"You are a Professional lawyer who can analyse documents thorougly\")\n",
        "except Exception as e:\n",
        "  print(str(e))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
