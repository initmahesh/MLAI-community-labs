{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHkGbKJFexWM"
      },
      "source": [
        "# LLM-as-a-Judge with Advanced Azure AI Evaluation\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](<https://colab.research.google.com/github/sachin0034/MLAI-community-labs/blob/main/Class-Labs/Lab-14(LLM-Judge-lab)/Lab-2(LLM-as-a-Judge%20with%20Advanced%20Azure%20AI%20Evaluation)/LLM_as_a_Judge_with_Advanced_Azure_AI_Evaluation.ipynb>)\n",
        "\n",
        "In previous iterations of our Legal Document Analyzer, we leveraged OpenAI models for both the extraction of key terms from legal contracts and their subsequent evaluation. As demonstrated in our prior lab, this approach provided initial insights into the capabilities of large language models for legal text analysis. Building upon that foundation, this current project significantly enhances our evaluation methodology by integrating pre-trained evaluators from Azure AI. This shift allows us to utilize specialized, robust metrics for assessing the quality of our LLM's extractions, specifically focusing on aspects like groundedness, coherence, relevance, and fluency, thereby providing a more comprehensive and nuanced understanding of model performance.\n",
        "\n",
        "# Intro to Azure AI Evaluation\n",
        "Azure AI evaluations are a set of tools and features within Azure AI Studio designed to assess the performance and quality of generative AI models and applications. They provide a structured way to measure various aspects of AI responses, including accuracy, groundedness, and safety, using both built-in and customizable metrics.\n",
        "\n",
        "[Read more about Azure AI Evaluation](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/evaluate-sdk)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l07wygtljVy5"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "> ‚ö†Ô∏è **Note:** Make sure you have an active **Azure subscription account**. Without it, you won't be able to run this notebook with Azure integrations.  \n",
        "\n",
        "---\n",
        "\n",
        "Before you get started, please make sure you have the following ready:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Sample Contract File for Testing\n",
        "\n",
        "To try out the contract analysis workflow, download the sample contract file provided below:\n",
        "\n",
        "- [Download Sample Contract (Google Drive)](https://drive.google.com/file/d/1E557kdNBZ5cDUvVDLNrEVRuKcRSYDG3Z/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. OpenAI API Key\n",
        "\n",
        "You‚Äôll need your own OpenAI API key to access the language models used for contract evaluation. If you don‚Äôt have one yet, follow this step-by-step guide to generate your API key:\n",
        "\n",
        "- [How to get your own OpenAI API key (Medium article)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Azure Setup (for Azure OpenAI / Azure AI Foundry Users)\n",
        "\n",
        "If you're using Azure services, ensure you have the following:\n",
        "\n",
        "- ‚úÖ **An active Azure subscription**\n",
        "- ‚úÖ **Sufficient Azure portal permissions** (Contributor, Cognitive Services Contributor, or Owner at the subscription level)\n",
        "- ‚úÖ **An Azure AI Foundry resource or an Azure OpenAI resource deployed**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì¶ Installation\n",
        "\n",
        "Install all the required packages using the following command:\n",
        "\n",
        "#### ‚úÖ Package Breakdown\n",
        "\n",
        "| Package                  | Purpose                                                                 |\n",
        "|--------------------------|-------------------------------------------------------------------------|\n",
        "| `langchain`              | Framework for developing LLM-powered applications                      |\n",
        "| `pypdf`                  | Extract text from PDF documents                                        |\n",
        "| `docx2txt`               | Read and convert `.docx` files to plain text                           |\n",
        "| `pandas`                 | Data manipulation and table formatting                                 |\n",
        "| `openai`                 | Access to OpenAI‚Äôs GPT models                                          |\n",
        "| `gradio`                 | Frontend interface to run your LLM app interactively                   |\n",
        "| `azure-ai-generative`    | Azure SDK for working with generative AI models                        |\n",
        "| `langchain-community`    | Community-contributed integrations and tools for LangChain             |\n",
        "| `azure-ai-evaluation`    | Evaluation tools for scoring LLM responses                            |\n",
        "| `azure-ai-projects`      | Tools to manage and structure LLM workflows in Azure                   |\n",
        "| `semantic-kernel`        | SDK for integrating AI models with symbolic reasoning & memory         |\n",
        "\n"
      ],
      "metadata": {
        "id": "Te5vALMeO_9t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TQewGxC4_EYu"
      },
      "outputs": [],
      "source": [
        "! pip install langchain pypdf docx2txt pandas openai gradio azure-ai-generative langchain-community azure-ai-evaluation  azure-ai-projects semantic-kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HnT_VmDaQmRz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import tempfile\n",
        "import re\n",
        "import json\n",
        "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pandas as pd\n",
        "from langchain.schema import Document\n",
        "from openai import OpenAI\n",
        "from openai import APIError, APIConnectionError, RateLimitError\n",
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "# Azure AI Evaluation imports\n",
        "from azure.ai.evaluation import GroundednessEvaluator, CoherenceEvaluator, RelevanceEvaluator, FluencyEvaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Configuration Setup\n",
        "\n",
        "### 1. üîë OpenAI API Key\n",
        "\n",
        "To use the language models for contract evaluation, you‚Äôll need your own OpenAI API key.\n",
        "\n",
        "If you haven‚Äôt generated one yet, follow this guide:\n",
        "\n",
        "- üëâ [How to get your own OpenAI API key (Medium article)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. ‚òÅÔ∏è Azure Configuration (for Azure OpenAI / Azure AI Foundry Users)\n",
        "\n",
        "If you're integrating with Azure services, ensure the following:\n",
        "\n",
        "- ‚úÖ **An active Azure subscription**\n",
        "- ‚úÖ **Proper permissions in the Azure portal** (Contributor, Cognitive Services Contributor, or Owner at subscription level)\n",
        "- ‚úÖ **An Azure AI Foundry or Azure OpenAI resource deployed**\n",
        "\n",
        "> **‚ö†Ô∏è Note:** Azure OpenAI and Foundry services **require a premium Azure subscription**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìò Reference Documentation\n",
        "\n",
        "**Azure AI Foundry Setup:**\n",
        "- [Create an Azure AI Foundry Project](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/create-projects?tabs=ai-foundry&pivots=fdp-project)\n",
        "\n",
        "\n",
        "**Azure OpenAI Configuration:**\n",
        "- [Azure OpenAI API](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/reference)\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "zlee4pbSPmNX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "815aPc_IQqHM"
      },
      "outputs": [],
      "source": [
        "# Initialize OpenAI client\n",
        "\n",
        "# Get your open api key from here : https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327\n",
        "client = OpenAI(api_key='Insert Your API Key')\n",
        "\n",
        "# Initialize Azure AI project and Azure OpenAI connection with your environment variables\n",
        "# Generate Below Keys : https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/create-projects?tabs=ai-foundry&pivots=fdp-project\n",
        "azure_ai_project = {\n",
        "    \"subscription_id\": \"Insert your subscription_id\",\n",
        "    \"resource_group_name\": \"Insert your resource_group_name\",\n",
        "    \"project_name\": \"Insert your project_name\",\n",
        "}\n",
        "\n",
        "# Generate Below Keys : https://learn.microsoft.com/en-us/azure/ai-foundry/openai/reference\n",
        "model_config = {\n",
        "    \"azure_endpoint\": \"Insert your azure_endpoint\",\n",
        "    \"api_key\": \"Insert your api_key\",\n",
        "    \"azure_deployment\": \"Insert your azure_deployment\",\n",
        "    \"api_version\": \"Insert your api_version\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Azure Evaluators Initialization\n",
        "\n",
        "We are using Azure‚Äôs **predefined evaluators**, which are already trained on relevant prompt-response pairs. These evaluators analyze the LLM-generated outputs and return evaluation scores directly ‚Äî no custom prompt engineering is needed.\n",
        "\n",
        "The following evaluators are initialized:\n",
        "\n",
        "- **Groundedness** ‚Äì Checks if the response is factual and based on the source content.\n",
        "- **Coherence** ‚Äì Assesses the logical flow and structure of the response.\n",
        "- **Relevance** ‚Äì Determines if the response accurately addresses the specific key term.\n",
        "- **Fluency** ‚Äì Evaluates grammar, clarity, and language quality.\n",
        "\n",
        "---\n",
        "\n",
        "### üîë Key Terms to Extract & Evaluate\n",
        "\n",
        "These predefined key terms are extracted from the contract and evaluated by the LLM:\n",
        "\n",
        "- Service Warranty  \n",
        "- Limitation of Liability  \n",
        "- Governing Law  \n",
        "- Termination for Cause  \n",
        "- Payment Terms  \n",
        "- Confidentiality Obligations\n"
      ],
      "metadata": {
        "id": "diuLBY_2Se_b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "OSyoUqzoQsyK"
      },
      "outputs": [],
      "source": [
        "# Initialize Azure evaluators\n",
        "groundedness_eval = GroundednessEvaluator(model_config=model_config)\n",
        "coherence_eval = CoherenceEvaluator(model_config=model_config)\n",
        "relevance_eval = RelevanceEvaluator(model_config=model_config)\n",
        "fluency_eval = FluencyEvaluator(model_config=model_config)\n",
        "\n",
        "KEY_TERMS = [\n",
        "    \"Service Warranty\",\n",
        "    \"Limitation of Liability\",\n",
        "    \"Governing Law\",\n",
        "    \"Termination for Cause\",\n",
        "    \"Payment Terms\",\n",
        "    \"Confidentiality Obligations\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìÑ Document Loading\n",
        "\n",
        "#### üìÇ Extracting Text from Files\n",
        "\n",
        "The `extract_text_from_file` function supports **multiple document formats** and handles file parsing based on the extension:\n",
        "\n",
        "- **PDF**: Uses `PyPDFLoader` to extract content from each page.\n",
        "- **DOCX/DOC**: Uses `Docx2txtLoader` to retrieve structured text from Word documents.\n",
        "- **TXT**: Uses `TextLoader` to process plain text files.\n",
        "- **CSV**: Uses `pandas` to read tabular data and convert it into string format for LLM consumption.\n",
        "\n",
        "The function returns both the **raw text** and a list of `Document` objects, which can be used for downstream evaluation or processing.\n"
      ],
      "metadata": {
        "id": "xHRfkhbQSt4N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "T_LLea9sQzwo"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_file(file_path):\n",
        "    \"\"\"\n",
        "    Extracts text from various file types with improved error handling.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the extracted text (str) and a list of Document objects.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the file type is unsupported.\n",
        "        Exception: For errors during file reading or processing.\n",
        "    \"\"\"\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    try:\n",
        "        if ext == \".pdf\":\n",
        "            loader = PyPDFLoader(file_path)\n",
        "            docs = loader.load()\n",
        "            text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        elif ext in [\".docx\", \".doc\"]:\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "            docs = loader.load()\n",
        "            text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        elif ext in [\".txt\"]:\n",
        "            loader = TextLoader(file_path)\n",
        "            docs = loader.load()\n",
        "            text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        elif ext == \".csv\":\n",
        "            df = pd.read_csv(file_path)\n",
        "            text = df.to_string(index=False)\n",
        "            docs = [Document(page_content=text, metadata={\"page\": \"N/A\"})]\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file type\")\n",
        "        return text, docs\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        raise\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"Error: CSV file is empty or malformed at {file_path}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the file {file_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "def chunk_document(docs, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Splits a list of Document objects into smaller chunks.\n",
        "\n",
        "    Args:\n",
        "        docs (list): A list of Langchain Document objects.\n",
        "        chunk_size (int): The maximum size of each chunk in characters.\n",
        "        chunk_overlap (int): The number of characters to overlap between chunks.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of smaller Langchain Document chunks.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîÅ Why We Use `query_llm_with_retry`\n",
        "\n",
        "We use this function to **ensure reliable communication with the LLM**. APIs can fail due to rate limits, network issues, or temporary service disruptions. This retry mechanism helps prevent crashes and improves the robustness of the application by automatically retrying failed requests.\n"
      ],
      "metadata": {
        "id": "DEo10r8_S915"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Cb3DgJ1yQ3tu"
      },
      "outputs": [],
      "source": [
        "def query_llm_with_retry(messages, model=\"gpt-4o-mini\", max_retries=3, delay=1):\n",
        "    \"\"\"\n",
        "    Sends a query to the LLM with retry logic for API errors.\n",
        "\n",
        "    Args:\n",
        "        messages (list): The list of messages for the chat completion.\n",
        "        model (str): The LLM model to use.\n",
        "        max_retries (int): Maximum number of retries.\n",
        "        delay (int): Delay in seconds between retries.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the LLM's response.\n",
        "\n",
        "    Raises:\n",
        "        Exception: If the LLM call fails after all retries.\n",
        "    \"\"\"\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=messages\n",
        "            )\n",
        "            return completion.choices[0].message.content\n",
        "        except (APIError, APIConnectionError, RateLimitError) as e:\n",
        "            print(f\"API error during LLM call (Attempt {i+1}/{max_retries}): {e}\")\n",
        "            if i < max_retries - 1:\n",
        "                time.sleep(delay * (2 ** i))\n",
        "            else:\n",
        "                print(\"Max retries reached. LLM call failed.\")\n",
        "                raise\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during LLM call (Attempt {i+1}/{max_retries}): {e}\")\n",
        "            if i < max_retries - 1:\n",
        "                time.sleep(delay * (2 ** i))\n",
        "            else:\n",
        "                print(\"Max retries reached. LLM call failed.\")\n",
        "                raise"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÑ Function: `extract_key_terms(docs, key_terms)`\n",
        "\n",
        "### ‚úÖ Purpose\n",
        "Extracts legal clauses relevant to predefined key terms (e.g., *Payment Terms*, *Governing Law*) from a document by using a Large Language Model (LLM). This enables automated compliance checks or contract analysis without manual review.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why Use It?\n",
        "- Legal documents are often long and complex.\n",
        "- LLMs have token limits, so we split the document into chunks and analyze each individually.\n",
        "- This function ensures we can **reliably find and summarize legal obligations** across entire documents using AI.\n",
        "\n",
        "---\n",
        "\n",
        "### üß± Inputs\n",
        "\n",
        "| Parameter   | Type    | Description                                                                 |\n",
        "|-------------|---------|-----------------------------------------------------------------------------|\n",
        "| `docs`      | `list`  | A list of Langchain `Document` objects containing the document content.     |\n",
        "| `key_terms` | `list`  | List of strings representing legal terms to search for (e.g., `[\"Confidentiality\"]`). |\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ Internal Workflow\n",
        "\n",
        "1. **Chunking**: The input document is split into smaller overlapping pieces to fit LLM limits.\n",
        "2. **Term Loop**: For each term in `key_terms`, every chunk is checked via LLM.\n",
        "3. **LLM Prompting**:\n",
        "   - The model is prompted with the chunk and asked if the key term is relevant.\n",
        "   - If **relevant**, it provides a **short summary**.\n",
        "4. **Result Handling**:\n",
        "   - Relevant summaries are collected with associated **page numbers**.\n",
        "   - If not found, the result is marked as `\"Not found\"`.\n",
        "\n",
        "---\n",
        "\n",
        "### üì§ Output\n",
        "\n",
        "Returns a `dict` where:\n",
        "\n",
        "- **Key**: A legal term.\n",
        "- **Value**: A dictionary containing:\n",
        "  - `\"answer\"` ‚Äì a brief summary or `\"Not found.\"`\n",
        "  - `\"page_number\"` ‚Äì page(s) where relevant clauses were found.\n",
        "\n",
        "#### Example:\n",
        "```json\n",
        "{\n",
        "  \"Confidentiality Obligations\": {\n",
        "    \"answer\": \"Found relevant information. Summary: The agreement prohibits both parties from disclosing trade secrets.\",\n",
        "    \"page_number\": \"3, 5\"\n",
        "  },\n",
        "  \"Governing Law\": {\n",
        "    \"answer\": \"Not found.\",\n",
        "    \"page_number\": \"Not applicable\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "B8c455qOTPNG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "vY8ThKBeQ9Av"
      },
      "outputs": [],
      "source": [
        "def extract_key_terms(docs, key_terms):\n",
        "    \"\"\"\n",
        "    Extracts clauses related to key terms from document chunks using LLM.\n",
        "    Uses chunking to handle token limits and attempts to correlate results with page numbers.\n",
        "\n",
        "    Args:\n",
        "        docs (list): A list of Langchain Document objects (can be chunks).\n",
        "        key_terms (list): List of key terms to extract.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with each key term and its associated extracted answer and page number.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    chunks = chunk_document(docs)\n",
        "\n",
        "    for term in key_terms:\n",
        "        term_results = []\n",
        "        found_in_chunks = False\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            prompt = (\n",
        "                f\"You are a legal document analysis assistant. Your task is to determine if the following document snippet \"\n",
        "                f\"contains information pertaining to the term: '{term}'. If it does, extract the relevant clause(s) and provide \"\n",
        "                f\"a very short summary. If relevant information is found, respond in the following structured format:\\n\\n\"\n",
        "                f\"Relevant: Yes  \\n\"\n",
        "                f\"Summary: <A very short and to the point summary of the relevant clause(s)>\\n\\n\"\n",
        "                f\"If the term is not found in this snippet, respond exactly with:  \\n\"\n",
        "                f\"Relevant: No  \\n\\n\"\n",
        "                f\"Document Snippet (Chunk {i+1}):  \\n\"\n",
        "                f\"{chunk.page_content}  \\n\\n\"\n",
        "                f\"Please ensure that your analysis is precise and adheres to the legal context, \"\n",
        "                f\"maintaining clarity and conciseness in your summary.\"\n",
        "            )\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal contract analysis assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                answer = query_llm_with_retry(messages, model=\"gpt-4o-mini\")\n",
        "                if \"Relevant: Yes\" in answer:\n",
        "                    summary_match = re.search(r\"Summary:\\s*(.*)\", answer, re.DOTALL)\n",
        "                    summary = summary_match.group(1).strip() if summary_match else \"Summary extraction failed.\"\n",
        "                    page_number = chunk.metadata.get(\"page\", \"N/A\")\n",
        "                    term_results.append({\"clause_summary\": summary, \"page_number\": page_number, \"chunk_index\": i})\n",
        "                    found_in_chunks = True\n",
        "                elif \"Relevant: No\" in answer:\n",
        "                    pass\n",
        "                else:\n",
        "                    print(f\"Warning: Unexpected LLM response format for term '{term}' in chunk {i+1}. Response:\\n{answer}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred while processing chunk {i+1} for term '{term}': {e}\")\n",
        "\n",
        "        if not found_in_chunks or not term_results:\n",
        "            results[term] = {\"answer\": \"Not found.\", \"page_number\": \"Not applicable\"}\n",
        "        else:\n",
        "            combined_summary = \" \".join([res[\"clause_summary\"] for res in term_results])\n",
        "            page_numbers = sorted(list(set([res[\"page_number\"] for res in term_results])))\n",
        "            page_info = \", \".join(map(str, page_numbers)) if page_numbers and page_numbers != [\"N/A\"] else \"N/A\"\n",
        "\n",
        "            results[term] = {\n",
        "                \"answer\": f\"Found relevant information. Summary: {combined_summary}\",\n",
        "                \"page_number\": page_info\n",
        "            }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7nhhiyqfRFc9"
      },
      "outputs": [],
      "source": [
        "def truncate_text(text, max_length):\n",
        "    \"\"\"Truncate text to specified length for display purposes\"\"\"\n",
        "    return text[:max_length] + \"...\" if len(text) > max_length else text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Function: `azure_judge(entry, results_df)`\n",
        "\n",
        "### üîπ Purpose\n",
        "Evaluates a single LLM response (`entry`) using four quality metrics ‚Äî **groundedness**, **coherence**, **relevance**, and **fluency** ‚Äî and appends the results to the `results_df` DataFrame.\n",
        "\n",
        "---\n",
        "\n",
        "### üß± Parameters\n",
        "\n",
        "| Name         | Type        | Description                                      |\n",
        "|--------------|-------------|--------------------------------------------------|\n",
        "| `entry`      | dict        | A dictionary with keys like query, context, LLM response, etc. |\n",
        "| `results_df` | DataFrame   | A Pandas DataFrame to which the evaluation results are appended. |\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è Evaluation Metrics\n",
        "\n",
        "- **Groundedness:** Checks how well the response is based on provided context.\n",
        "- **Coherence:** Measures logical flow and consistency of the response.\n",
        "- **Relevance:** Assesses how relevant the response is to the original query.\n",
        "- **Fluency:** Evaluates the grammatical correctness and readability.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Behavior\n",
        "\n",
        "- Extracts necessary inputs from the `entry`.\n",
        "- Calls evaluator functions with these inputs.\n",
        "- Appends evaluation results (or `None` in case of an error) to `results_df`.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Returns\n",
        "\n",
        "- Updated `results_df` with a new row of evaluation scores and metadata.\n",
        "\n"
      ],
      "metadata": {
        "id": "wqeXYwTeTiOW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "dDh-BJDyRQPt"
      },
      "outputs": [],
      "source": [
        "def azure_judge(entry, results_df):\n",
        "    \"\"\"Evaluate a single entry using all evaluators and add to results dataframe\"\"\"\n",
        "\n",
        "    # Format inputs for each evaluator - using llm_response instead of ground_truth\n",
        "    groundedness_input = {\n",
        "        \"query\": entry[\"query\"],\n",
        "        \"context\": entry[\"context\"],\n",
        "        \"response\": entry[\"llm_response\"]  # Changed from ground_truth to llm_response\n",
        "    }\n",
        "\n",
        "    coherence_input = {\n",
        "        \"query\": entry[\"query\"],\n",
        "        \"response\": entry[\"llm_response\"]  # Changed from ground_truth to llm_response\n",
        "    }\n",
        "\n",
        "    relevance_input = {\n",
        "        \"query\": entry[\"query\"],\n",
        "        \"response\": entry[\"llm_response\"]  # Changed from ground_truth to llm_response\n",
        "    }\n",
        "\n",
        "    fluency_input = {\n",
        "        \"response\": entry[\"llm_response\"]  # Changed from ground_truth to llm_response\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Get all scores\n",
        "        groundedness_score = groundedness_eval(**groundedness_input)\n",
        "        coherence_score = coherence_eval(**coherence_input)\n",
        "        relevance_score = relevance_eval(**relevance_input)\n",
        "        fluency_score = fluency_eval(**fluency_input)\n",
        "\n",
        "        # Create new row for the dataframe\n",
        "        new_row = {\n",
        "            'Key Term Name': entry['Key Term Name'], # Ensure Key Term Name is passed through\n",
        "            'Context': entry['context'],\n",
        "            'Query': entry['query'],\n",
        "            'LLM Response': entry['llm_response'],\n",
        "            'Groundedness Score': groundedness_score['groundedness'],\n",
        "            'Coherence Score': coherence_score['coherence'],\n",
        "            'Relevance Score': relevance_score['relevance'],\n",
        "            'Fluency Score': fluency_score['fluency']\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating entry: {e}\")\n",
        "        # Create row with error indicators\n",
        "        new_row = {\n",
        "            'Key Term Name': entry['Key Term Name'],\n",
        "            'Context': entry['context'],\n",
        "            'Query': entry['query'],\n",
        "            'LLM Response': entry['llm_response'],\n",
        "            'Groundedness Score': None,\n",
        "            'Coherence Score': None,\n",
        "            'Relevance Score': None,\n",
        "            'Fluency Score': None\n",
        "        }\n",
        "\n",
        "    # Use concat with a pre-defined DataFrame\n",
        "    new_row_df = pd.DataFrame([new_row])\n",
        "    return pd.concat([results_df, new_row_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function: `generate_evaluation_data`\n",
        "\n",
        "The `generate_evaluation_data` function prepares a structured dataset for evaluating how well an LLM has extracted information about specific legal key terms from a document. It iterates through a predefined list of key terms, checks if a term exists in the LLM result, and retrieves the associated answer. For each valid term, it constructs an evaluation entry consisting of:\n",
        "- the key term name,\n",
        "- a query requesting extraction of information for that term,\n",
        "- a truncated version (first 2000 characters) of the original document text as context,\n",
        "- and the LLM‚Äôs response.\n",
        "\n",
        "These entries are compiled into a list and returned. This dataset is typically used for scoring the LLM outputs against evaluation metrics such as groundedness, fluency, coherence, and relevance.\n"
      ],
      "metadata": {
        "id": "Y6S_4d9xUsQd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "whEQUAWCYLe-"
      },
      "outputs": [],
      "source": [
        "def generate_evaluation_data(llm_results,document_text):\n",
        "    \"\"\"Create evaluation dataset from LLM and ground truth results\"\"\"\n",
        "    evaluation_data = []\n",
        "\n",
        "    for term in KEY_TERMS:\n",
        "        if term in llm_results :\n",
        "            llm_answer = llm_results[term].get('answer', 'Not found.')\n",
        "\n",
        "\n",
        "            # Create evaluation entry\n",
        "            entry = {\n",
        "                'Key Term Name': term,  # Add the key term name here\n",
        "                'query': f\"Extract information about {term} from the legal document.\",\n",
        "                'context': document_text[:2000],  # Use first 2000 chars as context\n",
        "                'llm_response': llm_answer\n",
        "            }\n",
        "            evaluation_data.append(entry)\n",
        "\n",
        "    return evaluation_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function: `evaluate_responses`\n",
        "\n",
        "The `evaluate_responses` function assesses LLM-generated outputs using Azure's pre-trained evaluators (groundedness, coherence, relevance, and fluency). It begins by generating structured evaluation data for each key legal term based on the LLM's response and a portion of the document text. It then initializes an empty DataFrame with appropriate columns and iteratively evaluates each entry using the `azure_judge` function. The final output is a DataFrame containing evaluation scores for each response. If any error occurs during processing, the function catches the exception and returns a DataFrame indicating the failure. This process helps quantify the quality and reliability of the LLM's outputs.\n"
      ],
      "metadata": {
        "id": "hWmL1wDRU27V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Ao7teojcRYTL"
      },
      "outputs": [],
      "source": [
        "def evaluate_responses(llm_results, document_text):\n",
        "    \"\"\"Evaluate LLM responses against ground truth using Azure evaluators\"\"\"\n",
        "    try:\n",
        "        # Create evaluation dataset\n",
        "        evaluation_data = generate_evaluation_data(llm_results,document_text)\n",
        "\n",
        "        # Initialize results dataframe with all necessary columns\n",
        "        results_df = pd.DataFrame(columns=[\n",
        "            'Key Term Name', 'Context', 'Query', 'LLM Response', 'Groundedness Score',\n",
        "            'Coherence Score', 'Relevance Score', 'Fluency Score'\n",
        "        ])\n",
        "\n",
        "        # Evaluate each entry\n",
        "        for entry in evaluation_data:\n",
        "            results_df = azure_judge(entry, results_df)\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during evaluation: {e}\")\n",
        "        # Return an empty DataFrame or a DataFrame with error info for Gradio\n",
        "        return pd.DataFrame({'Error': [f\"Evaluation failed: {e}\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function: `process_document`\n",
        "\n",
        "The `process_document` function handles the end-to-end flow of extracting and evaluating legal key terms from a contract file. It first reads and extracts text from the provided contract document, then uses an LLM to identify and extract predefined key legal terms. Once extracted, it evaluates the quality of these responses using Azure's evaluation metrics (groundedness, coherence, relevance, fluency). The function returns both the LLM-extracted results and the evaluation results. If an error occurs at any stage, it catches the exception and ensures a fallback response structure is returned, enabling smooth downstream handling in interfaces like Gradio.\n"
      ],
      "metadata": {
        "id": "2Svw7m8mU9pP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "5wRH1UYcRbzv"
      },
      "outputs": [],
      "source": [
        "def process_document(contract_file):\n",
        "    \"\"\"\n",
        "    Processes the uploaded contract and optional ground truth document.\n",
        "\n",
        "    Args:\n",
        "        contract_file (str): Path to the contract document.\n",
        "        ground_truth_file (str, optional): Path to the ground truth document. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the LLM extraction results, ground truth results, and evaluation results.\n",
        "    \"\"\"\n",
        "    llm_results = {}\n",
        "\n",
        "    evaluation_results = None\n",
        "    document_text = \"\"\n",
        "\n",
        "    try:\n",
        "        # Step 1: Extract text and documents from the contract file\n",
        "        print(f\"Processing contract file: {contract_file}\")\n",
        "        contract_text, contract_docs = extract_text_from_file(contract_file)\n",
        "        document_text = contract_text\n",
        "\n",
        "        # Step 2: Extract key terms using the LLM on document chunks\n",
        "        print(\"Extracting key terms using LLM...\")\n",
        "        llm_results = extract_key_terms(contract_docs, KEY_TERMS)\n",
        "        print(\"Key term extraction complete.\")\n",
        "        # Step 4: Evaluate LLM responses against ground truth\n",
        "        print(\"Starting evaluation...\")\n",
        "        evaluation_results = evaluate_responses(llm_results,document_text)\n",
        "        print(\"Evaluation complete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during document processing: {e}\")\n",
        "        llm_results = {\"Error\": f\"Processing failed: {e}\"}\n",
        "\n",
        "        # Ensure evaluation_results is a DataFrame even on error for Gradio\n",
        "        evaluation_results = pd.DataFrame({'Error': [f\"Processing failed: {e}\"]})\n",
        "\n",
        "    return llm_results, evaluation_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìÑ Application Flow: Legal Document Analyzer with Azure Evaluation\n",
        "\n",
        "This application provides a user-friendly interface for analyzing legal documents using an LLM (like GPT-4) and evaluating the extracted results with Azure's language evaluators.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ End-to-End Flow\n",
        "\n",
        "1. **User Uploads Document**\n",
        "   - The user uploads a contract document (`.pdf`, `.docx`, `.txt`, etc.) via the Gradio interface.\n",
        "\n",
        "2. **Button Click Triggers Analysis**\n",
        "   - Clicking the \"Analyze Document\" button invokes the `display_results` function.\n",
        "\n",
        "3. **Document Processing**\n",
        "   - Internally, `display_results` calls `process_document`, which performs:\n",
        "     - **Text Extraction**: Extracts text from the uploaded file.\n",
        "     - **LLM Term Extraction**: Extracts key legal terms using the LLM.\n",
        "     - **Evaluation**: Evaluates the extracted responses using Azure metrics (Groundedness, Coherence, Relevance, Fluency).\n",
        "\n",
        "4. **Formatting Results**\n",
        "   - The LLM-extracted terms are formatted into a Markdown string.\n",
        "   - The evaluation scores are structured into a clean `DataFrame` for display.\n",
        "\n",
        "5. **Displaying Results**\n",
        "   - The Gradio interface displays:\n",
        "     - A **Markdown section** showing the key terms and their extracted answers.\n",
        "     - A **DataFrame table** with evaluation scores for each term.\n",
        "\n",
        "6. **Error Handling**\n",
        "   - If any step fails (e.g., file parsing or API errors), a fallback response is returned with appropriate error messaging, ensuring the UI doesn't break.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Outcome\n",
        "\n",
        "The user receives:\n",
        "- **LLM Extraction Results**: Each key legal term and its extracted answer.\n",
        "- **Evaluation Table**: Objective scores assessing how well the extracted answer aligns with expected responses based on Azure's evaluation tools.\n",
        "\n",
        "---\n",
        "\n",
        "> üí° **Note**: You must have an active **Azure Premium subscription** to access the evaluation capabilities.\n"
      ],
      "metadata": {
        "id": "FNvZTZ-vVGU9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "collapsed": true,
        "id": "izQZk13p-uU_"
      },
      "outputs": [],
      "source": [
        "def display_results(contract_file):\n",
        "    \"\"\"\n",
        "    Display results in a formatted way for Gradio interface\n",
        "    \"\"\"\n",
        "    llm_results, evaluation_results = process_document(contract_file)\n",
        "\n",
        "    # Format LLM Results\n",
        "    llm_output = \"## LLM Extraction Results\\n\\n\"\n",
        "    for term, result in llm_results.items():\n",
        "        if isinstance(result, dict) and 'answer' in result:\n",
        "            llm_output += f\"**{term}:**\\n\"\n",
        "            llm_output += f\"- Answer: {result['answer']}\\n\"\n",
        "        else:\n",
        "            llm_output += f\"**{term}:** {result}\\n\\n\"\n",
        "\n",
        "\n",
        "\n",
        "    # Prepare Evaluation Results for Gradio DataFrame\n",
        "    if evaluation_results is not None and not evaluation_results.empty:\n",
        "        if 'Error' in evaluation_results.columns:\n",
        "            # If there's an error, just return a simple DataFrame with the error message\n",
        "            # Gradio DataFrame can display this, but it won't be in the desired evaluation format.\n",
        "            # You might want to handle this error display differently in the UI if needed.\n",
        "            # For now, it will show a table with one column 'Error' and the message.\n",
        "            evaluation_df_for_gradio = evaluation_results\n",
        "        else:\n",
        "            # Define the desired column order for the final table\n",
        "            desired_columns = [\n",
        "                'Key Term Name', 'Query', 'LLM Response',\n",
        "                'Groundedness Score', 'Coherence Score', 'Relevance Score', 'Fluency Score'\n",
        "            ]\n",
        "\n",
        "            # Filter and reorder columns\n",
        "            # Drop 'Context' as it's not requested in the final output format.\n",
        "            display_df = evaluation_results.copy()\n",
        "            if 'Context' in display_df.columns:\n",
        "                display_df = display_df.drop(columns=['Context'])\n",
        "\n",
        "            final_columns = [col for col in desired_columns if col in display_df.columns]\n",
        "            evaluation_df_for_gradio = display_df[final_columns]\n",
        "    else:\n",
        "        # Return an empty DataFrame with the desired columns if no evaluation is performed\n",
        "        # This prevents Gradio from throwing an error about unexpected output type.\n",
        "        evaluation_df_for_gradio = pd.DataFrame(columns=[\n",
        "            'Key Term Name', 'Query', 'LLM Response',\n",
        "            'Groundedness Score', 'Coherence Score', 'Relevance Score', 'Fluency Score'\n",
        "        ])\n",
        "        # You could also add a row indicating \"No evaluation performed\"\n",
        "        # evaluation_df_for_gradio.loc[0] = [\"N/A\"] * len(evaluation_df_for_gradio.columns)\n",
        "        # evaluation_df_for_gradio.loc[0, 'Key Term Name'] = \"No evaluation performed (ground truth file required).\"\n",
        "\n",
        "    return llm_output, evaluation_df_for_gradio\n",
        "\n",
        "# Gradio Interface\n",
        "def create_interface():\n",
        "    \"\"\"Create Gradio interface for the legal document analyzer\"\"\"\n",
        "\n",
        "    with gr.Blocks(title=\"Legal Document Analyzer with Azure Evaluation\") as interface:\n",
        "        gr.Markdown(\"# Legal Document Analyzer with Azure Evaluation\")\n",
        "        gr.Markdown(\"Upload a legal contract document and optionally a ground truth file to extract key terms and evaluate the results.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                contract_file = gr.File(\n",
        "                    label=\"Upload Contract Document\",\n",
        "                    file_types=[\".pdf\", \".docx\", \".doc\", \".txt\", \".csv\"]\n",
        "                )\n",
        "                analyze_btn = gr.Button(\"Analyze Document\", variant=\"primary\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                llm_output = gr.Markdown(label=\"LLM Results\")\n",
        "\n",
        "        with gr.Row():\n",
        "            # Changed from gr.HTML to gr.DataFrame\n",
        "            # REMOVE headers=\"keys\" - Gradio will infer from the DataFrame\n",
        "            evaluation_output = gr.DataFrame(label=\"Evaluation Results\", wrap=True) # wrap=True for better text wrapping in cells\n",
        "\n",
        "        analyze_btn.click(\n",
        "            fn=display_results,\n",
        "            inputs=[contract_file],\n",
        "            outputs=[llm_output,evaluation_output]\n",
        "        )\n",
        "\n",
        "    return interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "7_bizVrFRkCa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "collapsed": true,
        "outputId": "030c1caf-4c98-481e-b340-320191d91e75"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-47-1241966818.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minterface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_interface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001b[0m\n\u001b[1;32m   2763\u001b[0m                 \u001b[0;31m# Cannot run async functions in background other than app's scope.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2764\u001b[0m                 \u001b[0;31m# Workaround by triggering the app endpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2765\u001b[0;31m                 resp = httpx.get(\n\u001b[0m\u001b[1;32m   2766\u001b[0m                     \u001b[0;34mf\"{self.local_api_url}startup-events\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2767\u001b[0m                     \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mssl_verify\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, headers, cookies, auth, proxy, follow_redirects, verify, timeout, trust_env)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mon\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mGET\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mrequests\u001b[0m \u001b[0mshould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minclude\u001b[0m \u001b[0ma\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m     return request(\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, params, content, data, files, json, headers, cookies, auth, proxy, timeout, follow_redirects, verify, trust_env)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m     with Client(\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mcookies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, auth, params, headers, cookies, verify, cert, trust_env, http1, http2, proxy, mounts, timeout, follow_redirects, limits, max_redirects, event_hooks, base_url, transport, default_encoding)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mproxy_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_proxy_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_env_proxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         self._transport = self._init_transport(\n\u001b[0m\u001b[1;32m    689\u001b[0m             \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0mcert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_init_transport\u001b[0;34m(self, verify, cert, trust_env, http1, http2, limits, transport)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m         return HTTPTransport(\n\u001b[0m\u001b[1;32m    732\u001b[0m             \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mcert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, verify, cert, trust_env, http1, http2, limits, proxy, uds, local_address, retries, socket_options)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mURL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mssl_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_ssl_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproxy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_config.py\u001b[0m in \u001b[0;36mcreate_ssl_context\u001b[0;34m(verify, cert, trust_env)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Default case...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_default_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcafile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcertifi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mverify\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROTOCOL_TLS_CLIENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mcreate_default_context\u001b[0;34m(purpose, cafile, capath, cadata)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcafile\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcapath\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_verify_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcafile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_mode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCERT_NONE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;31m# no explicit cafile, capath or cadata but the verify mode is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    interface = create_interface()\n",
        "    interface.launch(debug=True, share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}