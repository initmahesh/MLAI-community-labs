{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0516c151",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge Simply Explained: A Complete Guide to Run LLM Evals\n",
    "\n",
    "Recently, the concept of “LLM as a Judge” has been gaining significant traction in the AI and NLP communities. As someone deeply involved in the field of LLM evaluation, I’ve seen firsthand how LLM judges are rapidly becoming the preferred method for evaluating language models. The reasons are clear: compared to traditional human evaluators, LLM judges offer faster, more scalable, and cost-effective assessments—eliminating much of the slow, expensive, and labor-intensive work that comes with manual review.\n",
    "\n",
    "However, it’s important to recognize that LLM judges are not without their own challenges and limitations. Blindly relying on them can lead to misleading results and unnecessary frustration. That’s why, in this guide, I’ll share everything I’ve learned about leveraging LLM judges for system evaluation, including:\n",
    "\n",
    "- The core principles behind LLM-as-a-Judge\n",
    "- The practical benefits and pitfalls of automated evaluation\n",
    "- Step-by-step instructions for setting up and running LLM-based evals \n",
    "\n",
    "---\n",
    "\n",
    "## What exactly is “LLM as a Judge”?\n",
    "\n",
    "“LLM-as-a-Judge” refers to the process of using Large Language Models (LLMs) to evaluate the outputs of other LLM systems. Instead of relying on human evaluators—which can be slow, expensive, and inconsistent—this approach leverages the reasoning and language understanding capabilities of LLMs to provide automated, scalable assessments.\n",
    "\n",
    "The process typically works as follows:\n",
    "1. **Define Evaluation Criteria:** You start by crafting an evaluation prompt that clearly specifies the criteria you want to assess (such as accuracy, relevance, faithfulness, bias, or any custom metric).\n",
    "2. **Present Inputs and Outputs:** The LLM judge is given the original input (e.g., a question or task) and the output generated by the LLM system under evaluation.\n",
    "3. **Automated Scoring:** The LLM judge reviews the information and assigns a score or rating based on the defined criteria.\n",
    "\n",
    "LLM judges are commonly used to power advanced evaluation metrics like G-Eval, answer relevancy, faithfulness, and bias detection. By automating the evaluation process, LLM-as-a-Judge enables faster, more consistent, and more scalable assessments—making it an increasingly popular choice for both research and production environments.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you get started, please make sure you have the following ready:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Sample Contract File for Testing\n",
    "\n",
    "To try out the contract analysis workflow, download the sample contract file provided below:\n",
    "\n",
    "- [Download Sample Contract (Google Drive)](https://drive.google.com/file/d/1E557kdNBZ5cDUvVDLNrEVRuKcRSYDG3Z/view?usp=sharing)\n",
    "\n",
    "### 2. Ground Truth CSV File\n",
    "\n",
    "Download the ground truth CSV file from the link below:\n",
    "\n",
    "- [Download Ground Truth CSV (Google Drive)](https://drive.google.com/file/d/1E557kdNBZ5cDUvVDLNrEVRuKcRSYDG3Z/view?usp=sharing)\n",
    "\n",
    "### 3. OpenAI API Key\n",
    "\n",
    "You’ll need your own OpenAI API key to access the language models used for contract evaluation. If you don’t have one yet, follow this step-by-step guide to generate your API key:\n",
    "\n",
    "- [How to get your own OpenAI API key (Medium article)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61733e35",
   "metadata": {},
   "source": [
    "# Step 1: Install the Dependencies\n",
    "\n",
    "Run the following command in your terminal or Jupyter notebook to install all required packages:\n",
    "\n",
    "```python\n",
    "!pip install gradio langchain openai python-docx PyPDF2 pandas\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "| Package       | Purpose / Use in Project                                                                 |\n",
    "|---------------|-----------------------------------------------------------------------------------------|\n",
    "| **gradio**    | Build interactive web UIs for machine learning and data apps. Lets users upload files, view results, and interact with your tool in a browser. |\n",
    "| **langchain** | Framework for building applications powered by large language models (LLMs). Helps with document loading, processing, and LLM integration.      |\n",
    "| **openai**    | Official Python client for OpenAI’s API. Allows your code to send prompts and receive responses from models like GPT-4.                         |\n",
    "| **python-docx** | Read, write, and extract text from Microsoft Word (.docx) files. Used to process contract documents in Word format.                        |\n",
    "| **PyPDF2**    | Read and extract text from PDF files. Enables your tool to analyze contracts provided as PDFs.                                                  |\n",
    "| **pandas**    | Powerful data analysis and manipulation library. Used to organize, process, and display results in tables (dataframes).                        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78af67c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gradio in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (5.36.2)\n",
      "Requirement already satisfied: langchain in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (0.3.25)\n",
      "Requirement already satisfied: openai in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (1.95.1)\n",
      "Requirement already satisfied: python-docx in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (1.2.0)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (3.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (2.3.1)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: audioop-lts<1.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.2.1)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.116.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.6.0)\n",
      "Requirement already satisfied: gradio-client==1.10.4 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (1.10.4)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.33.4)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (2.3.1)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (3.10.16)\n",
      "Requirement already satisfied: packaging in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (11.0.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (2.11.7)\n",
      "Requirement already satisfied: pydub in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.11.5)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (4.13.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.35.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio-client==1.10.4->gradio) (2025.5.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio-client==1.10.4->gradio) (15.0.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (0.3.60)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (0.3.30)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from python-docx) (6.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "! pip install gradio langchain openai python-docx PyPDF2 pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca2b6a",
   "metadata": {},
   "source": [
    "## After Installing Dependencies: Let's Start Importing!\n",
    "\n",
    "Now that you’ve installed all the necessary libraries, let’s import them into your Python script or notebook. Here’s a summary of each import and its purpose:\n",
    "\n",
    "| Import Statement                                                                 | Purpose / Usage                                                                                                 |\n",
    "|----------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n",
    "| `import gradio as gr`                                                            | Imports Gradio for building interactive web interfaces for your app.                                            |\n",
    "| `from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader` | Imports document loaders from LangChain to extract text from PDF, DOCX, and TXT files.                         |\n",
    "| `from openai import OpenAI`                                                      | Imports the OpenAI client to interact with language models like GPT-4 for contract analysis.                    |\n",
    "| `import pandas as pd`                                                            | Imports Pandas for organizing, processing, and displaying results in tables (dataframes).                      |\n",
    "| `import os`                                                                     | Imports Python’s built-in OS module for handling file paths and interacting with the operating system.          |\n",
    "| `import tempfile`                                                               | Imports the tempfile module to safely create and manage temporary files and directories during file processing. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da24358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from langchain.schema import Document\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import tempfile\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252947bb",
   "metadata": {},
   "source": [
    "# Designing Key Terms and Evaluation Metrics\n",
    "\n",
    "Hey! Now that we’re building our LLM contract evaluation system, let’s talk about two of the most important foundations: **Key Terms** and **Evaluation Metrics**.\n",
    "\n",
    "---\n",
    "\n",
    "## What are Key Terms?\n",
    "\n",
    "Key terms are the specific contract clauses or topics that we want our system to automatically extract and analyze from any uploaded contract. Think of them as the “must-find” items in every contract review. By defining these up front, we ensure our tool is always looking for the most important legal concepts.\n",
    "\n",
    "Here are the key terms we’ve chosen:\n",
    "\n",
    "| Key Term                   | What It Means (in contracts)                                  |\n",
    "|----------------------------|---------------------------------------------------------------|\n",
    "| Service Warranty           | Guarantees and standards for services provided                |\n",
    "| Limitation of Liability    | Limits on legal responsibility for damages or losses          |\n",
    "| Governing Law              | Which jurisdiction’s laws apply to the contract               |\n",
    "| Termination for Cause      | When and how the contract can be ended early                  |\n",
    "| Payment Terms              | Details about payment amounts, schedules, and methods         |\n",
    "| Confidentiality Obligations| Rules about keeping information private                       |\n",
    "\n",
    "---\n",
    "\n",
    "## What are Evaluation Metrics?\n",
    "\n",
    "Once we extract these key terms, we need a way to judge how well the extraction (and the LLM’s answer) matches what we want. That’s where evaluation metrics come in! These are the criteria we use to score and justify each answer.\n",
    "\n",
    "We group our metrics into three categories, inspired by the HHH (Helpful, Honest, Harmless) framework:\n",
    "\n",
    "---\n",
    "\n",
    "### Helpful\n",
    "| Metric                                                        | What It Measures                                      |\n",
    "|---------------------------------------------------------------|-------------------------------------------------------|\n",
    "| Was the information extracted as per the question asked?      | Did the answer directly address the key term?         |\n",
    "| Was the information complete?                                 | Is all relevant information included?                 |\n",
    "| Was the information enough to make a conclusive decision?     | Is the answer sufficient for decision-making?         |\n",
    "| Were associated red flags covered in the extracted output?    | Are potential issues or risks mentioned?              |\n",
    "\n",
    "---\n",
    "\n",
    "### Honest\n",
    "| Metric                                                        | What It Measures                                      |\n",
    "|---------------------------------------------------------------|-------------------------------------------------------|\n",
    "| Was the information extracted from all relevant clauses?      | Are multiple relevant sections included if needed?    |\n",
    "| Was the page number of extracted information correct?         | Are page references accurate?                         |\n",
    "| Was the AI reasoning discussing the relevant clause?          | Is the explanation focused on the right part?         |\n",
    "| Does the information stay within document scope?              | Is the answer limited to the uploaded contract?       |\n",
    "\n",
    "---\n",
    "\n",
    "### Harmless\n",
    "| Metric                                                        | What It Measures                                      |\n",
    "|---------------------------------------------------------------|-------------------------------------------------------|\n",
    "| Were results free from misleading claims?                     | Are there any false or misleading statements?         |\n",
    "| Does the tool avoid generic/non-contract answers?             | Is the answer specific to the contract, not generic?  |\n",
    "| Did the AI avoid illegal or insensitive justifications?       | Are explanations appropriate and lawful?              |\n",
    "| Did the tool prevent false claims about people/entities?      | Are there any incorrect statements about parties?     |\n",
    "| Did the tool context hateful/profane content?                 | Is the output free from inappropriate language?       |\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "- We define key terms to focus our extraction.\n",
    "- We use a set of evaluation metrics (grouped as Helpful, Honest, Harmless) to systematically judge the quality, accuracy, and safety of every answer our LLM provides.\n",
    "\n",
    "This structure ensures our contract analysis is thorough, reliable, and responsible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9726237",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_TERMS = [\n",
    "    \"Service Warranty\",\n",
    "    \"Limitation of Liability\",\n",
    "    \"Governing Law\",\n",
    "    \"Termination for Cause\",\n",
    "    \"Payment Terms\",\n",
    "    \"Confidentiality Obligations\"\n",
    "]\n",
    "\n",
    "EVALUATION_METRICS = [\n",
    "    \"Was the information extracted as per the question asked in the key term?\",\n",
    "    \"Was the information complete?\",\n",
    "    \"Was the information enough to make a conclusive decision?\",\n",
    "    \"Were associated red flags covered in the extracted output?\",\n",
    "    \"Was the information extracted from all relevant clauses?\",\n",
    "    \"Was the page number of extracted information correct?\",\n",
    "    \"Was the AI reasoning discussing the relevant clause?\",\n",
    "    \"Does the information stay within document scope?\",\n",
    "    \"Were results free from misleading claims?\",\n",
    "    \"Does the tool avoid generic/non-contract answers?\",\n",
    "    \"Did the AI avoid illegal or insensitive justifications?\",\n",
    "    \"Did the tool prevent false claims about people/entities?\",\n",
    "    \"Did the tool context hateful/profane content?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a6a43",
   "metadata": {},
   "source": [
    "## 📄 What Does `extract_text_from_file` Do?\n",
    "\n",
    "Hey! Now that we have our key terms and evaluation metrics set up, let’s talk about how we actually get the text out of the documents we want to analyze. That’s where the `extract_text_from_file` function comes in!\n",
    "\n",
    "---\n",
    "\n",
    "### What’s the Purpose?\n",
    "\n",
    "This function is designed to **extract all the text** from a contract file, no matter if it’s a PDF, Word document, plain text, or even a CSV. It’s the first step in our pipeline—turning a file into something our LLM can read and analyze.\n",
    "\n",
    "---\n",
    "\n",
    "### How Does It Work? (Step by Step)\n",
    "\n",
    "1. **Figure Out the File Type**\n",
    "   - The function looks at the file extension (like `.pdf`, `.docx`, `.txt`, or `.csv`) to see what kind of document you’ve uploaded.\n",
    "\n",
    "2. **Pick the Right Loader**\n",
    "   - Depending on the file type, it uses a special tool (called a “loader”) to read the file:\n",
    "     - **PDFs:** Uses `PyPDFLoader`\n",
    "     - **Word Docs (.docx, .doc):** Uses `Docx2txtLoader`\n",
    "     - **Text Files (.txt):** Uses `TextLoader`\n",
    "     - **CSV Files:** Uses `pandas.read_csv` to read the table and turn it into a string\n",
    "\n",
    "3. **Extract the Text**\n",
    "   - For PDFs, Word, and text files, it grabs the text from each page or section and joins them all together into one big string.\n",
    "   - For CSVs, it converts the whole table into a string.\n",
    "\n",
    "4. **Handle Unsupported Files**\n",
    "   - If you upload a file type it doesn’t recognize, it raises an error so you know something’s wrong.\n",
    "\n",
    "5. **Return the Results**\n",
    "   - It gives you back two things:\n",
    "     - The **full extracted text** (as a string)\n",
    "     - The **list of document objects** (which can be useful if you want to know about page numbers or other metadata later)\n",
    "\n",
    "---\n",
    "\n",
    "### Why Is This Important?\n",
    "\n",
    "- **Universal Input:** You can upload contracts in different formats, and this function will handle them all.\n",
    "- **Foundation for Analysis:** The extracted text is what we’ll feed into our LLM to find key terms and evaluate the contract.\n",
    "- **Error Handling:** It makes sure you don’t accidentally try to process a file type that isn’t supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7afea5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_file(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    elif ext in [\".docx\", \".doc\"]:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    elif ext in [\".txt\"]:\n",
    "        loader = TextLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    elif ext == \".csv\":\n",
    "        df = pd.read_csv(file_path)\n",
    "        text = df.to_string(index=False)\n",
    "        docs = [type('Doc', (object,), {'page_content': text})()]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "    return text, docs  # docs for page numbers if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb97e5",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI Client\n",
    "\n",
    "To interact with OpenAI’s language models (such as GPT-4), you need to create a client object using your own API key. This allows your application to send prompts and receive responses from OpenAI’s servers.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Code\n",
    "\n",
    "```python\n",
    "client = OpenAI(api_key='sk-...your-own-api-key-here...')\n",
    "```\n",
    "---\n",
    "\n",
    "### For a Step-by-Step Guide\n",
    "\n",
    "You can follow this detailed tutorial:  \n",
    "[How to get your own OpenAI API key (Medium article)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
    "\n",
    "---\n",
    "\n",
    "### Important Note About API Keys\n",
    "\n",
    "- **Security:** Never share your OpenAI API key publicly or commit it to version control (like GitHub). Treat it like a password.\n",
    "- **Personal Key Required:** The API key in the example above is for demonstration only. You must use your own unique API key to access OpenAI services.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "You need your own OpenAI API key to use the language models. Never share your key, and always keep it secure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "730cfdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key='Insert Your API Key Here')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca46ca6",
   "metadata": {},
   "source": [
    "## 🔍 `extract_key_terms` Function — Step-by-Step Explanation\n",
    "\n",
    "Hey! Let’s break down what the `extract_key_terms` function does, step by step, in a clear table format:\n",
    "\n",
    "| **Step** | **What Happens**                                                                                                    | **Why It’s Important**                                  |\n",
    "|----------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|\n",
    "| 1        | Loops through each key term in the provided list.                                                                   | Ensures all important contract clauses are checked.     |\n",
    "| 2        | For each term, constructs a prompt asking the AI to extract relevant sections from the contract text.               | Guides the AI to focus on the specific clause.          |\n",
    "| 3        | Sends the prompt to the OpenAI language model (e.g., GPT-4) for analysis.                                           | Leverages advanced AI for accurate extraction.          |\n",
    "| 4        | Receives the AI’s answer, which should include the relevant text and, if possible, page numbers.                    | Provides both the content and its location.             |\n",
    "| 5        | Uses a regular expression to try to extract the page number from the AI’s answer, if mentioned.                     | Helps with citation and navigation in the document.     |\n",
    "| 6        | Stores the answer and page number for each key term in a results dictionary.                                        | Organizes results for easy access and further use.      |\n",
    "| 7        | Returns the dictionary mapping each key term to its extracted answer and page number.                               | Makes the output easy to use in later steps.            |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35324c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_terms(text, key_terms):\n",
    "    results = {}\n",
    "    for term in key_terms:\n",
    "        prompt = (\n",
    "            f\"You are a legal document analysis assistant.\\n\"\n",
    "            f\"Your task is to extract all clause(s) in the following contract that pertain specifically to the term: '{term}'.\\n\"\n",
    "            f\"For each relevant clause, return the following structured response, keeping the summary extremely concise and to the point (no more than 2 sentences, focusing only on the key obligation or restriction):\\n\\n\"\n",
    "            f\"Clause: <Clause number or title>\\n\"\n",
    "            f\"Page: <Page number(s) if available>\\n\"\n",
    "            f\"Summary:\\n\"\n",
    "            f\"<A very brief summary of the clause, only the main point related to the term>\\n\\n\"\n",
    "            f\"If the term is not found, respond exactly with:\\n\"\n",
    "            f\"'Not found.'\\n\\n\"\n",
    "            f\"Document:\\n{text}...\"  # Truncated to fit token limit\n",
    "        )\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a legal contract analysis assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "        print(\"****************LLM Answer*******************\")\n",
    "        print(answer)        # Try to extract page number if mentioned\n",
    "        page_number = None\n",
    "        if \"page\" in answer.lower():\n",
    "            import re\n",
    "            match = re.search(r'page[s]?\\s*(\\d+)', answer, re.IGNORECASE)\n",
    "            if match:\n",
    "                page_number = match.group(1)\n",
    "        results[term] = {\"answer\": answer, \"page_number\": page_number}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994322f",
   "metadata": {},
   "source": [
    "## 🟢 What is \"Ground Truth\"?\n",
    "\n",
    "Before we dive into the function, let’s clarify what **ground truth** means in this context:\n",
    "\n",
    "> **Ground truth** refers to the correct, reference answers that a human expert would provide after carefully reading and analyzing the contract.  \n",
    "> These are the *verbatim* sections or clauses from the document that directly address each key term.  \n",
    "> We use ground truth answers as a gold standard to compare and evaluate how well the AI (LLM) is performing.\n",
    "\n",
    "---\n",
    "\n",
    "## 🟢 `extract_ground_truth` Function — Step-by-Step Table\n",
    "\n",
    "Let’s break down what the `extract_ground_truth` function does, step by step, in a clear table format:\n",
    "\n",
    "| **Step** | **What Happens**                                                                                                    | **Why It’s Important**                                  |\n",
    "|----------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|\n",
    "| 1        | Loops through each key term in the provided list.                                                                   | Ensures all important contract clauses are checked.     |\n",
    "| 2        | For each term, constructs a prompt asking the AI to extract the *ground truth* (verbatim text) for that key term.   | Focuses the AI on finding the exact, original wording.  |\n",
    "| 3        | Sends the prompt to the OpenAI language model (e.g., GPT-4) for analysis.                                           | Leverages advanced AI for precise extraction.           |\n",
    "| 4        | Receives the AI’s answer in a structured JSON format, including the answer and page number if available.            | Provides both the content and its location.             |\n",
    "| 5        | Uses a regular expression to try to extract the page number from the AI’s answer, if mentioned.                     | Helps with citation and navigation in the document.     |\n",
    "| 6        | Stores the answer and page number for each key term in a results dictionary.                                        | Organizes results for easy access and further use.      |\n",
    "| 7        | Returns the dictionary mapping each key term to its ground truth answer and page number.                            | Makes the output easy to use in later steps.            |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63def8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_ground_truth(text, key_terms):\n",
    "    \"\"\"\n",
    "    Extracts ground truth answers for each key term from a legal document.\n",
    "\n",
    "    Args:\n",
    "        text (str): The full text of the document.\n",
    "        key_terms (list): List of key terms to extract.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with each key term and its associated extracted answer and page number.\n",
    "    \"\"\"\n",
    "    ground_truth = {}\n",
    "\n",
    "    for term in key_terms:\n",
    "        prompt = (\n",
    "    f\"You are a legal document analysis assistant. \"\n",
    "    f\"Your task is to extract the *ground truth* from the provided legal document for the key term: '{term}'. \"\n",
    "    f\"The ground truth is the exact text (verbatim) from the document that directly addresses or defines the key term. \"\n",
    "    f\"If available, also include the page number(s) where this text appears. \"\n",
    "    f\"If the key term is not mentioned or no relevant section exists, respond with 'Not found'.\\n\\n\"\n",
    "    f\"Return your response in the following JSON format:\\n\"\n",
    "    f'{{\\n  \"term\": \"{term}\",\\n  \"ground_truth_answer\": \"<verbatim text>\",\\n  \"page_number\": \"<page number or Not mentioned>\"\\n}}\\n\\n'\n",
    "    f\"Document:\\n{text}...\"  # Truncate to stay within token limits\n",
    ")\n",
    "\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a legal contract analysis assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        answer = completion.choices[0].message.content.strip()\n",
    "        print(\"****************GROUNDTRUTHANSWER*******************\")\n",
    "        print(answer);\n",
    "\n",
    "        # Attempt to extract page number from the answer\n",
    "        page_number = None\n",
    "        page_match = re.search(r'page[s]?\\s*(\\d+)', answer, re.IGNORECASE)\n",
    "        if page_match:\n",
    "            page_number = page_match.group(1)\n",
    "\n",
    "        ground_truth[term] = {\n",
    "            \"ground_truth_answer\": answer,\n",
    "            \"page_number\": page_number or \"Not mentioned\"\n",
    "        }\n",
    "\n",
    "    return ground_truth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a418c07c",
   "metadata": {},
   "source": [
    "## 🏆 What Does `judge_key_term` Do?\n",
    "\n",
    "Let’s talk about how we actually **evaluate** the answers that our LLM extracts from the contract. It’s not enough to just pull out information—we need to judge how good, accurate, and reliable those answers are. That’s where the `judge_key_term` function comes in!\n",
    "\n",
    "---\n",
    "\n",
    "### What Are We Doing Here?\n",
    "\n",
    "This function systematically evaluates how well the extracted answer for each key term matches up to the ground truth (the human-verified answer) using a set of evaluation metrics. It leverages an AI model to provide both a numerical score and a brief justification for each metric, for every key term.\n",
    "\n",
    "In other words:  \n",
    "- For every key term (like \"Service Warranty\" or \"Payment Terms\"),  \n",
    "- For every evaluation metric (like \"Was the information complete?\"),  \n",
    "- We ask the AI to **score** the extracted answer and **explain** its reasoning.\n",
    "\n",
    "This gives us a detailed, multi-dimensional assessment of the LLM’s performance!\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Table\n",
    "\n",
    "| **Step** | **What Happens**                                                                                                    | **Why It’s Important**                                  |\n",
    "|----------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|\n",
    "| 1        | Loops through each key term in the provided list.                                                                   | Ensures every important contract clause is evaluated.   |\n",
    "| 2        | For each key term, retrieves the LLM-extracted answer and the ground truth answer.                                  | Sets up the comparison for evaluation.                  |\n",
    "| 3        | For each evaluation metric, constructs a prompt asking the AI to judge the extracted answer against the metric.     | Focuses the AI on a specific aspect of answer quality.  |\n",
    "| 4        | Sends the prompt to the OpenAI language model (e.g., GPT-4o) and receives a response with:                          | Leverages AI for consistent, expert-like evaluation.    |\n",
    "|          | - A score from 1 (poor) to 5 (excellent)                                                                            |                                                         |\n",
    "|          | - A brief justification (1-2 sentences) explaining the score                                                        |                                                         |\n",
    "| 5        | Parses the score and justification from the AI’s response using regular expressions.                                | Converts the AI’s output into structured data.          |\n",
    "| 6        | Compiles the results for each metric, including key term, answers, metric name, score, and justification.           | Organizes evaluation data for easy analysis.            |\n",
    "| 7        | Returns a list of all evaluation results for further processing or display.                                         | Provides a comprehensive evaluation report.             |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7abb7fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_key_term(key_terms, extract_key_terms_response, extract_ground_truth_response, metrics):\n",
    "    results = []\n",
    "    for term in key_terms:\n",
    "        llm_answer = extract_key_terms_response.get(term, {}).get(\"answer\", \"Not found\")\n",
    "        ground_truth = extract_ground_truth_response.get(term, {}).get(\"ground_truth_answer\", \"Not found\")\n",
    "        page_number = extract_key_terms_response.get(term, {}).get(\"page_number\", None)\n",
    "        for metric in metrics:\n",
    "            prompt = (\n",
    "                f\"You are an expert contract evaluator. \"\n",
    "                f\"Evaluate the following extracted answer for the key term '{term}' \"\n",
    "                f\"against the evaluation metric: '{metric}'.\\n\"\n",
    "                f\"Extracted Answer: {llm_answer}\\n\"\n",
    "                f\"Ground Truth Answer: {ground_truth}\\n\"\n",
    "                \"For this metric, provide:\\n\"\n",
    "                \"- A score from 1 (poor) to 5 (excellent)\\n\"\n",
    "                \"- A brief justification (1-2 sentences)\\n\"\n",
    "                \"Respond in the format: Score: <number>\\nJustification: <text>\"\n",
    "            )\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a contract evaluation expert.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            content = completion.choices[0].message.content\n",
    "            print(\"*********JUDGE EVULATION ANSWER\");\n",
    "            print(content);\n",
    "            import re\n",
    "            score_match = re.search(r\"Score:\\s*(\\d+)\", content)\n",
    "            justification_match = re.search(r\"Justification:\\s*(.*)\", content, re.DOTALL)\n",
    "            score = int(score_match.group(1)) if score_match else None\n",
    "            justification = justification_match.group(1).strip() if justification_match else content\n",
    "            results.append({\n",
    "                \"key_term_name\": term,\n",
    "                \"llm_extracted_ans_from_doc\": llm_answer,\n",
    "                \"page_number\": page_number,\n",
    "                \"ground_truth_answer\": ground_truth,\n",
    "                \"evulation_metric_name\": metric,\n",
    "                \"score\": score,\n",
    "                \"justification\": justification\n",
    "            })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed1b1cc",
   "metadata": {},
   "source": [
    "### What Does `mark_evaluation_pass_fail` Do?\n",
    "\n",
    "This function takes your evaluation results (either as a list of dictionaries or a DataFrame) and adds a new column called `is_pass`.  \n",
    "- If the score for a metric is 3 or higher, `is_pass` is set to `True` (pass).\n",
    "- If the score is below 3 or missing, `is_pass` is set to `False` (fail).\n",
    "\n",
    "This makes it easy to quickly see which evaluations meet your passing criteria!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6252095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_evaluation_pass_fail(evals):\n",
    "    \"\"\"\n",
    "    Adds a column 'is_pass' to the evaluation results, marking True if score >= 3, else False.\n",
    "    Accepts either a list of dicts or a pandas DataFrame.\n",
    "    Returns a DataFrame with the new column.\n",
    "    \"\"\"\n",
    "    if isinstance(evals, list):\n",
    "        df = pd.DataFrame(evals)\n",
    "    else:\n",
    "        df = evals.copy()\n",
    "    df['is_pass'] = df['score'].apply(lambda x: True if x is not None and x >= 3 else False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeeb4ba",
   "metadata": {},
   "source": [
    "## 🚦 What Does `process_documents` Do?\n",
    "\n",
    "This function is the main driver for the contract analysis workflow. It ties together all the core steps: reading the contract, extracting key terms, comparing to ground truth (if available), evaluating the results, and formatting everything for easy display.\n",
    "\n",
    "---\n",
    "\n",
    "### What Are We Doing Here?\n",
    "\n",
    "- We start by extracting all the text from the uploaded contract file.\n",
    "- Next, we use the LLM to extract the key terms from the contract.\n",
    "- If a ground truth file is provided, we extract the reference answers for each key term; otherwise, we mark them as \"Not found.\"\n",
    "- We then evaluate each key term’s extracted answer against the ground truth using all our evaluation metrics, scoring and justifying each one.\n",
    "- Finally, we organize the results into DataFrames for easy display, splitting them into three groups based on the Helpful, Honest, and Harmless metric categories.\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Table\n",
    "\n",
    "| **Step** | **What Happens**                                                                                                    | **Why It’s Important**                                  |\n",
    "|----------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|\n",
    "| 1        | Extracts text and document objects from the uploaded contract file using `extract_text_from_file`.                   | Converts the contract into a format suitable for further analysis.       |\n",
    "| 2        | Extracts key terms from the contract text using `extract_key_terms` and the predefined `KEY_TERMS` list.            | Identifies and isolates the most important clauses in the contract.      |\n",
    "| 3        | (If provided) Extracts ground truth answers for each key term using `extract_ground_truth`.                         | Provides a reference for evaluating the LLM’s answers.                   |\n",
    "| 4        | Judges each key term’s extracted answer against the ground truth using `judge_key_term` and all evaluation metrics. | Produces a set of scores and justifications for each metric.             |\n",
    "| 5        | Formats each evaluation result for display, extracting clean text and page numbers.                                 | Keeps results organized and traceable.                                   |\n",
    "| 6        | Prepares a DataFrame with all results, arranging columns in a clear order.                                          | Makes it simple to present and analyze the results in tabular form.      |\n",
    "| 7        | Splits the DataFrame into three based on metric category (Helpful, Honest, Harmless).                              | Allows for focused review of each evaluation dimension.                  |\n",
    "| 8        | Returns the extracted contract text and all three DataFrames (plus the full one).                                  | Provides all necessary outputs for downstream use (e.g., UI display).    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a7cdc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(contract_file, ground_truth_file=None):\n",
    "\n",
    "    # Step 1: Extract text from contract file\n",
    "    text, docs = extract_text_from_file(contract_file)\n",
    "    \n",
    "    # Step 2: Extract key terms\n",
    "    key_term_results = extract_key_terms(text, KEY_TERMS)\n",
    "    \n",
    "    # Step 3: Extract ground truth if provided\n",
    "    if ground_truth_file is not None:\n",
    "        with open(ground_truth_file, 'r', encoding='utf-8') as f:\n",
    "            ground_truth_text = f.read()\n",
    "        ground_truth_results = extract_ground_truth(ground_truth_text, KEY_TERMS)\n",
    "    else:\n",
    "        ground_truth_results = {term: {\"ground_truth_answer\": \"Not found\", \"page_number\": \"Not found\"} for term in KEY_TERMS}\n",
    "    \n",
    "    # Step 4: Judge each key term\n",
    "    evals = judge_key_term(\n",
    "        KEY_TERMS,\n",
    "        key_term_results,\n",
    "        ground_truth_results,\n",
    "        EVALUATION_METRICS\n",
    "    )\n",
    "\n",
    "    # Step 5: Format each evaluation result for display\n",
    "    for e in evals:\n",
    "        term = e[\"key_term_name\"]\n",
    "        llm_ans = e[\"llm_extracted_ans_from_doc\"]\n",
    "        # Extract only the text after 'Text:'\n",
    "        if llm_ans:\n",
    "            text_match = re.search(r'Text:\\s*(.*)', llm_ans, re.DOTALL)\n",
    "            e[\"llm_extracted_ans_from_doc\"] = text_match.group(1).strip() if text_match else llm_ans\n",
    "            # Extract page number from LLM answer\n",
    "            page_match = re.search(r'Page:\\s*(\\d+)', llm_ans)\n",
    "            e[\"llm_page_number\"] = page_match.group(1) if page_match else \"Not found\"\n",
    "        else:\n",
    "            e[\"llm_page_number\"] = \"Not found\"\n",
    "        # Show only the ground_truth_answer value and page number\n",
    "        gt = ground_truth_results.get(term, {})\n",
    "        e[\"ground_truth_answer\"] = gt.get(\"ground_truth_answer\", \"Not found\")\n",
    "        # e[\"ground_truth_answer_page_number\"] = gt.get(\"page_number\", \"Not found\")\n",
    "    \n",
    "    # Step 6: Prepare DataFrame with new columns in the correct order\n",
    "    df = pd.DataFrame(evals)\n",
    "    display_cols = [\n",
    "        \"key_term_name\",\n",
    "        \"llm_extracted_ans_from_doc\",\n",
    "        \"llm_page_number\",\n",
    "        \"ground_truth_answer\",\n",
    "        # \"ground_truth_answer_page_number\",\n",
    "        \"evulation_metric_name\",\n",
    "        \"score\",\n",
    "        \"justification\"\n",
    "    ]\n",
    "    df = df[display_cols]\n",
    "    \n",
    "    # Step 7: Split DataFrame into three based on metric index\n",
    "    metric_groups = [EVALUATION_METRICS[:4], EVALUATION_METRICS[4:8], EVALUATION_METRICS[8:]]\n",
    "    df1 = df[df[\"evulation_metric_name\"].isin(metric_groups[0])].reset_index(drop=True)\n",
    "    df2 = df[df[\"evulation_metric_name\"].isin(metric_groups[1])].reset_index(drop=True)\n",
    "    df3 = df[df[\"evulation_metric_name\"].isin(metric_groups[2])].reset_index(drop=True)\n",
    "    return text, df1, df2, df3, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b4303",
   "metadata": {},
   "source": [
    "## Gradio App Interface: LLM Contract Judge\n",
    "\n",
    "This section defines the interactive web interface for the contract analysis tool using Gradio. The interface allows users to upload contract files, extract key terms, evaluate them using an LLM, and view the results in a user-friendly format.\n",
    "\n",
    "---\n",
    "\n",
    "| UI Element / Step         | Description                                                                                                   | Why It’s Important                                                      |\n",
    "|---------------------------|---------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
    "| **App Container**         | Uses `gr.Blocks()` to create a modular, flexible Gradio app.                                                  | Allows for a clean, organized, and interactive user interface.           |\n",
    "| **Title Markdown**        | Displays a title and brief instructions at the top of the app.                                                | Helps users understand the app’s purpose and how to use it.              |\n",
    "| **Upload & Extract Tab**  | Provides a tab for uploading contract files (PDF, DOCX, TXT).                                                 | Lets users easily provide the documents they want to analyze.            |\n",
    "| **File Upload Widget**    | Allows users to upload a contract file.                                                                       | Supports multiple file formats for flexibility.                          |\n",
    "| **Extract Button**        | A button labeled \"Extract & Evaluate\" to start the analysis process.                                          | Gives users control over when to begin processing.                       |\n",
    "| **Extracted Text Box**    | Displays the extracted text from the uploaded contract.                                                       | Offers transparency and lets users review what was extracted.             |\n",
    "| **Results Table Tab**     | Provides a separate tab to display the evaluation results in a table format.                                  | Organizes results for easy review and comparison.                        |\n",
    "| **Results Dataframe**     | Shows a table with columns for key term, extracted answer, page number, evaluation metric, score, and justification. | Presents detailed evaluation results in a structured, readable way.      |\n",
    "| **run_all Function**      | Defines the function that runs the full analysis pipeline when the button is clicked.                         | Connects the UI to the backend logic for seamless operation.             |\n",
    "| **Button Click Event**    | Links the \"Extract & Evaluate\" button to the `run_all` function, passing the uploaded file as input.          | Ensures user actions trigger the correct processing workflow.            |\n",
    "| **App Launch**            | Calls `demo.launch()` to start the Gradio app and make it accessible in the browser.                          | Makes the tool available for interactive use.                            |\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- **User-Friendly:**  \n",
    "  The Gradio interface makes it easy for users to interact with complex AI-powered contract analysis tools without needing to write code.\n",
    "\n",
    "- **Transparency:**  \n",
    "  Users can see both the raw extracted text and the detailed evaluation results, increasing trust in the tool.\n",
    "\n",
    "- **Efficiency:**  \n",
    "  The app streamlines the workflow from document upload to actionable insights, all in one place.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "This Gradio app provides an accessible, interactive front-end for your contract analysis pipeline, allowing users to upload documents, trigger analysis, and review results with ease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e524fd68",
   "metadata": {},
   "source": [
    "# When you run the last cell in your notebook, you’ll see a message like the one shown in the image below. Click on the \"Running on local URL\" link—you will be redirected to a new screen where you can interact with the LLM Contract Judge app.\n",
    "\n",
    "![Gradio Local URL Example](Images//img-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024957e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer accurately describes termination clauses related to \"Termination for Cause\" without making misleading claims, such as specifying immediate termination for irremediable breaches and a 30-day remediation period for remediable breaches. However, the absence of a direct comparison to a ground truth answer limits the possibility of a perfect score.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answers provide specific contractual terms related to 'Termination for Cause', referencing particular clauses and pages, which avoids generic or non-contractual responses. However, the explanation slightly lacks detail on specific conditions or processes involved in determining what constitutes a material breach, which prevents it from receiving a perfect score.\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer appropriately states the conditions under which termination for cause may occur without resorting to illegal or insensitive justifications. It simply outlines the parties' rights to terminate in the event of specified breaches or harm, aligning with standard contract practices.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer does not include any false claims about people or entities, as it accurately describes the termination clauses from the contract without misattributing actions or consequences to either party beyond what is stipulated. The answer does not fabricate or misrepresent the roles of the involved parties in relation to the termination provisions.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer contains no hateful or profane content. It is a straightforward explanation of the conditions under which either party may terminate the agreement, along with the consequences of failing to remedy a breach.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer provides comprehensive details related to payment terms, including invoicing requirements, payment timelines, and consequences for non-payment. While it addresses most aspects of payment-related obligations, the absence of information in the 'Ground Truth Answer' prevents a perfect score, as it indicates a potential gap in alignment with the specific question asked in the key term.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer provides a comprehensive overview of payment terms including invoicing timelines, payment deadlines, responsibility for expenses, taxes, and consequences for non-payment. However, it lacks information on specific payment methods or penalties for late payments beyond service suspension, which would be necessary for a complete evaluation of payment terms.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer provides a comprehensive outline of the payment terms, including invoicing, payment deadlines, expenses, tax obligations, and potential service suspension for non-payment, which generally suffices to make a conclusive decision. However, additional details such as the frequency of invoices beyond the Order Form effective date would provide more clarity.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted clauses effectively cover key areas of potential red flags in payment terms, such as the timeline for payment, handling of expenses, tax responsibilities, and service suspension for non-payment. However, the clause on invoicing does not explicitly mention dispute resolution for invoices, which could be a concern.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer provides a comprehensive overview of the relevant clauses concerning payment terms, including invoicing, payment periods, expenses, taxes, and repercussions for non-payment. However, the absence of a corresponding ground truth answer limits complete verification against ground truth, preventing a perfect score. The extraction appears thorough within the information presented.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 2\n",
      "Justification: The extracted answer provides specific clauses and the corresponding page numbers where payment terms are discussed. However, since the ground truth answer indicates \"Not found,\" it suggests a discrepancy or inconsistency in confirming the extracted information's page accuracy within the document.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3\n",
      "Justification: The extracted answer identifies relevant clauses related to payment terms, such as invoicing and payment timelines, reimbursement of expenses, and taxation considerations. However, the Ground Truth Answer indicates that there is no specific clause found for 'Payment Terms,' and the extracted answer does not clearly consolidate these components into a unified discussion specifically labeled as 'Payment Terms'.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer provides detailed information regarding payment terms, invoicing, expenses, taxes, and consequences for non-payment, staying largely within the scope of a 'Payment Terms' key term. However, given that the ground truth answer is listed as \"Not found,\" it suggests a potential discrepancy in document interpretation or extraction accuracy, preventing a perfect score.\n",
      "****************LLM Answer*******************\n",
      "Not found.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer provides clear and relevant details about the payment terms, such as invoicing, payment timeline, responsibility for expenses, taxes, and the consequences of non-payment. There are no misleading claims, as the summary accurately reflects standard contractual obligations without suggesting any untrue or exaggerated terms. However, this could be further improved by explicitly confirming \"ground truth\" alignment, prompting a slight deduction.\n",
      "****************LLM Answer*******************\n",
      "Clause: 10. LIMITATIONS OF LIABILITY\n",
      "Page: 5\n",
      "Summary: Neither party will be liable for indirect, incidental, special, or consequential damages, and liability for damages is capped at the fees paid or payable in the preceding 12 months, excluding certain obligations like restrictions, indemnification, or confidentiality breaches.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer provides specific contractual terms related to invoicing, payment deadlines, expense reimbursements, tax obligations, and consequences for non-payment, which goes beyond generic statements typically found in non-contractual contexts. However, a more detailed explanation of specific terms, such as late payment penalties or methods of payment, could enhance the completeness and specificity of the payment terms.\n",
      "****************LLM Answer*******************\n",
      "Clause: Governing Law and Jurisdiction\n",
      "Page: 8\n",
      "Summary: The Agreement is governed by the laws of India, with the courts of Bangalore, Karnataka, having exclusive jurisdiction.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer provides a factual summary of the contract clauses related to payment terms without any illegal or insensitive justifications. The clauses are described in a straightforward manner, aligning with common contractual practices regarding invoicing, expenses, taxes, and suspension for non-payment.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately presents the 'Payment Terms' without making any false claims about the parties involved. It specifies the invoicing process, payment deadlines, tax obligations, and consequences of non-payment, clearly attributing responsibilities to Whatfix and the Customer without misleading information.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer contains no hateful or profane content. It objectively outlines the payment terms in a business-like manner, using clear and appropriate language throughout.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer provides a detailed account of the confidentiality obligations as outlined in the contract, including both the requirement to maintain confidentiality and the exceptions to these obligations. However, the \"Ground Truth Answer\" is marked as \"Not found,\" suggesting a possible disparity between the expected answer and the extraction or discrepancies in the availability of information. Nonetheless, the extraction accurately addresses the key aspects of confidentiality obligations relevant to the question.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer comprehensively covers the confidentiality obligations by detailing the parties' responsibilities regarding confidentiality, exceptions to these obligations, and procedures upon termination. The inclusion of clause references and page numbers supports the completeness of the information provided.\n",
      "****************LLM Answer*******************\n",
      "Clause: Termination\n",
      "Page: 4-5\n",
      "Summary: Allows either party to terminate the agreement if the other party commits a material breach that is irremediable or fails to remedy a remediable breach within 30 days (or 5 days in the case of non-payment).\n",
      "\n",
      "Clause: Effect of Termination\n",
      "Page: 5\n",
      "Summary: Upon termination, Whatfix ceases service provision, and all usage rights terminate; the Customer must pay all due amounts if terminated for breach, while pre-paid amounts for undelivered services are to be refunded if Whatfix breaches.\n",
      "\n",
      "Clause: Suspension and Termination\n",
      "Page: 4-5\n",
      "Summary: Whatfix can suspend services if non-payment extends 15 days post-notification, or if user activity causes harm, and may terminate immediately for uncured breaches or bankruptcy events.\n",
      "\n",
      "Clause: Term of SaaS Agreement\n",
      "Page: 4\n",
      "Summary: The agreement continues until terminated per Section 7.2, with renewal defaults unless proper notice of non-renewal is given 30 days prior to term expiration.\n",
      "\n",
      "Clause: Termination for Ongoing Harm\n",
      "Page: 4\n",
      "Summary: Whatfix can suspend services if customer use causes immediate harm, notifying the customer to strive for resolution.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer provides a comprehensive view of the confidentiality obligations by detailing the duration, restrictions, and exceptions, as well as actions required upon termination. However, the absence of a ground truth answer indicates that further context or verification may be beneficial for full assurance.\n",
      "****************LLM Answer*******************\n",
      "Clause: 6.2 Invoicing and Payment\n",
      "Page: 3\n",
      "Summary: Whatfix invoices Customer for all Fees on the Order Form effective date, and Customer must pay undisputed invoices within 30 days of receipt. Fees are non-refundable and payable in the currency detailed in the Order Form.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer adequately covers key aspects of confidentiality obligations, including the treatment and exceptions of confidential information and the actions required upon termination. However, it could include more detailed stipulations for the measures to be taken in case of legal disclosures or potential breaches to fully address associated red flags.\n",
      "****************LLM Answer*******************\n",
      "Clause: Confidentiality\n",
      "Page: 6-7\n",
      "Summary: Each party must treat the other party's confidential information as confidential during the term of the agreement and for 5 years thereafter, using it only to fulfill contractual obligations and not disclosing it to third parties.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer provides a comprehensive coverage of confidentiality obligations by including clauses related to confidentiality restrictions, exceptions, and actions upon termination. However, it lacks explicit confirmation that there are no additional relevant clauses elsewhere in the document, which prevents it from achieving a perfect score.\n",
      "****************GROUNDTRUTHANSWER*******************\n",
      "```json\n",
      "{\n",
      "  \"term\": \"Service Warranty\",\n",
      "  \"ground_truth_answer\": \"Whatfix warrants services will follow documentation and industry standards.\",\n",
      "  \"page_number\": \"5\"\n",
      "}\n",
      "```\n",
      "****************GROUNDTRUTHANSWER*******************\n",
      "```json\n",
      "{\n",
      "  \"term\": \"Limitation of Liability\",\n",
      "  \"ground_truth_answer\": \"Each party's liability is limited to fees paid in the previous 12 months.\",\n",
      "  \"page_number\": \"5\"\n",
      "}\n",
      "```\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 2  \n",
      "Justification: While the extracted answer provides specific clause numbers and page references for confidentiality-related obligations, it contrasts with the \"Ground Truth Answer: Not found,\" indicating a possible discrepancy or lack of verification of the page numbers' accuracy in the source document. This discrepancy suggests potential unreliability in the extracted information.\n",
      "****************GROUNDTRUTHANSWER*******************\n",
      "```json\n",
      "{\n",
      "  \"term\": \"Governing Law\",\n",
      "  \"ground_truth_answer\": \"This agreement is governed by Indian law, with courts in Bangalore having jurisdiction.\",\n",
      "  \"page_number\": \"9\"\n",
      "}\n",
      "```\n",
      "****************GROUNDTRUTHANSWER*******************\n",
      "```json\n",
      "{\n",
      "  \"term\": \"Termination for Cause\",\n",
      "  \"ground_truth_answer\": \"Either party may terminate for uncured breach or insolvency.\",\n",
      "  \"page_number\": \"4\"\n",
      "}\n",
      "```\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The AI reasoning clearly discusses the relevant clauses concerning 'Confidentiality Obligations', detailing the requirements for maintaining confidentiality, exceptions to these obligations, and actions upon termination. This comprehensive explanation aligns with the key term's scope, justifying the high score.\n",
      "****************GROUNDTRUTHANSWER*******************\n",
      "```json\n",
      "{\n",
      "  \"term\": \"Payment Terms\",\n",
      "  \"ground_truth_answer\": \"Customer must pay undisputed invoices within 30 days; fees are non-refundable.\",\n",
      "  \"page_number\": \"3\"\n",
      "}\n",
      "```\n",
      "****************GROUNDTRUTHANSWER*******************\n",
      "```json\n",
      "{\n",
      "  \"term\": \"Confidentiality Obligations\",\n",
      "  \"ground_truth_answer\": \"Confidential info must be protected for 5 years; perpetual for software.\",\n",
      "  \"page_number\": \"6\"\n",
      "}\n",
      "```\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer primarily focuses on confidentiality obligations as outlined in the agreement, detailing both restrictions and exceptions, and addressing what happens upon termination. This information is consistent with the document's scope regarding confidentiality. However, while comprehensive, it partially references clause 7.6(c) related to termination, which although relevant, extends slightly beyond the immediate context of the confidentiality obligations themselves.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer indicates that the information regarding the 'Service Warranty' was not found, while the ground truth clearly provides a specific warranty statement. This suggests a complete failure in extraction of the relevant information.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately reflects the key terms regarding confidentiality obligations without making any misleading claims. It clearly outlines the duration of confidentiality, permissible uses of information, exceptions, and post-termination obligations, aligning well with standard confidentiality obligations in contracts.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer specifically addresses the confidentiality obligations within the contract, detailing the treatment of confidential information, exceptions, and the actions required upon termination. It avoids generic or non-contractual language, focusing on precise contractual terms and their implications, which matches the evaluation metric criteria comprehensively.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer fails to provide any information regarding the 'Service Warranty' term, leaving it entirely unaddressed. In contrast, the ground truth answer specifies that Whatfix's services are warranted to comply with documentation and industry standards, highlighting a complete absence of the necessary information in the extraction.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer provided as \"Not found\" does not provide any information about the 'Service Warranty' term, leaving the evaluator unable to make a conclusive decision about this aspect of the contract.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer complies with legal standards by clearly outlining the confidentiality obligations and exceptions, without relying on illegal or insensitive justifications. The clauses are standard and adhere to typical legal norms for confidentiality agreements.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer indicates that the term 'Service Warranty' was not found, which means associated red flags related to the service warranty—such as adherence to documentation and industry standards—were not covered. Hence, it does not satisfy the evaluation metric.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer is \"Not found,\" suggesting that the evaluation failed to identify any relevant information about the 'Service Warranty' from the contract. Given that the ground truth provides explicit details about service conformity to documentation and industry standards, it is clear that the extraction missed all pertinent clauses relevant to the term.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately details the confidentiality obligations without making any false claims about people or entities. It clearly specifies how confidential information is to be treated, outlines exceptions, and describes the actions necessary upon termination, all without attributing false information or intentions.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer indicates that the information was not found, meaning no page number was provided in contrast to the ground truth which specifies page number \"5\" for the Service Warranty term. Therefore, the extracted information fails to meet the required metric.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer does not contain any hateful or profane content. It is a straightforward summary of confidentiality obligations and exceptions, focusing solely on the contractual terms without any inappropriate language.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer indicates that the term 'Service Warranty' was not found, while the ground truth specifies the details of this term, suggesting that the AI failed to identify and discuss the relevant clause. This discrepancy results in a poor evaluation score.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 2  \n",
      "Justification: The extracted answer \"Not found\" fails to identify or provide any relevant information about the 'Service Warranty' term, which is present in the ground truth. This indicates the extracted information does not stay within the document scope concerning the warranty term and does not effectively cover the details as specified in the contract.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer fails to acknowledge the presence of the 'Service Warranty' term in the contract, which is misleading given that the ground truth indicates a clear warranty regarding service adherence to documentation and industry standards. This omission could mislead stakeholders about the existence of any service warranty provisions in the contract.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer \"Not found\" suggests the tool failed to provide any relevant contract-specific information or acknowledgment related to the 'Service Warranty' term. This reflects a significant gap compared to the detailed and specific 'Service Warranty' provision provided in the ground truth answer, which specifically refers to adherence to documentation and industry standards, highlighting a poor alignment with the evaluation metric.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer of \"Not found\" does not attempt to construct a potentially illegal or insensitive justification for the absence of information on a service warranty. Therefore, it appropriately avoids any unintended legal or ethical issues that might arise from overextending beyond the provided data.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3\n",
      "Justification: The extracted answer of \"Not found\" does not actively prevent false claims about people or entities, but it also doesn't contribute to spreading them. It indicates a lack of information on the 'Service Warranty' which might imply a need for more robust extraction processes to ensure relevant terms are accurately captured.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The evaluation metric focuses on the presence of hateful or profane content, which is not present in either the extracted answer or the ground truth. Both answers are benign and do not contain any such content, making the evaluation robust under this specific metric.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer accurately captures the essence of the 'Limitation of Liability' term by mentioning the cap on liability to the fees paid in the previous 12 months, consistent with the ground truth. However, it includes additional details about exclusions and types of damages not requested, which detracts slightly from precision.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer is largely complete as it correctly identifies the core limitation of liability terms, including the cap on damages being limited to the fees paid in the previous 12 months, consistent with the ground truth. It also provides additional context about exclusions and types of damages not covered, which adds valuable detail, although not explicitly present in the ground truth.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer provides a comprehensive overview of the limitation of liability clause, including important details such as the exclusion of indirect and consequential damages and the cap on liability based on fees paid in the preceding 12 months. However, a minor mismatch in specifying that \"certain obligations\" are exceptions could have added clarity for a perfect score.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer provides a more comprehensive overview than the ground truth, including details about types of damages not covered and exceptions such as indemnification and confidentiality breaches. This includes vital information commonly seen as red flags, such as exceptions in liability limitations. However, further context on these specific exceptions could provide a more complete evaluation.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer includes additional relevant details about the types of damages not covered by the limitation and exemptions to the liability cap, which provides a more comprehensive understanding of the 'Limitation of Liability' clause compared to the ground truth answer. However, the ground truth focuses on the critical point of cap limitation. The extraction effectively captures relevant elements from the clause but contains slightly more information than is strictly necessary.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately identifies the page number (5) for the 'Limitation of Liability' clause, which matches the ground truth page number provided.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The AI reasoning discussed the relevant clause effectively by accurately summarizing the key elements of the 'Limitation of Liability' clause, including the exclusion of indirect damages and the cap on damages at the fees paid in the preceding 12 months, while also mentioning exclusions like restrictions, indemnity, or confidentiality breaches. This aligns well with the ground truth answer provided.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer contains detailed information within the scope of the document that directly pertains to the 'Limitation of Liability' term. It covers liability exclusions and the cap on damages, which are relevant and aligned with the ground truth details provided in the context.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer accurately outlines the limitation of liability by capping liability at the fees paid or payable in the preceding 12 months, which aligns with the ground truth. However, it adds additional information about exclusions and types of damages not mentioned in the ground truth, which, while not inherently misleading, adds complexity that could potentially lead to misunderstanding.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer provides a specific and detailed description of the 'Limitation of Liability' clause, clearly identifying the scope of liability and the cap in relation to fees paid, along with the exclusion of certain obligations. This detailed contractual language avoids generic or non-contractual descriptions, aligning well with the evaluation metric.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The AI's extracted answer appropriately summarizes the 'Limitation of Liability' clause without resorting to illegal or insensitive justifications. It clearly describes the limitation on liability, aligns with standard legal practices, and avoids any improper rationale, focusing solely on the cap related to fees and excluding certain obligations.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately reflects the clause that neither party will be liable for indirect, incidental, special, or consequential damages, and confirms that the liability is capped at the fees paid or payable in the preceding 12 months. This accurately matches the details in the ground truth answer and prevents false claims about the involved parties' liability exposure.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer does not contain any hateful or profane content; it neutrally describes the limitation of liability terms in the contract, focusing on the scope of liability and the cap without using any disrespectful language.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer closely matches the ground truth answer in terms of content, correctly identifying the governing law as Indian and the jurisdiction as Bangalore courts. However, there is a minor discrepancy in the page number, with the extracted answer indicating page 8 while the ground truth answer indicates page 9.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer completely addresses the key term 'Governing Law' by specifying that the agreement is governed by the laws of India, and it further details the jurisdiction of the courts in Bangalore, Karnataka. While the page number differs, the content of the answer is fully aligned with the ground truth.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer provides sufficient information to make a conclusive decision regarding the governing law, as it confirms that the agreement is governed by Indian law and specifies the courts of Bangalore as having exclusive jurisdiction. The minor discrepancy in page numbers does not affect the clarity or completeness of the legal terms described.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer aligns well with the ground truth answer in terms of the key details: the governing law is Indian law, and the exclusive jurisdiction is with the courts of Bangalore, Karnataka. However, the page number discrepancy may indicate misreference or extraction errors. Other potential red flags, such as ambiguity or conflicting jurisdiction details, are not evident in the extract.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer captures the essential elements of the 'Governing Law' term, including the applicable law (Indian law) and the specified jurisdiction (courts in Bangalore). However, there is a minor inconsistency regarding the page number, as the extracted answer refers to page 8, while the ground truth answer indicates page 9. Despite this discrepancy, the extracted content is largely complete and accurate.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer indicates that the clause \"Governing Law and Jurisdiction\" is located on page 8, whereas the ground truth specifies it is on page 9. This discrepancy in page numbers is critical as it affects the accuracy and reliability of locating the term within the document.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The AI reasoning directly discusses the relevant clause, accurately summarizing the governing law and jurisdiction as being under Indian law, with the courts in Bangalore having jurisdiction. Despite the slight page discrepancy, the core information aligns well with the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately reflects the scope of the document, providing a consistent summary of both the governing law and jurisdiction details as found in the ground truth answer. The discrepancy in the page number does not affect the relevance or scope of the information presented.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer closely matches the ground truth by indicating that Indian law governs the agreement and specifying the jurisdiction of Bangalore courts. However, there is a minor discrepancy in the page number, which does not affect the accuracy of the governing law itself but could be slightly misleading if precision in documentation location is critical.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer directly addresses the 'Governing Law' term and provides specific details regarding the applicable laws and jurisdiction, aligning well with the ground truth answer. It avoids using generic or non-contract language, effectively capturing the essential legal elements of the clause.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The AI provided a concise justification that correctly identifies the governing law and jurisdiction as the laws of India and the courts of Bangalore, Karnataka, respectively. There are no illegal or insensitive justifications present in the explanation.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer aligns accurately with the ground truth answer regarding the 'Governing Law' term, accurately stating the laws of India and the jurisdiction of Bangalore courts. There are no false claims about people or entities, thus meeting the evaluation metric effectively.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer contains no hateful or profane content. It simply states the governing law and jurisdiction for the contract, making it appropriate and relevant to the context.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer captures the essence of \"Termination for Cause,\" noting termination rights for material breaches and provisions related to insolvency, consistent with the ground truth. However, it includes additional, somewhat redundant information about service suspension and effects of termination not explicitly required by the ground truth.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer provides comprehensive details on the termination conditions, including termination for material breach, insolvency, and non-payment, aligning closely with the \"Termination for Cause\" described in the ground truth. However, the mention of insolvency is somewhat implicit rather than explicit, which slightly detracts from completeness.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer provides detailed scenarios for termination related to breaches and insolvency, aligning well with the ground truth, although the reference to \"Termination for Cause\" is implicitly covered across multiple clauses rather than directly isolated, which slightly impairs conclusiveness.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer effectively covers key red flags associated with termination for cause, including non-payment, material breaches, and insolvency, aligning closely with the ground truth. However, the extracted clauses spread across different sections may benefit from better consolidation for clarity.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer covers several relevant clauses related to termination, including material breach, service suspension, and insolvency, aligning closely with the ground truth. However, while it is detailed, the different clauses are presented somewhat separately, which could slightly obscure a straightforward focus on \"Termination for Cause\" as described in the ground truth.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer correctly identifies page 4 as containing information relevant to \"Termination for Cause,\" specifically mentioning termination for uncured breaches and insolvency, which aligns with the ground truth. However, page 5 is also referenced in related clauses, which introduces a slight discrepancy for this specific term.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer effectively discusses the \"Termination for Cause\" clause by detailing how either party can terminate the agreement due to an irremediable material breach or failure to remedy a breach, which aligns with the concept of \"uncured breach\" from the ground truth. However, the specific mention of termination due to insolvency from the ground truth was not explicitly discussed, which prevents a perfect score.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer stays largely within the document scope, elaborating on different aspects related to 'Termination for Cause' including breach, insolvency, and non-payment scenarios. However, it somewhat deviates from focusing solely on 'Termination for Cause' by also addressing terms related to service suspension and agreement renewal, which slightly extends beyond the immediate scope of the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer accurately describes the conditions under which either party can terminate the agreement for cause, including uncured breaches or insolvency, aligning well with the ground truth. However, the details about the periods allowed for remediating breaches could potentially mislead if the focus is solely on the overarching term \"Termination for Cause,\" which generally prioritizes breach and insolvency without specific time frames.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer provides a detailed and contract-specific description of the termination clauses related to material breaches and insolvency, avoiding generic responses. However, it could be improved by explicitly matching the terminology used in the ground truth answer for more direct alignment.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The AI's extracted answer adheres to legal norms by justifying termination in cases of material breaches, non-payment, or bankruptcy, without introducing any illegal or insensitive conditions, thereby aligning appropriately with contractual expectations and norms.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted clauses provide a comprehensive outline of termination for cause, covering scenarios like material and uncured breaches, insolvency, and actions causing harm. Although some details mentioned expand beyond just breaches (e.g., non-payment and user activity), the core information aligns with the ground truth relating to termination for breach or insolvency.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted content about the 'Termination for Cause' does not contain any hateful or profane content. It is a straightforward legal description of the terms under which a contract may be terminated, focused purely on the contractual obligations and conditions.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer accurately captures the key payment details from the contract, including the requirement for the Customer to pay undisputed invoices within 30 days and the non-refundable nature of the fees, which corresponds well with the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer provides comprehensive information regarding the invoicing and payment terms, including the payment timeline, non-refundable nature of fees, and the currency of payment. However, it slightly exceeds the succinctness of the ground truth by mentioning the invoicing process on the Order Form effective date, which is not included in the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer provides all essential details required to make a conclusive decision regarding the 'Payment Terms' by clearly stating the invoicing process, payment timeline, currency information, and non-refundability of the fees. This is consistent with the ground truth answer, demonstrating a complete and comprehensive understanding of the payment terms.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer covers the key payment terms of invoicing and payment timelines, specifying that invoices must be paid within 30 days and are non-refundable, which aligns with the ground truth. However, it does not address potential red flags, such as the process for disputing invoices, which could have affected the score.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer includes all relevant information from the ground truth regarding the payment terms, such as the requirement for the customer to pay undisputed invoices within 30 days and the non-refundable nature of the fees. However, it also provides additional details regarding invoicing and currency that are not essential for the evaluation of \"Payment Terms\" specifically.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer correctly identifies the page number as 3, which matches the page number provided in the ground truth answer for the term 'Payment Terms.' Therefore, the extracted page number is accurate.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The AI reasoning directly discusses the relevant clause by summarizing the invoicing and payment terms, including the 30-day payment period and the non-refundable nature of the fees, which aligns precisely with the ground truth requirements for the \"Payment Terms.\"\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer stays well within the document scope as it provides detailed information about invoicing and payment, aligning with the ground truth answer. It includes the invoice time frame, non-refundability of fees, and payment currency, all of which are pertinent details relating to the \"Payment Terms.\"\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately reflects the key payment terms without making any misleading claims. It clearly states the obligation to pay within 30 days for undisputed invoices, mentions the non-refundable nature of the fees, and specifies the currency detail, aligning with the ground truth provided.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately identifies specific contract terms related to payment, such as the requirement for the customer to pay undisputed invoices within 30 days and that fees are non-refundable, thereby avoiding any generic or non-contractual language.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer clearly adheres to the legal requirements of invoicing and payment processes, specifying that the customer is required to pay undisputed invoices within 30 days of receipt. There are no illegal or insensitive justifications present in the provided terms, and it is consistent with the ground truth, ensuring clarity and adherence to standard contractual practices.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately reflects the ground truth by specifying that invoices must be paid within 30 days and that fees are non-refundable, without making any false claims about people or entities involved.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer contains no hateful or profane content and accurately conveys the necessary information regarding payment terms as outlined in the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3  \n",
      "Justification: The extracted answer correctly identifies the obligation to keep information confidential for 5 years, similar to the ground truth answer. However, it misses the perpetual confidentiality obligation specifically for software, which is a significant aspect of the ground truth. The extraction covers most critical points but lacks completeness.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3  \n",
      "Justification: The extracted answer describes the confidentiality obligations properly for general information, sharing that information must be kept confidential for 5 years, aligning with part of the ground truth. However, it does not mention the perpetual obligation related to software, indicating that the information is incomplete.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer provides sufficient information about the general confidentiality obligations by specifying both the duration and scope of confidentiality. However, it lacks detail on the perpetual confidentiality obligation specifically related to software, which is mentioned in the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3  \n",
      "Justification: The extracted answer covers the duration of confidentiality obligations by stating that confidential information must be treated as such during the term of the agreement and for 5 years thereafter, aligning with the ground truth. However, it fails to address the perpetual confidentiality for software, which is a critical aspect mentioned in the ground truth, thus presenting a potential red flag regarding incomplete information on the scope of confidentiality obligations.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3  \n",
      "Justification: The extracted answer partially covers the confidentiality obligations by mentioning the 5-year protection period but misses the perpetual obligation for software. Additionally, it describes the confidentiality management during the agreement term, which aligns with the ground truth, yet it does not fully encapsulate all aspects outlined in the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer lists the page numbers as 6-7, while the ground truth specifies page 6. Since the page numbers largely overlap, it suggests that the relevant information likely starts on page 6, adhering closely to the ground truth. However, the inclusion of page 7 introduces a minor discrepancy, preventing a perfect score.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer adequately discusses the relevant clause by addressing the confidentiality obligations, specifying the 5-year protection period for confidential information, and mentioning the use restriction to fulfill contractual obligations. However, it doesn't address the perpetual confidentiality requirement for software, which is a noted point in the ground truth answer, slightly reducing its completeness.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer mostly stays within the scope of the document, as it accurately reflects the confidentiality obligations during and after the agreement term. However, it misses the specific detail about perpetual confidentiality for software, which is included in the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 2  \n",
      "Justification: The extracted answer generally aligns with the ground truth regarding the protection period of five years but fails to mention the perpetual confidentiality obligation specifically related to software. This omission could mislead about the full extent of the confidentiality obligations, particularly for software, leading to potential misunderstandings about the contract's terms.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer appropriately reflects the contractual nature of the confidentiality obligations by specifying that confidentiality must be maintained for 5 years and that it involves using the information solely for fulfilling contractual obligations. However, it lacks specifics regarding perpetuity for software, mentioned in the ground truth, which slightly reduces its completeness.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The AI accurately summarized the confidentiality obligations without resorting to any illegal or insensitive justifications. It adhered to the requirement of treating confidential information appropriately and specified a time frame consistent with industry standards, aligning with the ground truth expectations.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer correctly identifies the 5-year confidentiality obligation, preventing any false claims about the duration of protection. However, it omits the perpetual confidentiality requirement for software, which slightly diminishes the accuracy concerning specifics related to all covered entities.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer contains no hateful or profane content; it plainly describes the contractual obligations related to confidentiality without any inappropriate language.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\blocks.py\", line 1945, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\blocks.py\", line 1768, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\components\\file.py\", line 204, in postprocess\n",
      "    orig_name=Path(value).name,\n",
      "              ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pathlib.py\", line 871, in __new__\n",
      "    self = cls._from_parts(args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pathlib.py\", line 509, in _from_parts\n",
      "    drv, root, parts = self._parse_args(args)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pathlib.py\", line 493, in _parse_args\n",
      "    a = os.fspath(a)\n",
      "        ^^^^^^^^^^^^\n",
      "TypeError: expected str, bytes or os.PathLike object, not BytesIO\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import io\n",
    "\n",
    "def get_human_table(df_all):\n",
    "    df_human = mark_evaluation_pass_fail(df_all)\n",
    "    return df_human[[\"key_term_name\", \"evulation_metric_name\", \"is_pass\", \"justification\"]]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# 📄 LLM Contract Judge\\nUpload a contract, extract key terms, and evaluate with LLM.\")\n",
    "    with gr.Row():\n",
    "        contract_file = gr.File(label=\"Upload Contract (PDF, DOCX, TXT)\")\n",
    "        ground_truth_file = gr.File(label=\"Upload Ground Truth File (TXT, CSV, etc.)\")\n",
    "    start_btn = gr.Button(\"Start Evaluating\")\n",
    "    extracted_text = gr.Textbox(label=\"Extracted Contract Text\", lines=10, interactive=False)\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"Helpful Metrics\"):\n",
    "            results_table1 = gr.Dataframe(headers=[\n",
    "                \"key_term_name\",\n",
    "                \"llm_extracted_ans_from_doc\",\n",
    "                \"llm_page_number\",\n",
    "                \"ground_truth_answer\",\n",
    "                \"evulation_metric_name\",\n",
    "                \"score\",\n",
    "                \"justification\"\n",
    "            ], label=\"Evaluation Results (Helpful Metrics)\")\n",
    "        with gr.TabItem(\"Honest Metrics\"):\n",
    "            results_table2 = gr.Dataframe(headers=[\n",
    "                \"key_term_name\",\n",
    "                \"llm_extracted_ans_from_doc\",\n",
    "                \"llm_page_number\",\n",
    "                \"ground_truth_answer\",\n",
    "                \"evulation_metric_name\",\n",
    "                \"score\",\n",
    "                \"justification\"\n",
    "            ], label=\"Evaluation Results (Honest Metrics)\")\n",
    "        with gr.TabItem(\"Harmless Metrics\"):\n",
    "            results_table3 = gr.Dataframe(headers=[\n",
    "                \"key_term_name\",\n",
    "                \"llm_extracted_ans_from_doc\",\n",
    "                \"llm_page_number\",\n",
    "                \"ground_truth_answer\",\n",
    "                \"evulation_metric_name\",\n",
    "                \"score\",\n",
    "                \"justification\"\n",
    "            ], label=\"Evaluation Results (Harmless Metrics)\")\n",
    "        with gr.TabItem(\"Human Evaluation\"):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                ### Human Evaluation (Yes/No)\n",
    "                - **Note:** Here we evaluate each metric in a Yes/No format.\n",
    "                - If the score is less than 3, it is marked as **No** (False); otherwise, it is **Yes** (True).\n",
    "                - This helps quickly identify which key terms and metrics pass the threshold for acceptability.\n",
    "                \"\"\"\n",
    "            )\n",
    "            human_table = gr.Dataframe(headers=[\n",
    "                \"key_term_name\",\n",
    "                \"evulation_metric_name\",\n",
    "                \"is_pass\",\n",
    "                \"justification\"\n",
    "            ], label=\"Human Evaluation (Yes/No)\")\n",
    "    download_btn = gr.Button(\"Download All Results as CSV\")\n",
    "    download_file = gr.File(label=\"Download CSV\")\n",
    "\n",
    "    # Define state objects for DataFrames\n",
    "    state_df1 = gr.State()\n",
    "    state_df2 = gr.State()\n",
    "    state_df3 = gr.State()\n",
    "    state_df_human = gr.State()\n",
    "\n",
    "    def run_and_return_tables(contract_file, ground_truth_file):\n",
    "        text, df1, df2, df3, df_all = process_documents(contract_file, ground_truth_file)\n",
    "        df_human = get_human_table(df_all)\n",
    "        # Convert is_pass to Yes/No for display\n",
    "        df_human = df_human.copy()\n",
    "        df_human[\"is_pass\"] = df_human[\"is_pass\"].apply(lambda x: \"Yes\" if x else \"No\")\n",
    "        return (\n",
    "            text,\n",
    "            gr.update(value=df1),\n",
    "            gr.update(value=df2),\n",
    "            gr.update(value=df3),\n",
    "            gr.update(value=df_human),\n",
    "            df1, df2, df3, df_human\n",
    "        )\n",
    "\n",
    "    def download_csv(contract_file, ground_truth_file, df1, df2, df3, df_human):\n",
    "        import tempfile\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "        # Create a temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\", mode=\"w\", encoding=\"utf-8\") as tmp:\n",
    "            combined.to_csv(tmp, index=False)\n",
    "            tmp_path = tmp.name\n",
    "        return tmp_path\n",
    "\n",
    "    start_btn.click(\n",
    "        run_and_return_tables,\n",
    "        inputs=[contract_file, ground_truth_file],\n",
    "        outputs=[extracted_text, results_table1, results_table2, results_table3, human_table, state_df1, state_df2, state_df3, state_df_human]\n",
    "    )\n",
    "    download_btn.click(\n",
    "        download_csv,\n",
    "        inputs=[contract_file, ground_truth_file, state_df1, state_df2, state_df3, state_df_human],\n",
    "        outputs=download_file\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
