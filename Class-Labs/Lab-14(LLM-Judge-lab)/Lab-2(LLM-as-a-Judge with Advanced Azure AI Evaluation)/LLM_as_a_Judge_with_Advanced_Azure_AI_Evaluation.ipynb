{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHkGbKJFexWM"
      },
      "source": [
        "# LLM-as-a-Judge with Advanced Azure AI Evaluation\n",
        "\n",
       "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sachin0034/MLAI-community-labs/blob/main/Class-Labs/Lab-14(LLM-Judge-lab)/Lab-2(LLM-as-a-Judge%20with%20Advanced%20Azure%20AI%20Evaluation)/LLM_as_a_Judge_with_Advanced_Azure_AI_Evaluation.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In previous iterations of our Legal Document Analyzer, we leveraged OpenAI models for both the extraction of key terms from legal contracts and their subsequent evaluation. As demonstrated in our prior lab, this approach provided initial insights into the capabilities of large language models for legal text analysis. Building upon that foundation, this current project significantly enhances our evaluation methodology by integrating pre-trained evaluators from Azure AI. This shift allows us to utilize specialized, robust metrics for assessing the quality of our LLM's extractions, specifically focusing on aspects like groundedness, coherence, relevance, and fluency, thereby providing a more comprehensive and nuanced understanding of model performance.\n",
        "\n",
        "# Intro to Azure AI Evaluation\n",
        "Azure AI evaluations are a set of tools and features within Azure AI Studio designed to assess the performance and quality of generative AI models and applications. They provide a structured way to measure various aspects of AI responses, including accuracy, groundedness, and safety, using both built-in and customizable metrics.\n",
        "\n",
        "[Read more about Azure AI Evaluation](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/evaluate-sdk)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l07wygtljVy5"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "> ‚ö†Ô∏è **Note:** Make sure you have an active **Azure subscription account**. Without it, you won't be able to run this notebook with Azure integrations.  \n",
        "\n",
        "---\n",
        "\n",
        "Before you get started, please make sure you have the following ready:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Sample Contract File for Testing\n",
        "\n",
        "To try out the contract analysis workflow, download the sample contract file provided below:\n",
        "\n",
        "- [Download Sample Contract (Google Drive)](https://drive.google.com/file/d/1E557kdNBZ5cDUvVDLNrEVRuKcRSYDG3Z/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Ground Truth CSV File\n",
        "\n",
        "Download the ground truth CSV file from the link below:\n",
        "\n",
        "- [Download Ground Truth CSV (Google Drive)](https://drive.google.com/file/d/1E557kdNBZ5cDUvVDLNrEVRuKcRSYDG3Z/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. OpenAI API Key\n",
        "\n",
        "You‚Äôll need your own OpenAI API key to access the language models used for contract evaluation. If you don‚Äôt have one yet, follow this step-by-step guide to generate your API key:\n",
        "\n",
        "- [How to get your own OpenAI API key (Medium article)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Azure Setup (for Azure OpenAI / Azure AI Foundry Users)\n",
        "\n",
        "If you're using Azure services, ensure you have the following:\n",
        "\n",
        "- ‚úÖ **An active Azure subscription**\n",
        "- ‚úÖ **Sufficient Azure portal permissions** (Contributor, Cognitive Services Contributor, or Owner at the subscription level)\n",
        "- ‚úÖ **An Azure AI Foundry resource or an Azure OpenAI resource deployed**\n",
        "\n",
        "---\n",
        "\n",
        "### üìò Useful Documentation Links\n",
        "\n",
        "**Configure and Connect to Azure AI Foundry:**\n",
        "- [Create a new Azure AI Foundry project (Official docs)](https://learn.microsoft.com/en-us/azure/ai-services/foundry/how-to-create-project)\n",
        "- [Configure a connection to use Azure AI Foundry Models](https://learn.microsoft.com/en-us/azure/ai-services/foundry/how-to-use-model)\n",
        "\n",
        "**Azure OpenAI API Reference:**\n",
        "- [Azure OpenAI parameters and authentication](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference)\n",
        "- [LangChain `AzureChatOpenAI` Configuration Example](https://python.langchain.com/docs/integrations/chat/azure_openai)\n",
        "\n",
        "**General Azure AI Setup Guide:**\n",
        "- [Configure your environment for Azure AI resources](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/configure-environment)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFHN54u5V6hg"
      },
      "source": [
        "# üì¶ Cell 1 - Installing the Essentials\n",
        "\n",
        "Hey there! üëã  \n",
        "Before we dive into the magic, let‚Äôs pack our toolkit. This cell installs all the essential Python packages we'll need on our journey.  \n",
        "Think of it as preparing your inventory before entering the dungeon. üßô‚Äç‚ôÇÔ∏è‚öîÔ∏èüõ°Ô∏è\n",
        "\n",
        "| Package               | What it Does                                                                 |\n",
        "|-----------------------|------------------------------------------------------------------------------|\n",
        "| `langchain`           | Your command center for building with language models. Makes chaining prompts a breeze! |\n",
        "| `pypdf`               | Helps you read and extract text from PDF files. Like opening scrolls of ancient text. üìú |\n",
        "| `docx2txt`            | Extracts text from `.docx` files‚ÄîWord documents won't hide anything from you now! |\n",
        "| `pandas`              | Powerful tool for data manipulation and analysis. Like Excel, but 100x cooler. üìä |\n",
        "| `openai`              | The official package to talk to OpenAI‚Äôs powerful GPT models. Say hello to your AI buddy! üëã |\n",
        "| `gradio`              | Quickly build web UIs to interact with your models. Test your ideas live, instantly! ‚ö° |\n",
        "| `azure-ai-generative`| Enables you to connect with Azure‚Äôs AI services. If you‚Äôre an Azure fan, you‚Äôll love this. ‚òÅÔ∏è |\n",
        "\n",
        "And here's the magic wand to install them all at once:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TQewGxC4_EYu",
        "outputId": "a5a6bd06-2cc7-441b-ad1e-65e93633513e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.8.0)\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.11/dist-packages (0.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.96.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: azure-ai-generative in /usr/local/lib/python3.11/dist-packages (1.0.0b11)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.69)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: azure-ai-resources>=1.0.0b7 in /usr/local/lib/python3.11/dist-packages (from azure-ai-generative) (1.0.0b9)\n",
            "Requirement already satisfied: mlflow-skinny<3,>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-generative) (2.22.1)\n",
            "Requirement already satisfied: opencensus-ext-azure~=1.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-generative) (1.1.15)\n",
            "Requirement already satisfied: opencensus-ext-logging<=0.1.1 in /usr/local/lib/python3.11/dist-packages (from azure-ai-generative) (0.1.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: azure-ai-ml>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-resources>=1.0.0b7->azure-ai-generative) (1.28.1)\n",
            "Requirement already satisfied: azure-mgmt-resource<23.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-resources>=1.0.0b7->azure-ai-generative) (22.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (0.59.0)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (1.35.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (5.29.5)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative) (0.5.3)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from opencensus-ext-azure~=1.0->azure-ai-generative) (1.35.0)\n",
            "Requirement already satisfied: azure-identity<2.0.0,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from opencensus-ext-azure~=1.0->azure-ai-generative) (1.23.1)\n",
            "Requirement already satisfied: opencensus<1.0.0,>=0.11.4 in /usr/local/lib/python3.11/dist-packages (from opencensus-ext-azure~=1.0->azure-ai-generative) (0.11.4)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.11/dist-packages (from opencensus-ext-azure~=1.0->azure-ai-generative) (5.9.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: msrest<1.0.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.7.1)\n",
            "Requirement already satisfied: azure-mgmt-core>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (1.6.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.5 in /usr/local/lib/python3.11/dist-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (3.26.1)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (4.24.0)\n",
            "Requirement already satisfied: strictyaml<2.0.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (1.7.3)\n",
            "Requirement already satisfied: colorama<1.0.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.4.6)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (2.10.1)\n",
            "Requirement already satisfied: azure-storage-blob>=12.10.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (12.26.0)\n",
            "Requirement already satisfied: azure-storage-file-share in /usr/local/lib/python3.11/dist-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (12.22.0)\n",
            "Requirement already satisfied: azure-storage-file-datalake>=12.2.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (12.21.0)\n",
            "Requirement already satisfied: pydash<9.0.0,>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (7.0.7)\n",
            "Requirement already satisfied: isodate<1.0.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.7.2)\n",
            "Requirement already satisfied: azure-common>=1.1 in /usr/local/lib/python3.11/dist-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (1.1.28)\n",
            "Requirement already satisfied: azure-monitor-opentelemetry in /usr/local/lib/python3.11/dist-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (1.6.7)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.11/dist-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure~=1.0->azure-ai-generative) (43.0.3)\n",
            "Requirement already satisfied: msal>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure~=1.0->azure-ai-generative) (1.32.3)\n",
            "Requirement already satisfied: msal-extensions>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure~=1.0->azure-ai-generative) (1.3.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (2.38.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from opencensus<1.0.0,>=0.11.4->opencensus-ext-azure~=1.0->azure-ai-generative) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus<1.0.0,>=0.11.4->opencensus-ext-azure~=1.0->azure-ai-generative) (2.25.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (0.56b0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure~=1.0->azure-ai-generative) (1.17.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (5.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure~=1.0->azure-ai-generative) (1.70.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure~=1.0->azure-ai-generative) (1.26.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (4.9.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.26.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from msrest<1.0.0,>=0.6.18->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (2.0.0)\n",
            "Requirement already satisfied: azure-core-tracing-opentelemetry~=1.0.0b11 in /usr/local/lib/python3.11/dist-packages (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (1.0.0b12)\n",
            "Requirement already satisfied: azure-monitor-opentelemetry-exporter~=1.0.0b31 in /usr/local/lib/python3.11/dist-packages (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (1.0.0b40)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-django~=0.53b0 in /usr/local/lib/python3.11/dist-packages (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.56b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi~=0.53b0 in /usr/local/lib/python3.11/dist-packages (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.56b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-flask~=0.53b0 in /usr/local/lib/python3.11/dist-packages (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.56b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-psycopg2~=0.53b0 in /usr/local/lib/python3.11/dist-packages (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.56b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-requests~=0.53b0 in /usr/local/lib/python3.11/dist-packages (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.56b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-urllib~=0.53b0 in /usr/local/lib/python3.11/dist-packages (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.56b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-urllib3~=0.53b0 in /usr/local/lib/python3.11/dist-packages (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.56b0)\n",
            "Requirement already satisfied: opentelemetry-resource-detector-azure~=0.1.4 in /usr/local/lib/python3.11/dist-packages (from azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.1.5)\n",
            "Requirement already satisfied: fixedint==0.1.6 in /usr/local/lib/python3.11/dist-packages (from azure-monitor-opentelemetry-exporter~=1.0.0b31->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.1.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure~=1.0->azure-ai-generative) (2.22)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-wsgi==0.56b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-django~=0.53b0->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.56b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.56b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-django~=0.53b0->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.56b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.56b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-django~=0.53b0->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.56b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.56b0->opentelemetry-instrumentation-django~=0.53b0->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (1.17.2)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.56b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi~=0.53b0->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.56b0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.56b0->opentelemetry-instrumentation-fastapi~=0.53b0->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (3.9.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-dbapi==0.56b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-psycopg2~=0.53b0->azure-monitor-opentelemetry->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (0.56b0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.5.0->msrest<1.0.0,>=0.6.18->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative) (3.3.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain pypdf docx2txt pandas openai gradio azure-ai-generative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTbTwBCRWF0G"
      },
      "source": [
        "# üì¶ Cell 2 - Bringing the Community Power with `langchain-community`\n",
        "\n",
        "Hey, welcome back! üëã  \n",
        "Now that we‚Äôve got the base tools ready, let‚Äôs add something special: the **LangChain Community** package! üåç‚ú®\n",
        "\n",
        "This package includes connectors and integrations built by the awesome LangChain community‚Äî  \n",
        "so you get access to a wider range of tools, wrappers, loaders, and chains that aren't part of the core library.\n",
        "\n",
        "| Package               | What it Does                                                                 |\n",
        "|-----------------------|------------------------------------------------------------------------------|\n",
        "| `langchain-community`| A collection of community-contributed tools and integrations for LangChain. Think of it as a treasure chest üß∞ full of plugins to level up your LangChain experience!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AoakXxYk_Skr",
        "outputId": "bb8f2726-42f3-453b-eedb-0307ef1c2b50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.69)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.6)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 mypy-extensions-1.1.0 pydantic-settings-2.10.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUS9M6g9WL0l"
      },
      "source": [
        "# üß™ Cell 3 - Evaluating Like a Pro with `azure-ai-evaluation`\n",
        "\n",
        "Hey again! üéØ  \n",
        "Now that you've got tools to build cool LLM apps, let‚Äôs add something that helps you **evaluate how well your AI is doing**. Because hey, building is one thing‚Äîbut knowing if it actually works well? That's next-level! üí°\n",
        "\n",
        "In this cell, we install:\n",
        "\n",
        "| Package               | What it Does                                                                 |\n",
        "|-----------------------|------------------------------------------------------------------------------|\n",
        "| `azure-ai-evaluation` | A Microsoft Azure package to help you evaluate AI models and workflows. Great for benchmarking, scoring, and analyzing LLM output quality in a more structured and automated way!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZCQmqFegG1P2",
        "outputId": "6904cb49-1109-430d-edd1-7fa036eecd6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: azure-ai-evaluation in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: promptflow-devkit>=1.17.1 in /usr/local/lib/python3.11/dist-packages (from azure-ai-evaluation) (1.18.1)\n",
            "Requirement already satisfied: promptflow-core>=1.17.1 in /usr/local/lib/python3.11/dist-packages (from azure-ai-evaluation) (1.18.1)\n",
            "Requirement already satisfied: pyjwt>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-evaluation) (2.10.1)\n",
            "Requirement already satisfied: azure-identity>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-evaluation) (1.23.1)\n",
            "Requirement already satisfied: azure-core>=1.30.2 in /usr/local/lib/python3.11/dist-packages (from azure-ai-evaluation) (1.35.0)\n",
            "Requirement already satisfied: nltk>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from azure-ai-evaluation) (3.9.1)\n",
            "Requirement already satisfied: azure-storage-blob>=12.10.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-evaluation) (12.26.0)\n",
            "Requirement already satisfied: httpx>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from azure-ai-evaluation) (0.28.1)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from azure-ai-evaluation) (2.2.2)\n",
            "Requirement already satisfied: openai>=1.78.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-evaluation) (1.96.1)\n",
            "Requirement already satisfied: ruamel.yaml<1.0.0,>=0.17.10 in /usr/local/lib/python3.11/dist-packages (from azure-ai-evaluation) (0.18.14)\n",
            "Requirement already satisfied: msrest>=0.6.21 in /usr/local/lib/python3.11/dist-packages (from azure-ai-evaluation) (0.7.1)\n",
            "Requirement already satisfied: Jinja2>=3.1.6 in /usr/local/lib/python3.11/dist-packages (from azure-ai-evaluation) (3.1.6)\n",
            "Requirement already satisfied: aiohttp>=3.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-evaluation) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.0->azure-ai-evaluation) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.0->azure-ai-evaluation) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.0->azure-ai-evaluation) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.0->azure-ai-evaluation) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.0->azure-ai-evaluation) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.0->azure-ai-evaluation) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.0->azure-ai-evaluation) (1.20.1)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.2->azure-ai-evaluation) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.2->azure-ai-evaluation) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.2->azure-ai-evaluation) (4.14.1)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.11/dist-packages (from azure-identity>=1.16.0->azure-ai-evaluation) (43.0.3)\n",
            "Requirement already satisfied: msal>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity>=1.16.0->azure-ai-evaluation) (1.32.3)\n",
            "Requirement already satisfied: msal-extensions>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity>=1.16.0->azure-ai-evaluation) (1.3.1)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob>=12.10.0->azure-ai-evaluation) (0.7.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.1->azure-ai-evaluation) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.1->azure-ai-evaluation) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.1->azure-ai-evaluation) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.1->azure-ai-evaluation) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.1->azure-ai-evaluation) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.6->azure-ai-evaluation) (3.0.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from msrest>=0.6.21->azure-ai-evaluation) (2.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->azure-ai-evaluation) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->azure-ai-evaluation) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->azure-ai-evaluation) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->azure-ai-evaluation) (4.67.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.78.0->azure-ai-evaluation) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.78.0->azure-ai-evaluation) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.78.0->azure-ai-evaluation) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.78.0->azure-ai-evaluation) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation) (2025.2)\n",
            "Requirement already satisfied: docstring_parser in /usr/local/lib/python3.11/dist-packages (from promptflow-core>=1.17.1->azure-ai-evaluation) (0.16)\n",
            "Requirement already satisfied: fastapi<1.0.0,>=0.109.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-core>=1.17.1->azure-ai-evaluation) (0.116.1)\n",
            "Requirement already satisfied: filetype>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-core>=1.17.1->azure-ai-evaluation) (1.2.0)\n",
            "Requirement already satisfied: flask<4.0.0,>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from promptflow-core>=1.17.1->azure-ai-evaluation) (3.1.1)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-core>=1.17.1->azure-ai-evaluation) (4.24.0)\n",
            "Requirement already satisfied: promptflow-tracing==1.18.1 in /usr/local/lib/python3.11/dist-packages (from promptflow-core>=1.17.1->azure-ai-evaluation) (1.18.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from promptflow-core>=1.17.1->azure-ai-evaluation) (5.9.5)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-tracing==1.18.1->promptflow-core>=1.17.1->azure-ai-evaluation) (1.35.0)\n",
            "Requirement already satisfied: tiktoken>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-tracing==1.18.1->promptflow-core>=1.17.1->azure-ai-evaluation) (0.9.0)\n",
            "Requirement already satisfied: argcomplete>=3.2.3 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (3.6.2)\n",
            "Requirement already satisfied: azure-monitor-opentelemetry-exporter<2.0.0,>=1.0.0b21 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (1.0.0b40)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (0.4.6)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (3.18.0)\n",
            "Requirement already satisfied: flask-cors<7.0.0,>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (6.0.1)\n",
            "Requirement already satisfied: flask-restx<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (1.3.0)\n",
            "Requirement already satisfied: gitpython<4.0.0,>=3.1.24 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (3.1.44)\n",
            "Requirement already satisfied: keyring<25.0.0,>=24.2.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (24.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.5 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (3.26.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (1.35.0)\n",
            "Requirement already satisfied: pillow<11.1.0,>=10.1.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (11.0.0)\n",
            "Requirement already satisfied: pydash<8.0.0,>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (7.0.7)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (1.1.1)\n",
            "Requirement already satisfied: sqlalchemy<3.0.0,>=1.4.48 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (2.0.41)\n",
            "Requirement already satisfied: strictyaml<2.0.0,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (1.7.3)\n",
            "Requirement already satisfied: tabulate<1.0.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (0.9.0)\n",
            "Requirement already satisfied: waitress<4.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from promptflow-devkit>=1.17.1->azure-ai-evaluation) (3.0.2)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml<1.0.0,>=0.17.10->azure-ai-evaluation) (0.2.12)\n",
            "Requirement already satisfied: fixedint==0.1.6 in /usr/local/lib/python3.11/dist-packages (from azure-monitor-opentelemetry-exporter<2.0.0,>=1.0.0b21->promptflow-devkit>=1.17.1->azure-ai-evaluation) (0.1.6)\n",
            "Requirement already satisfied: opentelemetry-api~=1.26 in /usr/local/lib/python3.11/dist-packages (from azure-monitor-opentelemetry-exporter<2.0.0,>=1.0.0b21->promptflow-devkit>=1.17.1->azure-ai-evaluation) (1.35.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.5->azure-identity>=1.16.0->azure-ai-evaluation) (1.17.1)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1.0.0,>=0.109.0->promptflow-core>=1.17.1->azure-ai-evaluation) (0.47.1)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask<4.0.0,>=2.2.3->promptflow-core>=1.17.1->azure-ai-evaluation) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask<4.0.0,>=2.2.3->promptflow-core>=1.17.1->azure-ai-evaluation) (2.2.0)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask<4.0.0,>=2.2.3->promptflow-core>=1.17.1->azure-ai-evaluation) (3.1.3)\n",
            "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.11/dist-packages (from flask-restx<2.0.0,>=1.2.0->promptflow-devkit>=1.17.1->azure-ai-evaluation) (10.0.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from flask-restx<2.0.0,>=1.2.0->promptflow-devkit>=1.17.1->azure-ai-evaluation) (6.5.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4.0.0,>=3.1.24->promptflow-devkit>=1.17.1->azure-ai-evaluation) (4.0.12)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->promptflow-core>=1.17.1->azure-ai-evaluation) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->promptflow-core>=1.17.1->azure-ai-evaluation) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->promptflow-core>=1.17.1->azure-ai-evaluation) (0.26.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.11/dist-packages (from keyring<25.0.0,>=24.2.0->promptflow-devkit>=1.17.1->azure-ai-evaluation) (3.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.11.4 in /usr/local/lib/python3.11/dist-packages (from keyring<25.0.0,>=24.2.0->promptflow-devkit>=1.17.1->azure-ai-evaluation) (8.7.0)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.11/dist-packages (from keyring<25.0.0,>=24.2.0->promptflow-devkit>=1.17.1->azure-ai-evaluation) (3.3.3)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from keyring<25.0.0,>=24.2.0->promptflow-devkit>=1.17.1->azure-ai-evaluation) (0.9.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.5->promptflow-devkit>=1.17.1->azure-ai-evaluation) (24.2)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->promptflow-devkit>=1.17.1->azure-ai-evaluation) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->promptflow-devkit>=1.17.1->azure-ai-evaluation) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.35.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->promptflow-devkit>=1.17.1->azure-ai-evaluation) (1.35.0)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-proto==1.35.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->promptflow-devkit>=1.17.1->azure-ai-evaluation) (5.29.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.78.0->azure-ai-evaluation) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.78.0->azure-ai-evaluation) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.78.0->azure-ai-evaluation) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.2->azure-ai-evaluation) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.2->azure-ai-evaluation) (2.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-evaluation) (3.3.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3.0.0,>=1.4.48->promptflow-devkit>=1.17.1->azure-ai-evaluation) (3.2.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity>=1.16.0->azure-ai-evaluation) (2.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.24->promptflow-devkit>=1.17.1->azure-ai-evaluation) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.11.4->keyring<25.0.0,>=24.2.0->promptflow-devkit>=1.17.1->azure-ai-evaluation) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<2.0.0,>=1.22.0->promptflow-tracing==1.18.1->promptflow-core>=1.17.1->azure-ai-evaluation) (0.56b0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from jaraco.classes->keyring<25.0.0,>=24.2.0->promptflow-devkit>=1.17.1->azure-ai-evaluation) (10.7.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install azure-ai-evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKlYd7KWWdlF"
      },
      "source": [
        "# üèóÔ∏è Cell 4 - Getting Set Up: Imports & Libraries\n",
        "\n",
        "Hey hey! üëã You've made it to Cell 4 ‚Äî nice work!  \n",
        "This one is like unpacking your toolbox üß∞ ‚Äî we‚Äôre importing everything we‚Äôll need to build, analyze, and serve your LLM-based app.\n",
        "\n",
        "Let‚Äôs break down what‚Äôs happening here:\n",
        "\n",
        "| Module / Package                             | Why We Need It                                                                 |\n",
        "|---------------------------------------------|--------------------------------------------------------------------------------|\n",
        "| `os`, `io`, `tempfile`, `re`, `json`, `time`| Core Python modules for file handling, regex, JSON manipulation, and timing. Basic stuff, but super powerful! |\n",
        "| `pandas as pd`                               | For tabular data handling and display. Think of it as Excel in code form. üßÆ     |\n",
        "| `gradio as gr`                               | To build a beautiful web-based UI for your app. Plug and play GUI components! üéõÔ∏è |\n",
        "| `langchain.document_loaders`                | We‚Äôre loading text from PDFs, Word docs, and plain `.txt` files. Time to feed the LLM! üìÑ |\n",
        "| `langchain.text_splitter`                   | Splits large chunks of text into manageable pieces. No more overwhelming the poor model! ‚úÇÔ∏è |\n",
        "| `langchain.schema.Document`                 | Represents a single document in a structured way. Used downstream for processing. üìö |\n",
        "| `openai` and error classes                  | Used to interact with OpenAI‚Äôs GPT models and handle API-related errors gracefully. ü§ñ‚ùå |\n",
        "| `azure.ai.evaluation`                       | Imports evaluation tools like:  \n",
        "‚Üí `GroundednessEvaluator`: Checks factual grounding  \n",
        "‚Üí `CoherenceEvaluator`: Checks flow and consistency  \n",
        "‚Üí `RelevanceEvaluator`: Checks how on-topic the response is  \n",
        "‚Üí `FluencyEvaluator`: Checks grammar and readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnT_VmDaQmRz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import tempfile\n",
        "import re\n",
        "import json\n",
        "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pandas as pd\n",
        "from langchain.schema import Document\n",
        "from openai import OpenAI\n",
        "from openai import APIError, APIConnectionError, RateLimitError\n",
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "# Azure AI Evaluation imports\n",
        "from azure.ai.evaluation import GroundednessEvaluator, CoherenceEvaluator, RelevanceEvaluator, FluencyEvaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3cTC3YeWnZ9"
      },
      "source": [
        "# üîê Cell 5 - Setting Up OpenAI & Azure Connections\n",
        "\n",
        "Now that we‚Äôve got all the imports ready, it‚Äôs time to connect with the powerful brains behind your app: **OpenAI** and **Azure OpenAI Service**.\n",
        "\n",
        "Let‚Äôs walk through what‚Äôs happening here:\n",
        "\n",
        "| Section                       | What It Does                                                                 |\n",
        "|------------------------------|------------------------------------------------------------------------------|\n",
        "| **OpenAI Client Initialization** | This sets up a secure connection to OpenAI's API using your API key. This client is what you‚Äôll use to send prompts and get responses from GPT models. ü§ñ |\n",
        "| **Azure AI Project Setup**     | You define the Azure AI project info here ‚Äî subscription ID, resource group, and project name. This ensures Azure knows where your project lives. üåç |\n",
        "| **Azure OpenAI Model Config**  | You specify important config like the endpoint, API key, deployment name (e.g., `gpt-4o-mini`), and API version. This tells your app which model to talk to and where. ‚öôÔ∏è |\n",
        "\n",
        "### ‚úÖ A Few Notes:\n",
        "\n",
        "- Make sure you **keep your API keys secret** üîê (like we just did here). Never expose them in public repos.\n",
        "- Ensure you **place all your keys and config values in a `.env` file or a secure secrets manager** ‚Äî **never hard-code them directly in scripts**. üîë\n",
        "- The values in the Azure config are needed to authenticate and route your request to the right deployment.\n",
        "- Using `gpt-4o-mini`? Awesome choice! It's fast and lightweight, but still powerful for evaluations and generation tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "815aPc_IQqHM"
      },
      "outputs": [],
      "source": [
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key='Insert Your API key')\n",
        "# Initialize Azure AI project and Azure OpenAI connection with your environment variables\n",
        "azure_ai_project = {\n",
        "    \"subscription_id\": \"Insert your subscription_id\",\n",
        "    \"resource_group_name\": \"Insert your resource_group_name\",\n",
        "    \"project_name\": \"Insert your project_name\",\n",
        "}\n",
        "\n",
        "model_config = {\n",
        "    \"azure_endpoint\": \"Insert your azure_endpoint\",\n",
        "    \"api_key\": \"Insert your api_key\",\n",
        "    \"azure_deployment\": \"Insert your azure_deployment\",\n",
        "    \"api_version\": \"Insert your api_version\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eyUTyS8WuI4"
      },
      "source": [
        "# üß† Cell 6 - Time to Evaluate! Azure AI Evaluators + Key Terms\n",
        "\n",
        "Hey again, look at you flying through this! ‚úàÔ∏è  \n",
        "In this cell, we‚Äôre setting up **automated evaluation tools** powered by Azure ‚Äî because building smart apps is cool, but building *smart apps that know they‚Äôre smart*? That‚Äôs elite. üß™üí°\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ What‚Äôs Happening in This Cell:\n",
        "\n",
        "| Element                         | What It Does                                                                 |\n",
        "|---------------------------------|------------------------------------------------------------------------------|\n",
        "| `GroundednessEvaluator`         | Checks if the AI‚Äôs response is **fact-based and grounded in source content**. Super useful to avoid hallucinations. üìö |\n",
        "| `CoherenceEvaluator`            | Evaluates if the response is **logically consistent** and makes sense from start to finish. üß© |\n",
        "| `RelevanceEvaluator`            | Measures how **on-topic** the response is with respect to the prompt or context. üéØ |\n",
        "| `FluencyEvaluator`              | Assesses grammar, structure, and readability. Helps you polish your AI output to sound natural and professional. ‚úçÔ∏è |\n",
        "| `KEY_TERMS`                     | A list of important contractual/legal terms to focus on. These will likely be used later to search or filter documents. üìú |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why This Matters:\n",
        "\n",
        "These evaluators allow your app to:\n",
        "- Auto-score and critique LLM outputs\n",
        "- Enforce higher quality standards\n",
        "- Pinpoint weak spots in the response (grounding, clarity, relevance, etc.)\n",
        "\n",
        "‚ú® **Pro tip:** Automating these evaluations helps in QA workflows, model comparison, and ensuring your product stays reliable as it scales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSyoUqzoQsyK"
      },
      "outputs": [],
      "source": [
        "# Initialize Azure evaluators\n",
        "groundedness_eval = GroundednessEvaluator(model_config=model_config)\n",
        "coherence_eval = CoherenceEvaluator(model_config=model_config)\n",
        "relevance_eval = RelevanceEvaluator(model_config=model_config)\n",
        "fluency_eval = FluencyEvaluator(model_config=model_config)\n",
        "\n",
        "KEY_TERMS = [\n",
        "    \"Service Warranty\",\n",
        "    \"Limitation of Liability\",\n",
        "    \"Governing Law\",\n",
        "    \"Termination for Cause\",\n",
        "    \"Payment Terms\",\n",
        "    \"Confidentiality Obligations\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehFhZ7ktW8UP"
      },
      "source": [
        "# üßæ Cell 7 - Extracting & Chunking Your Documents\n",
        "\n",
        "Now that your evaluators are ready to judge, let‚Äôs give them something to work with! This cell focuses on **loading documents** and **prepping them for AI consumption**.\n",
        "\n",
        "---\n",
        "\n",
        "## üì§ Function 1: `extract_text_from_file(file_path)`\n",
        "\n",
        "This function is your universal document reader ‚Äî it handles PDFs, Word docs, text files, and even CSVs.\n",
        "\n",
        "| Parameter        | What It Means                         |\n",
        "|------------------|----------------------------------------|\n",
        "| `file_path`      | The full path to the document you want to extract text from. |\n",
        "\n",
        "### üîÑ What It Does:\n",
        "- Checks the file extension\n",
        "- Uses the appropriate **LangChain loader** for `.pdf`, `.docx`, `.txt`\n",
        "- Converts `.csv` files to strings using Pandas\n",
        "- Returns:\n",
        "  - `text`: A raw string of all the content\n",
        "  - `docs`: A list of `Document` objects (LangChain‚Äôs format for handling documents)\n",
        "\n",
        "### üõë Error Handling Features:\n",
        "- Gracefully catches:\n",
        "  - Missing files\n",
        "  - Empty or malformed CSVs\n",
        "  - Any other sneaky exceptions\n",
        "- Raises clean errors and logs helpful messages\n",
        "\n",
        "üéØ **Why it's cool:**  \n",
        "This function makes your app compatible with a **variety of file types** without having to write separate logic for each one.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÇÔ∏è Function 2: `chunk_document(docs, chunk_size=1000, chunk_overlap=200)`\n",
        "\n",
        "Alright, now that you've got a document loaded ‚Äî it's probably huge, right? This function breaks it into digestible **chunks** for the LLM to handle better.\n",
        "\n",
        "| Parameter         | What It Means                                                        |\n",
        "|-------------------|-----------------------------------------------------------------------|\n",
        "| `docs`            | List of LangChain `Document` objects (from the previous function)     |\n",
        "| `chunk_size`      | Max number of characters per chunk                                    |\n",
        "| `chunk_overlap`   | Number of overlapping characters between chunks (helps with context!) |\n",
        "\n",
        "### üîß How It Works:\n",
        "- Uses `RecursiveCharacterTextSplitter` from LangChain\n",
        "- Breaks big docs into multiple smaller pieces\n",
        "- Adds overlap so no context is lost between chunks\n",
        "\n",
        "üí° **Use case tip:** This chunking is **essential** when you want to pass documents to an LLM without overwhelming it with too much content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_LLea9sQzwo"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_file(file_path):\n",
        "    \"\"\"\n",
        "    Extracts text from various file types with improved error handling.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the extracted text (str) and a list of Document objects.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the file type is unsupported.\n",
        "        Exception: For errors during file reading or processing.\n",
        "    \"\"\"\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    try:\n",
        "        if ext == \".pdf\":\n",
        "            loader = PyPDFLoader(file_path)\n",
        "            docs = loader.load()\n",
        "            text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        elif ext in [\".docx\", \".doc\"]:\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "            docs = loader.load()\n",
        "            text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        elif ext in [\".txt\"]:\n",
        "            loader = TextLoader(file_path)\n",
        "            docs = loader.load()\n",
        "            text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        elif ext == \".csv\":\n",
        "            df = pd.read_csv(file_path)\n",
        "            text = df.to_string(index=False)\n",
        "            docs = [Document(page_content=text, metadata={\"page\": \"N/A\"})]\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file type\")\n",
        "        return text, docs\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        raise\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"Error: CSV file is empty or malformed at {file_path}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the file {file_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "def chunk_document(docs, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Splits a list of Document objects into smaller chunks.\n",
        "\n",
        "    Args:\n",
        "        docs (list): A list of Langchain Document objects.\n",
        "        chunk_size (int): The maximum size of each chunk in characters.\n",
        "        chunk_overlap (int): The number of characters to overlap between chunks.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of smaller Langchain Document chunks.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6MSzsb_XL-Y"
      },
      "source": [
        "# üîÑ Cell 8 - Making LLM Calls with Retry Logic (Because Errors Happen!)\n",
        "\n",
        "You're about to learn how to make your app **error-tolerant** and **resilient** when communicating with OpenAI's API. This cell defines a function that retries the call if something goes wrong. Think of it as a polite \"try again please\" mechanism for your LLM.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Function: `query_llm_with_retry(...)`\n",
        "\n",
        "This function sends a chat message to the LLM (like GPT-4o), and if the API hiccups ‚Äî it *tries again*, up to a maximum number of retries. üí™\n",
        "\n",
        "| Parameter       | What It Does                                                                 |\n",
        "|-----------------|--------------------------------------------------------------------------------|\n",
        "| `messages`      | A list of chat messages in the format OpenAI‚Äôs Chat API expects. üì®             |\n",
        "| `model`         | The LLM model to use (default is `\"gpt-4o\"` ‚Äî great choice!) ü§ñ                 |\n",
        "| `max_retries`   | How many times to retry the call if it fails. Defaults to 3. üîÅ                |\n",
        "| `delay`         | Initial wait time (in seconds) between retries. Increases exponentially. ‚è≥    |\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ What's Happening Internally:\n",
        "\n",
        "- Wraps the LLM call in a loop with try-except blocks.\n",
        "- If an API error occurs (like `RateLimitError`, `APIConnectionError`, or a general `APIError`), it:\n",
        "  - Logs the error with a message.\n",
        "  - Waits using exponential backoff (`delay * (2^i)`)\n",
        "  - Tries again until `max_retries` is hit.\n",
        "- If it fails all attempts, it raises the error with a final message.\n",
        "\n",
        "---\n",
        "\n",
        "### üßØ Handles These Errors Like a Champ:\n",
        "\n",
        "| Error Type            | Why It Might Happen                                        |\n",
        "|------------------------|------------------------------------------------------------|\n",
        "| `APIError`             | General server-side error from OpenAI                     |\n",
        "| `APIConnectionError`   | Network or connectivity issue                             |\n",
        "| `RateLimitError`       | You‚Äôre sending too many requests too fast ‚Äî slow down! üê¢ |\n",
        "| `Exception`            | Anything else unexpected gets caught here                |\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Why This Rocks:**\n",
        "- Prevents your app from crashing on temporary issues\n",
        "- Helps maintain smooth user experience\n",
        "- Keeps retry behavior clean and configurable\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb3DgJ1yQ3tu"
      },
      "outputs": [],
      "source": [
        "def query_llm_with_retry(messages, model=\"gpt-4o\", max_retries=3, delay=1):\n",
        "    \"\"\"\n",
        "    Sends a query to the LLM with retry logic for API errors.\n",
        "\n",
        "    Args:\n",
        "        messages (list): The list of messages for the chat completion.\n",
        "        model (str): The LLM model to use.\n",
        "        max_retries (int): Maximum number of retries.\n",
        "        delay (int): Delay in seconds between retries.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the LLM's response.\n",
        "\n",
        "    Raises:\n",
        "        Exception: If the LLM call fails after all retries.\n",
        "    \"\"\"\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=messages\n",
        "            )\n",
        "            return completion.choices[0].message.content\n",
        "        except (APIError, APIConnectionError, RateLimitError) as e:\n",
        "            print(f\"API error during LLM call (Attempt {i+1}/{max_retries}): {e}\")\n",
        "            if i < max_retries - 1:\n",
        "                time.sleep(delay * (2 ** i))\n",
        "            else:\n",
        "                print(\"Max retries reached. LLM call failed.\")\n",
        "                raise\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during LLM call (Attempt {i+1}/{max_retries}): {e}\")\n",
        "            if i < max_retries - 1:\n",
        "                time.sleep(delay * (2 ** i))\n",
        "            else:\n",
        "                print(\"Max retries reached. LLM call failed.\")\n",
        "                raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcHdYICpXZ5d"
      },
      "source": [
        "# üìÑ Cell 9 - Extracting Legal Clauses Using GPT and Chunked Analysis\n",
        "\n",
        "Welcome to Cell 9 ‚Äî the **brainiest cell yet**! üß†  \n",
        "This function takes your documents and intelligently pulls out legal clauses related to key terms like `\"Service Warranty\"` or `\"Payment Terms\"` ‚Äî and it does so chunk by chunk using a GPT model.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Function: `extract_key_terms(docs, key_terms)`\n",
        "\n",
        "This function:\n",
        "- Splits documents into small pieces using `chunk_document()`\n",
        "- Sends each chunk to the GPT model with a **well-crafted prompt**\n",
        "- Asks the model to tell us whether a specific **key term** is mentioned\n",
        "- If yes, asks it to provide a **brief summary** of the clause\n",
        "\n",
        "| Parameter     | What It Means                                                                 |\n",
        "|---------------|--------------------------------------------------------------------------------|\n",
        "| `docs`        | A list of LangChain `Document` objects (usually chunks from previous steps)   |\n",
        "| `key_terms`   | A list of key phrases or legal terms to search for in the document            |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† How It Works:\n",
        "\n",
        "1. üîÅ **Loops over each term** in your `key_terms` list.\n",
        "2. ü™ì **Splits the document** into smaller, overlapping chunks (to respect token limits and retain context).\n",
        "3. üì¨ **For each chunk**, sends a system+user message to GPT asking:\n",
        "    - Is the term present?\n",
        "    - If yes, summarize it in **2 sentences** max.\n",
        "4. ‚úÖ If GPT replies `\"Relevant: Yes\"`, it:\n",
        "    - Extracts the summary using regex\n",
        "    - Tracks the chunk number and page number\n",
        "5. ‚ùå If GPT replies `\"Relevant: No\"`, it moves on to the next chunk.\n",
        "6. üìä Final result is a **dictionary per key term**, with:\n",
        "    - A combined summary of all matches\n",
        "    - Page numbers where it was found\n",
        "\n",
        "---\n",
        "\n",
        "## üõ°Ô∏è Resilience Features:\n",
        "\n",
        "| Feature               | Why It Helps                                                            |\n",
        "|------------------------|------------------------------------------------------------------------|\n",
        "| `query_llm_with_retry` | Handles OpenAI hiccups with smart retry logic ‚Äî no crashing! üîÅ          |\n",
        "| Regex parsing          | Extracts structured output from GPT reliably, with fallbacks. üïµÔ∏è‚Äç‚ôÇÔ∏è      |\n",
        "| Page metadata tracking | Associates summaries with original page numbers for better context üìç |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Output Example (per term):\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"Service Warranty\": {\n",
        "    \"answer\": \"Found relevant information. Summary: The warranty covers service defects within 12 months.\",\n",
        "    \"page_number\": \"1, 3\"\n",
        "  },\n",
        "  \"Termination for Cause\": {\n",
        "    \"answer\": \"Not found.\",\n",
        "    \"page_number\": \"Not applicable\"\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY8ThKBeQ9Av"
      },
      "outputs": [],
      "source": [
        "def extract_key_terms(docs, key_terms):\n",
        "    \"\"\"\n",
        "    Extracts clauses related to key terms from document chunks using LLM.\n",
        "    Uses chunking to handle token limits and attempts to correlate results with page numbers.\n",
        "\n",
        "    Args:\n",
        "        docs (list): A list of Langchain Document objects (can be chunks).\n",
        "        key_terms (list): List of key terms to extract.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with each key term and its associated extracted answer and page number.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    chunks = chunk_document(docs)\n",
        "\n",
        "    for term in key_terms:\n",
        "        term_results = []\n",
        "        found_in_chunks = False\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            prompt = (\n",
        "                f\"You are a legal document analysis assistant.\\n\"\n",
        "                f\"Your task is to determine if the following document snippet contains information pertaining to the term: '{term}'.\\n\"\n",
        "                f\"If it does, extract the relevant clause(s) and provide a very brief summary (no more than 2 sentences, focusing only on the key obligation or restriction).\\n\"\n",
        "                f\"If relevant information is found, respond in the following structured format:\\n\\n\"\n",
        "                f\"Relevant: Yes\\n\"\n",
        "                f\"Summary: <A very brief summary of the relevant clause(s)>\\n\\n\"\n",
        "                f\"If the term is not found in this snippet, respond exactly with:\\n\"\n",
        "                f\"Relevant: No\\n\\n\"\n",
        "                f\"Document Snippet (Chunk {i+1}):\\n{chunk.page_content}\"\n",
        "            )\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal contract analysis assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                answer = query_llm_with_retry(messages, model=\"gpt-4o-mini\")\n",
        "                print(f\"****************LLM Answer for '{term}' in Chunk {i+1}*******************\")\n",
        "                print(answer)\n",
        "\n",
        "                if \"Relevant: Yes\" in answer:\n",
        "                    summary_match = re.search(r\"Summary:\\s*(.*)\", answer, re.DOTALL)\n",
        "                    summary = summary_match.group(1).strip() if summary_match else \"Summary extraction failed.\"\n",
        "                    page_number = chunk.metadata.get(\"page\", \"N/A\")\n",
        "                    term_results.append({\"clause_summary\": summary, \"page_number\": page_number, \"chunk_index\": i})\n",
        "                    found_in_chunks = True\n",
        "                elif \"Relevant: No\" in answer:\n",
        "                    pass\n",
        "                else:\n",
        "                    print(f\"Warning: Unexpected LLM response format for term '{term}' in chunk {i+1}. Response:\\n{answer}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred while processing chunk {i+1} for term '{term}': {e}\")\n",
        "\n",
        "        if not found_in_chunks or not term_results:\n",
        "            results[term] = {\"answer\": \"Not found.\", \"page_number\": \"Not applicable\"}\n",
        "        else:\n",
        "            combined_summary = \" \".join([res[\"clause_summary\"] for res in term_results])\n",
        "            page_numbers = sorted(list(set([res[\"page_number\"] for res in term_results])))\n",
        "            page_info = \", \".join(map(str, page_numbers)) if page_numbers and page_numbers != [\"N/A\"] else \"N/A\"\n",
        "\n",
        "            results[term] = {\n",
        "                \"answer\": f\"Found relevant information. Summary: {combined_summary}\",\n",
        "                \"page_number\": page_info\n",
        "            }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR5mV23PXd0W"
      },
      "source": [
        "# üßæ Cell 10 - Extracting Verbatim Ground Truth from Legal Documents\n",
        "\n",
        "Boom! You‚Äôve arrived at Cell 10 ‚Äî the ultimate extractor. üß†‚ú®  \n",
        "While Cell 9 gave you brief summaries of legal clauses, this one is laser-focused on grabbing the **exact original wording** (aka \"ground truth\") for each key term. No paraphrasing ‚Äî we want it raw and accurate. üíØ\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Function: `extract_ground_truth(docs, key_terms)`\n",
        "\n",
        "This function extracts **verbatim clauses** related to specific legal terms using GPT. It processes each chunk of the document, asks for a structured JSON response, and builds a dictionary of exact matches.\n",
        "\n",
        "| Parameter     | Description                                                                  |\n",
        "|---------------|------------------------------------------------------------------------------|\n",
        "| `docs`        | A list of LangChain `Document` objects (pre-processed chunks from earlier)  |\n",
        "| `key_terms`   | List of legal/contract terms you're trying to extract from the document      |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† How It Works:\n",
        "\n",
        "1. üìñ **Splits your documents** into smaller chunks with `chunk_document()`\n",
        "2. üîÅ **Loops over each `term`** you want to search\n",
        "3. üì® Sends each chunk to the LLM with a prompt asking for:\n",
        "   - **Exact match text** if the term is found\n",
        "   - Page number metadata for traceability\n",
        "   - A strict JSON format for easier parsing\n",
        "4. ‚úÖ If GPT responds with a valid JSON and relevant clause, it's added to your results\n",
        "5. üßπ If no result is found in any chunk, marks the term as `\"Not found\"`\n",
        "\n",
        "---\n",
        "\n",
        "### üîê Prompt Magic:\n",
        "\n",
        "The prompt tells GPT to return its response like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"term\": \"Confidentiality Obligations\",\n",
        "  \"ground_truth_answer\": \"<verbatim text>\",\n",
        "  \"snippet_page\": \"3\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xnazb51RAzp"
      },
      "outputs": [],
      "source": [
        "def extract_ground_truth(docs, key_terms):\n",
        "    \"\"\"\n",
        "    Extracts ground truth answers for each key term from document chunks using LLM.\n",
        "    Parses JSON output and attempts to correlate results with page numbers.\n",
        "\n",
        "    Args:\n",
        "        docs (list): A list of Langchain Document objects (can be chunks).\n",
        "        key_terms (list): List of key terms to extract.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with each key term and its associated extracted ground truth and page number.\n",
        "    \"\"\"\n",
        "    ground_truth = {}\n",
        "    chunks = chunk_document(docs)\n",
        "\n",
        "    for term in key_terms:\n",
        "        ground_truth_answer = \"Not found.\"\n",
        "        page_number = \"Not applicable\"\n",
        "        found_verbatim = False\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            prompt = (\n",
        "                f\"You are a legal document analysis assistant. \"\n",
        "                f\"Your task is to extract the *ground truth* from the following document snippet for the key term: '{term}'. \"\n",
        "                f\"The ground truth is the exact text (verbatim) from the snippet that directly addresses or defines the key term. \"\n",
        "                f\"Return your response in the following JSON format:\\n\"\n",
        "                f'{{\\n  \"term\": \"{term}\",\\n  \"ground_truth_answer\": \"<verbatim text or Not found>\",\\n  \"snippet_page\": \"<page number of snippet or N/A>\"\\n}}\\n\\n'\n",
        "                f\"If the term is not mentioned or no relevant section exists in this snippet, the JSON should be:\\n\"\n",
        "                f'{{\\n  \"term\": \"{term}\",\\n  \"ground_truth_answer\": \"Not found\",\\n  \"snippet_page\": \"N/A\"\\n}}\\n\\n'\n",
        "                f\"Document Snippet (Chunk {i+1}, Page: {chunk.metadata.get('page', 'N/A')}):\\n{chunk.page_content}\"\n",
        "            )\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal contract analysis assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                answer_str = query_llm_with_retry(messages, model=\"gpt-4o-mini\")\n",
        "                print(f\"****************GROUNDTRUTHANSWER for '{term}' in Chunk {i+1}*******************\")\n",
        "                print(answer_str)\n",
        "\n",
        "                try:\n",
        "                    answer_json = json.loads(answer_str)\n",
        "                    llm_ground_truth = answer_json.get(\"ground_truth_answer\", \"Parsing failed.\")\n",
        "                    llm_snippet_page = answer_json.get(\"snippet_page\", \"N/A\")\n",
        "\n",
        "                    if llm_ground_truth != \"Not found\" and llm_ground_truth != \"Parsing failed.\":\n",
        "                        ground_truth_answer = llm_ground_truth\n",
        "                        page_number = chunk.metadata.get(\"page\", \"N/A\")\n",
        "                        found_verbatim = True\n",
        "                        break\n",
        "\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Warning: LLM response is not valid JSON for term '{term}' in chunk {i+1}. Response:\\n{answer_str}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred while processing chunk {i+1} for term '{term}': {e}\")\n",
        "\n",
        "        ground_truth[term] = {\n",
        "            \"ground_truth_answer\": ground_truth_answer,\n",
        "            \"page_number\": page_number\n",
        "        }\n",
        "\n",
        "    return ground_truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJZYSGIcXl74"
      },
      "source": [
        "### ‚úÇÔ∏è truncate_text(text, max_length)\n",
        "\n",
        "**Purpose:**  \n",
        "Truncates a given text to a specified character limit. Adds `\"...\"` at the end if truncation occurs.\n",
        "\n",
        "**Args:**\n",
        "- `text` (`str`): The input string to be truncated.\n",
        "- `max_length` (`int`): The maximum number of characters allowed.\n",
        "\n",
        "**Returns:**\n",
        "- A truncated string ending in `\"...\"` if it exceeds `max_length`, else the original text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nhhiyqfRFc9"
      },
      "outputs": [],
      "source": [
        "def truncate_text(text, max_length):\n",
        "    \"\"\"Truncate text to specified length for display purposes\"\"\"\n",
        "    return text[:max_length] + \"...\" if len(text) > max_length else text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVDdghA-X0DR"
      },
      "source": [
        "### `evaluate_entry(entry, results_df)`\n",
        "\n",
        "Evaluates a single key term entry from a legal document against four key criteria: **groundedness**, **coherence**, **relevance**, and **fluency** using Azure/OpenAI evaluation models. The function appends the evaluated scores and metadata into a running results DataFrame.\n",
        "\n",
        "#### **Parameters**\n",
        "- `entry` (`dict`):  \n",
        "  A dictionary containing:\n",
        "  - `'Key Term Name'` (`str`): The name of the legal clause (e.g., \"Payment Terms\").\n",
        "  - `'query'` (`str`): The search query or clause being evaluated.\n",
        "  - `'context'` (`str`): The document snippet or paragraph from which the ground truth is extracted.\n",
        "  - `'ground_truth'` (`str`): The verbatim text from the document associated with the key term.\n",
        "  - `'llm_response'` (`str`): The output provided by the LLM (optional for evaluation comparison).\n",
        "\n",
        "- `results_df` (`pd.DataFrame`):  \n",
        "  The running dataframe to which the evaluated result will be appended.\n",
        "\n",
        "#### **Returns**\n",
        "- `pd.DataFrame`:  \n",
        "  A new DataFrame with one additional row containing:\n",
        "  - Key Term Name\n",
        "  - Context\n",
        "  - Query\n",
        "  - Ground Truth\n",
        "  - LLM Response\n",
        "  - Groundedness Score\n",
        "  - Coherence Score\n",
        "  - Relevance Score\n",
        "  - Fluency Score\n",
        "\n",
        "#### **Dependencies**\n",
        "- `truncate_text(text, max_length)` - helper function to shorten long queries for console output.\n",
        "- `groundedness_eval`, `coherence_eval`, `relevance_eval`, `fluency_eval` - initialized Azure or OpenAI evaluation objects.\n",
        "\n",
        "#### **Example**\n",
        "```python\n",
        "entry = {\n",
        "    \"Key Term Name\": \"Confidentiality Obligations\",\n",
        "    \"query\": \"What are the confidentiality obligations?\",\n",
        "    \"context\": \"The parties shall keep all confidential information private...\",\n",
        "    \"ground_truth\": \"The parties agree not to disclose any proprietary or confidential info...\",\n",
        "    \"llm_response\": \"Parties must not share confidential information during or after the agreement.\"\n",
        "}\n",
        "\n",
        "results_df = evaluate_entry(entry, results_df)\n",
        "```\n",
        "\n",
        "This function helps build a structured and automated evaluation pipeline for contract analysis or legal clause extraction systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDh-BJDyRQPt"
      },
      "outputs": [],
      "source": [
        "def evaluate_entry(entry, results_df):\n",
        "    \"\"\"Evaluate a single entry using all evaluators and add to results dataframe\"\"\"\n",
        "    print(f\"Evaluating query: {truncate_text(entry['query'], 40)}\")\n",
        "\n",
        "    # Format inputs for each evaluator\n",
        "    groundedness_input = {\n",
        "        \"query\": entry[\"query\"],\n",
        "        \"context\": entry[\"context\"],\n",
        "        \"response\": entry[\"ground_truth\"]\n",
        "    }\n",
        "\n",
        "    coherence_input = {\n",
        "        \"query\": entry[\"query\"],\n",
        "        \"response\": entry[\"ground_truth\"]\n",
        "    }\n",
        "\n",
        "    relevance_input = {\n",
        "        \"query\": entry[\"query\"],\n",
        "        \"response\": entry[\"ground_truth\"]\n",
        "    }\n",
        "\n",
        "    fluency_input = {\n",
        "        \"response\": entry[\"ground_truth\"]\n",
        "    }\n",
        "\n",
        "    # Get all scores\n",
        "    groundedness_score = groundedness_eval(**groundedness_input)\n",
        "    coherence_score = coherence_eval(**coherence_input)\n",
        "    relevance_score = relevance_eval(**relevance_input)\n",
        "    fluency_score = fluency_eval(**fluency_input)\n",
        "\n",
        "    # Create new row for the dataframe\n",
        "    new_row = {\n",
        "        'Key Term Name': entry['Key Term Name'], # Ensure Key Term Name is passed through\n",
        "        'Context': entry['context'],\n",
        "        'Query': entry['query'],\n",
        "        'Ground Truth': entry['ground_truth'],\n",
        "        'LLM Response': entry['llm_response'],\n",
        "        'Groundedness Score': groundedness_score['groundedness'],\n",
        "        'Coherence Score': coherence_score['coherence'],\n",
        "        'Relevance Score': relevance_score['relevance'],\n",
        "        'Fluency Score': fluency_score['fluency']\n",
        "    }\n",
        "\n",
        "    # Use concat with a pre-defined DataFrame\n",
        "    new_row_df = pd.DataFrame([new_row])\n",
        "    return pd.concat([results_df, new_row_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qNfWx_3YU3E"
      },
      "source": [
        "### `create_evaluation_dataset(llm_results, ground_truth_results, document_text)`\n",
        "\n",
        "Generates a structured evaluation dataset combining LLM-generated answers and ground truth answers for a fixed set of legal key terms.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Parameters**\n",
        "- `llm_results` (`dict`):  \n",
        "  Dictionary where keys are legal key terms and values are dictionaries with at least an `\"answer\"` field containing the LLM's output.\n",
        "\n",
        "- `ground_truth_results` (`dict`):  \n",
        "  Dictionary where keys are the same legal key terms and values contain a `\"ground_truth_answer\"` (text extracted from the original document).\n",
        "\n",
        "- `document_text` (`str`):  \n",
        "  The full legal document text. Only the first 2000 characters are used as \"context\" to limit evaluation input size.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Returns**\n",
        "- `list[dict]`:  \n",
        "  A list of evaluation entries where each item is a dictionary with the following structure:\n",
        "  ```python\n",
        "  {\n",
        "      'Key Term Name': \"<Term Name>\",\n",
        "      'query': \"Extract information about <Term> from the legal document.\",\n",
        "      'context': \"<First 2000 chars of document_text>\",\n",
        "      'ground_truth': \"<Extracted answer from document>\",\n",
        "      'llm_response': \"<LLM's answer>\"\n",
        "  }\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "#### **Dependencies**\n",
        "- `KEY_TERMS` (global list):  \n",
        "  A list of predefined legal clauses like:\n",
        "  ```python\n",
        "  [\n",
        "      \"Service Warranty\",\n",
        "      \"Limitation of Liability\",\n",
        "      \"Governing Law\",\n",
        "      \"Termination for Cause\",\n",
        "      \"Payment Terms\",\n",
        "      \"Confidentiality Obligations\"\n",
        "  ]\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "#### **Example**\n",
        "```python\n",
        "dataset = create_evaluation_dataset(llm_results, ground_truth_results, full_doc_text)\n",
        "for row in dataset:\n",
        "    print(row['Key Term Name'], row['query'], row['llm_response'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "This function is essential to bridge model outputs and ground truth values before passing them to automated evaluators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whEQUAWCYLe-"
      },
      "outputs": [],
      "source": [
        "def create_evaluation_dataset(llm_results, ground_truth_results, document_text):\n",
        "    \"\"\"Create evaluation dataset from LLM and ground truth results\"\"\"\n",
        "    evaluation_data = []\n",
        "\n",
        "    for term in KEY_TERMS:\n",
        "        if term in llm_results and term in ground_truth_results:\n",
        "            llm_answer = llm_results[term].get('answer', 'Not found.')\n",
        "            ground_truth_answer = ground_truth_results[term].get('ground_truth_answer', 'Not found.')\n",
        "\n",
        "            # Create evaluation entry\n",
        "            entry = {\n",
        "                'Key Term Name': term,  # Add the key term name here\n",
        "                'query': f\"Extract information about {term} from the legal document.\",\n",
        "                'context': document_text[:2000],  # Use first 2000 chars as context\n",
        "                'ground_truth': ground_truth_answer,\n",
        "                'llm_response': llm_answer\n",
        "            }\n",
        "            evaluation_data.append(entry)\n",
        "\n",
        "    return evaluation_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeA1dnRLYNoP"
      },
      "source": [
        "### `evaluate_responses(llm_results, ground_truth_results, document_text)`\n",
        "\n",
        "Evaluates LLM-generated responses against extracted ground truth answers using a suite of Azure/OpenAI evaluation models. It computes metrics such as groundedness, coherence, relevance, and fluency, and returns a structured DataFrame of results.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Parameters**\n",
        "- `llm_results` (`list[dict]`):  \n",
        "  A list of LLM responses. Each dictionary typically includes:\n",
        "  - `'term'`: The key term being queried.\n",
        "  - `'llm_response'`: The LLM's output for that term.\n",
        "\n",
        "- `ground_truth_results` (`dict`):  \n",
        "  Dictionary mapping each key term to:\n",
        "  - `'ground_truth_answer'`: Verbatim extracted text from the document.\n",
        "  - `'page_number'`: (Optional) Page number from which the text was extracted.\n",
        "\n",
        "- `document_text` (`str`):  \n",
        "  The full text of the original legal document (or a combined snippet text), used as context for evaluators.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Returns**\n",
        "- `pd.DataFrame`:  \n",
        "  A DataFrame with one row per evaluated key term, including:\n",
        "  - `'Key Term Name'`\n",
        "  - `'Context'`\n",
        "  - `'Query'`\n",
        "  - `'Ground Truth'`\n",
        "  - `'LLM Response'`\n",
        "  - `'Groundedness Score'`\n",
        "  - `'Coherence Score'`\n",
        "  - `'Relevance Score'`\n",
        "  - `'Fluency Score'`\n",
        "\n",
        "If an error occurs during evaluation, returns a DataFrame with an `\"Error\"` column describing the issue.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Dependencies**\n",
        "- `create_evaluation_dataset(...)`:  \n",
        "  Assembles entries combining LLM outputs and ground truths for consistent evaluation.\n",
        "- `evaluate_entry(...)`:  \n",
        "  Evaluates a single entry and appends it to the result DataFrame.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Example**\n",
        "```python\n",
        "results_df = evaluate_responses(llm_results, ground_truth_results, full_document_text)\n",
        "\n",
        "# Show results\n",
        "print(results_df.head())\n",
        "```\n",
        "\n",
        "This function is intended for automated scoring and benchmarking of legal clause extraction or compliance generation pipelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao7teojcRYTL"
      },
      "outputs": [],
      "source": [
        "def evaluate_responses(llm_results, ground_truth_results, document_text):\n",
        "    \"\"\"Evaluate LLM responses against ground truth using Azure evaluators\"\"\"\n",
        "    try:\n",
        "        # Create evaluation dataset\n",
        "        evaluation_data = create_evaluation_dataset(llm_results, ground_truth_results, document_text)\n",
        "\n",
        "        # Initialize results dataframe with all necessary columns\n",
        "        results_df = pd.DataFrame(columns=[\n",
        "            'Key Term Name', 'Context', 'Query', 'Ground Truth', 'LLM Response', 'Groundedness Score',\n",
        "            'Coherence Score', 'Relevance Score', 'Fluency Score'\n",
        "        ])\n",
        "\n",
        "        # Evaluate each entry\n",
        "        for entry in evaluation_data:\n",
        "            results_df = evaluate_entry(entry, results_df)\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during evaluation: {e}\")\n",
        "        # Return an empty DataFrame or a DataFrame with error info for Gradio\n",
        "        return pd.DataFrame({'Error': [f\"Evaluation failed: {e}\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsAcgdYYYenD"
      },
      "source": [
        "### `process_document(contract_file, ground_truth_file=None)`\n",
        "\n",
        "| **Aspect**         | **Details**                                                                                                                                           |\n",
        "|--------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Description**    | Processes a legal contract and optional ground truth file to extract key legal terms and evaluate them using Azure AI Evaluators.                     |\n",
        "| **Parameters**     | `contract_file` (`str`): Path to the contract file. <br> `ground_truth_file` (`str`, optional): Path to the ground truth file. Defaults to `None`.    |\n",
        "| **Returns**        | A tuple of 3 elements:<br>‚Ä¢ `llm_results` (`dict`): Key term answers from the LLM<br>‚Ä¢ `ground_truth_results` (`dict` or `None`)<br>‚Ä¢ `evaluation_results` (`pd.DataFrame` or `None`) |\n",
        "| **Steps**          | 1. Extract text from contract file<br>2. Run LLM extraction for `KEY_TERMS`<br>3. If ground truth is provided:<br>&nbsp;&nbsp;&nbsp;a. Extract ground truth answers<br>&nbsp;&nbsp;&nbsp;b. Run evaluation using Azure evaluators |\n",
        "| **Evaluation**     | If ground truth is provided, each term's response is evaluated on:<br>‚Ä¢ Groundedness<br>‚Ä¢ Coherence<br>‚Ä¢ Relevance<br>‚Ä¢ Fluency                      |\n",
        "| **Error Handling** | If any error occurs:<br>‚Ä¢ Logs the error<br>‚Ä¢ Returns error dictionaries for `llm_results` and `ground_truth_results`<br>‚Ä¢ Returns an error DataFrame |\n",
        "| **Example**        | ```python<br>llm_out, gt_out, eval_df = process_document(\"contract.pdf\", \"ground_truth.pdf\")<br>print(eval_df.head())<br>```                         |\n",
        "| **Depends On**     | ‚Ä¢ `extract_text_from_file()`<br>‚Ä¢ `extract_key_terms()`<br>‚Ä¢ `extract_ground_truth()`<br>‚Ä¢ `evaluate_responses()`<br>‚Ä¢ Global: `KEY_TERMS`            |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wRH1UYcRbzv"
      },
      "outputs": [],
      "source": [
        "def process_document(contract_file, ground_truth_file=None):\n",
        "    \"\"\"\n",
        "    Processes the uploaded contract and optional ground truth document.\n",
        "\n",
        "    Args:\n",
        "        contract_file (str): Path to the contract document.\n",
        "        ground_truth_file (str, optional): Path to the ground truth document. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the LLM extraction results, ground truth results, and evaluation results.\n",
        "    \"\"\"\n",
        "    llm_results = {}\n",
        "    ground_truth_results = None\n",
        "    evaluation_results = None\n",
        "    document_text = \"\"\n",
        "\n",
        "    try:\n",
        "        # Step 1: Extract text and documents from the contract file\n",
        "        print(f\"Processing contract file: {contract_file}\")\n",
        "        contract_text, contract_docs = extract_text_from_file(contract_file)\n",
        "        document_text = contract_text\n",
        "\n",
        "        # Step 2: Extract key terms using the LLM on document chunks\n",
        "        print(\"Extracting key terms using LLM...\")\n",
        "        llm_results = extract_key_terms(contract_docs, KEY_TERMS)\n",
        "        print(\"Key term extraction complete.\")\n",
        "\n",
        "        # Step 3: Extract ground truth if provided\n",
        "        if ground_truth_file:\n",
        "            print(f\"\\nProcessing ground truth file: {ground_truth_file}\")\n",
        "            ground_truth_text, ground_truth_docs = extract_text_from_file(ground_truth_file)\n",
        "\n",
        "            print(\"Extracting ground truth using LLM...\")\n",
        "            ground_truth_results = extract_ground_truth(ground_truth_docs, KEY_TERMS)\n",
        "            print(\"Ground truth extraction complete.\")\n",
        "\n",
        "            # Step 4: Evaluate LLM responses against ground truth\n",
        "            print(\"Starting evaluation...\")\n",
        "            evaluation_results = evaluate_responses(llm_results, ground_truth_results, document_text)\n",
        "            print(\"Evaluation complete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during document processing: {e}\")\n",
        "        llm_results = {\"Error\": f\"Processing failed: {e}\"}\n",
        "        ground_truth_results = {\"Error\": f\"Processing failed: {e}\"}\n",
        "        # Ensure evaluation_results is a DataFrame even on error for Gradio\n",
        "        evaluation_results = pd.DataFrame({'Error': [f\"Processing failed: {e}\"]})\n",
        "\n",
        "    return llm_results, ground_truth_results, evaluation_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqsQJg_hYzBw"
      },
      "source": [
        "### `display_results(contract_file, ground_truth_file=None)`\n",
        "\n",
        "| **Aspect**         | **Details**                                                                                                                                                                |\n",
        "|--------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Description**    | Processes input files using `process_document()` and formats outputs for display in Gradio: LLM results, ground truth, and evaluation.                                      |\n",
        "| **Parameters**     | `contract_file` (`str` or `file`): Contract document to analyze.<br>`ground_truth_file` (`str` or `file`, optional): Ground truth reference file. Defaults to `None`.      |\n",
        "| **Returns**        | Tuple of: <br>‚Ä¢ `llm_output` (`str`): Markdown-formatted LLM results<br>‚Ä¢ `ground_truth_output` (`str`): Markdown-formatted ground truth<br>‚Ä¢ `evaluation_df_for_gradio` (`pd.DataFrame`) |\n",
        "| **LLM Output**     | Displayed with term-wise markdown formatting. Shows extracted answers for each key term.                                                                                     |\n",
        "| **Ground Truth**   | If provided, shows extracted ground truth for each key term. Otherwise, outputs a message that no file was provided.                                                       |\n",
        "| **Evaluation**     | If evaluation is successful, returns a filtered and formatted `DataFrame` with the following columns:<br>‚Ä¢ `Key Term Name`, `Query`, `Ground Truth`, `LLM Response`, `Groundedness Score`, `Coherence Score`, `Relevance Score`, `Fluency Score` |\n",
        "| **Error Handling** | If evaluation fails or input is missing, returns a default empty `DataFrame` or an error DataFrame for Gradio compatibility.                                               |\n",
        "\n",
        "---\n",
        "\n",
        "### `create_interface()`\n",
        "\n",
        "| **Aspect**         | **Details**                                                                                                                                                     |\n",
        "|--------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Description**    | Builds the Gradio user interface for uploading legal contracts and optional ground truth files, analyzing them, and displaying the results.                      |\n",
        "| **Returns**        | `interface` (`gr.Blocks`): A configured Gradio Blocks interface ready to launch or integrate into a `launch()` call.                                              |\n",
        "| **Layout**         | - Title and instructions using `Markdown`<br>- Two file inputs (`contract_file`, `ground_truth_file`)<br>- `Analyze` button<br>- Output: 2 `Markdown` and 1 `DataFrame` |\n",
        "| **Output Sections**| - LLM Extraction Results (Markdown)<br> - Ground Truth Results (Markdown)<br> - Evaluation Results (Gradio `DataFrame`)                                           |\n",
        "| **Interaction**    | On click of the Analyze button, the `display_results()` function is called, and its outputs are fed into the respective display sections.                        |\n",
        "| **File Support**   | Accepts `.pdf`, `.docx`, `.doc`, `.txt`, `.csv` for both contract and ground truth documents.                                                                    |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "collapsed": true,
        "id": "izQZk13p-uU_",
        "outputId": "fa8d5255-8831-407e-d0a2-e508e513a4b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://1ca899687ed3a39e6e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://1ca899687ed3a39e6e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://1ca899687ed3a39e6e.gradio.live\n"
          ]
        }
      ],
      "source": [
        "def display_results(contract_file, ground_truth_file=None):\n",
        "    \"\"\"\n",
        "    Display results in a formatted way for Gradio interface\n",
        "    \"\"\"\n",
        "    llm_results, ground_truth_results, evaluation_results = process_document(contract_file, ground_truth_file)\n",
        "\n",
        "    # Format LLM Results\n",
        "    llm_output = \"## LLM Extraction Results\\n\\n\"\n",
        "    for term, result in llm_results.items():\n",
        "        if isinstance(result, dict) and 'answer' in result:\n",
        "            llm_output += f\"**{term}:**\\n\"\n",
        "            llm_output += f\"- Answer: {result['answer']}\\n\"\n",
        "        else:\n",
        "            llm_output += f\"**{term}:** {result}\\n\\n\"\n",
        "\n",
        "    # Format Ground Truth Results\n",
        "    ground_truth_output = \"## Ground Truth Results\\n\\n\"\n",
        "    if ground_truth_results:\n",
        "        for term, result in ground_truth_results.items():\n",
        "            if isinstance(result, dict) and 'ground_truth_answer' in result:\n",
        "                ground_truth_output += f\"**{term}:**\\n\"\n",
        "                ground_truth_output += f\"- Answer: {result['ground_truth_answer']}\\n\"\n",
        "            else:\n",
        "                ground_truth_output += f\"**{term}:** {result}\\n\\n\"\n",
        "    else:\n",
        "        ground_truth_output = \"No ground truth file provided.\"\n",
        "\n",
        "    # Prepare Evaluation Results for Gradio DataFrame\n",
        "    if evaluation_results is not None and not evaluation_results.empty:\n",
        "        if 'Error' in evaluation_results.columns:\n",
        "            # If there's an error, just return a simple DataFrame with the error message\n",
        "            # Gradio DataFrame can display this, but it won't be in the desired evaluation format.\n",
        "            # You might want to handle this error display differently in the UI if needed.\n",
        "            # For now, it will show a table with one column 'Error' and the message.\n",
        "            evaluation_df_for_gradio = evaluation_results\n",
        "        else:\n",
        "            # Define the desired column order for the final table\n",
        "            desired_columns = [\n",
        "                'Key Term Name', 'Query', 'Ground Truth', 'LLM Response',\n",
        "                'Groundedness Score', 'Coherence Score', 'Relevance Score', 'Fluency Score'\n",
        "            ]\n",
        "\n",
        "            # Filter and reorder columns\n",
        "            # Drop 'Context' as it's not requested in the final output format.\n",
        "            display_df = evaluation_results.copy()\n",
        "            if 'Context' in display_df.columns:\n",
        "                display_df = display_df.drop(columns=['Context'])\n",
        "\n",
        "            final_columns = [col for col in desired_columns if col in display_df.columns]\n",
        "            evaluation_df_for_gradio = display_df[final_columns]\n",
        "    else:\n",
        "        # Return an empty DataFrame with the desired columns if no evaluation is performed\n",
        "        # This prevents Gradio from throwing an error about unexpected output type.\n",
        "        evaluation_df_for_gradio = pd.DataFrame(columns=[\n",
        "            'Key Term Name', 'Query', 'Ground Truth', 'LLM Response',\n",
        "            'Groundedness Score', 'Coherence Score', 'Relevance Score', 'Fluency Score'\n",
        "        ])\n",
        "        # You could also add a row indicating \"No evaluation performed\"\n",
        "        # evaluation_df_for_gradio.loc[0] = [\"N/A\"] * len(evaluation_df_for_gradio.columns)\n",
        "        # evaluation_df_for_gradio.loc[0, 'Key Term Name'] = \"No evaluation performed (ground truth file required).\"\n",
        "\n",
        "    return llm_output, ground_truth_output, evaluation_df_for_gradio\n",
        "\n",
        "# Gradio Interface\n",
        "def create_interface():\n",
        "    \"\"\"Create Gradio interface for the legal document analyzer\"\"\"\n",
        "\n",
        "    with gr.Blocks(title=\"Legal Document Analyzer with Azure Evaluation\") as interface:\n",
        "        gr.Markdown(\"# Legal Document Analyzer with Azure Evaluation\")\n",
        "        gr.Markdown(\"Upload a legal contract document and optionally a ground truth file to extract key terms and evaluate the results.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                contract_file = gr.File(\n",
        "                    label=\"Upload Contract Document\",\n",
        "                    file_types=[\".pdf\", \".docx\", \".doc\", \".txt\", \".csv\"]\n",
        "                )\n",
        "                ground_truth_file = gr.File(\n",
        "                    label=\"Upload Ground Truth Document (Optional)\",\n",
        "                    file_types=[\".pdf\", \".docx\", \".doc\", \".txt\", \".csv\"]\n",
        "                )\n",
        "                analyze_btn = gr.Button(\"Analyze Document\", variant=\"primary\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                llm_output = gr.Markdown(label=\"LLM Results\")\n",
        "            with gr.Column():\n",
        "                ground_truth_output = gr.Markdown(label=\"Ground Truth Results\")\n",
        "\n",
        "        with gr.Row():\n",
        "            # Changed from gr.HTML to gr.DataFrame\n",
        "            # REMOVE headers=\"keys\" - Gradio will infer from the DataFrame\n",
        "            evaluation_output = gr.DataFrame(label=\"Evaluation Results\", wrap=True) # wrap=True for better text wrapping in cells\n",
        "\n",
        "        analyze_btn.click(\n",
        "            fn=display_results,\n",
        "            inputs=[contract_file, ground_truth_file],\n",
        "            outputs=[llm_output, ground_truth_output, evaluation_output]\n",
        "        )\n",
        "\n",
        "    return interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoV_c3o7Y8Hy"
      },
      "source": [
        "### `if __name__ == \"__main__\":` Block Overview\n",
        "\n",
        "| **Aspect**         | **Details**                                                                                           |\n",
        "|--------------------|--------------------------------------------------------------------------------------------------------|\n",
        "| **Purpose**        | Entry point for running the app as a standalone script. Ensures the Gradio interface launches only when the script is executed directly. |\n",
        "| **Functionality**  | - Calls the function to create the Gradio UI interface.<br>- Launches the app with debug and public sharing enabled. |\n",
        "| **Parameters**     | `debug=True`: Enables debugging output in the console.<br>`share=True`: Generates a public Gradio link. |\n",
        "| **Usage**          | Run this Python script directly to open the interface locally and via a public URL (if online).       |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_bizVrFRkCa"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    interface = create_interface()\n",
        "    interface.launch(debug=True, share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
