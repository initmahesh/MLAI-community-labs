{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0516c151",
      "metadata": {
        "id": "0516c151"
      },
      "source": [
        "# LLM-as-a-Judge Simply Explained: A Complete Guide to Run LLM Evals\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sachin0034/MLAI-community-labs/blob/main/Class-Labs/Lab-14%28LLM-Judge-lab%29/Lab-2%28LLM-as-a-Judge%20with%20LLM%29/LLM_As_A_Judge.ipynb)",
        "\n",
        "\n",
        "\"LLM-as-a-judge\" is a technique where large language models — like GPT — are used to evaluate the quality of outputs generated by other AI models or systems. Instead of relying on human evaluators, which can be time-consuming and expensive, we use an LLM to act as the judge, scoring or ranking generated content based on factors like correctness, coherence, relevance, or even tone and style.\n",
        "\n",
        "This approach became popular because evaluating open-ended text (like summaries, chatbot replies, or creative writing) is inherently subjective. Traditional metrics like accuracy or BLEU scores often fall short since there’s no single 'right' answer. LLMs help fill that gap by providing nuanced judgments, often closer to how a human would interpret or assess the output.\n",
        "\n",
        "So in essence, LLM-as-a-judge is a scalable, cost-effective, and surprisingly reliable way to evaluate the quality of language model outputs — especially when human evaluation isn’t feasible at scale.\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before you get started, please make sure you have the following ready:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Sample Contract File for Testing\n",
        "\n",
        "To try out the contract analysis workflow, download the sample contract file provided below:\n",
        "\n",
        "- [Download Sample Contract (Google Drive)](https://drive.google.com/file/d/1E557kdNBZ5cDUvVDLNrEVRuKcRSYDG3Z/view?usp=sharing)\n",
        "\n",
        "### 2. OpenAI API Key\n",
        "\n",
        "You’ll need your own OpenAI API key to access the language models used for contract evaluation. If you don’t have one yet, follow this step-by-step guide to generate your API key:\n",
        "\n",
        "- [How to get your own OpenAI API key (Medium article)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61733e35",
      "metadata": {
        "id": "61733e35"
      },
      "source": [
        "## Step 1: Install the Dependencies\n",
        "Each dependency serves a specific purpose in the LLM Judge Lab:\n",
        "\n",
        "| Package        | Purpose / Use in Project                                                                                     |\n",
        "|----------------|--------------------------------------------------------------------------------------------------------------|\n",
        "| **gradio**     | Builds a web-based UI for interaction. Allows users to input text, upload files, and view model evaluations. |\n",
        "| **langchain**  | Manages the logic of LLM interactions — from document processing to chaining LLM calls.                      |\n",
        "| **openai**     | Connects the system to OpenAI’s models (e.g., GPT-4) for generating judgments or scores.                     |\n",
        "| **python-docx**| Parses and extracts content from `.docx` files for evaluation.                                               |\n",
        "| **PyPDF2**     | Extracts text from PDFs, enabling the model to assess uploaded PDF documents.                                |\n",
        "| **pandas**     | Structures and displays results in tables or dataframes for better analysis and comparison.                  |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "78af67c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78af67c4",
        "outputId": "ab04c589-3940-4d98-b5bb-2902d376a002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.96.1)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting PyPDF\n",
            "  Downloading pypdf-5.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.69)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.8.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.7/309.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, python-docx, PyPDF, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed PyPDF-5.8.0 dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-docx-1.2.0 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "! pip install gradio langchain openai python-docx PyPDF pandas langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2ca2b6a",
      "metadata": {
        "id": "c2ca2b6a"
      },
      "source": [
        "## 🚀 Step 2 : After Installing Dependencies: Let's Start Importing!\n",
        "\n",
        "Now that you've installed all the necessary libraries, it's time to import them into your Python script or Jupyter notebook.\n",
        "\n",
        "Start by importing **Gradio** to build the interactive web interface for your LLM-as-a-judge lab.\n",
        "\n",
        "Next, bring in **document loaders from LangChain** — specifically for handling PDF, DOCX, and plain text files. These will help you extract content from user-uploaded documents.\n",
        "\n",
        "Then, import the **OpenAI client**, which you'll use to connect to models like GPT-4 for analyzing and judging text.\n",
        "\n",
        "You’ll also want **Pandas** to organize and display results in table formats, especially when dealing with comparisons or scores.\n",
        "\n",
        "Finally, include Python’s built-in **os** and **tempfile** modules. These are useful for file path handling and safely working with temporary files during processing.\n",
        "\n",
        "Once these imports are in place, you're ready to move on to building the file processing and evaluation pipeline!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3da24358",
      "metadata": {
        "id": "3da24358"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
        "from langchain.schema import Document\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "import tempfile\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "252947bb",
      "metadata": {
        "id": "252947bb"
      },
      "source": [
        "## 🧠 Step 3: Define Key Terms and Evaluation Metrics\n",
        "\n",
        "Now, let’s define the **what** and the **how** of the evaluation:\n",
        "\n",
        "### 🔑 Key Terms\n",
        "\n",
        "These are the critical contract clauses we want the LLM to extract and analyze:\n",
        "\n",
        "- **Service Warranty**  \n",
        "- **Limitation of Liability**  \n",
        "- **Governing Law**  \n",
        "- **Termination for Cause**  \n",
        "- **Payment Terms**  \n",
        "- **Confidentiality Obligations**\n",
        "\n",
        "Each of these helps focus the LLM’s attention on high-priority legal elements.\n",
        "\n",
        "---\n",
        "\n",
        "### Helpful\n",
        "| Metric                                                        | What It Measures                                      |\n",
        "|---------------------------------------------------------------|-------------------------------------------------------|\n",
        "| Was the information extracted as per the question asked?      | Did the answer directly address the key term?         |\n",
        "| Was the information complete?                                 | Is all relevant information included?                 |\n",
        "| Was the information enough to make a conclusive decision?     | Is the answer sufficient for decision-making?         |\n",
        "| Were associated red flags covered in the extracted output?    | Are potential issues or risks mentioned?              |\n",
        "\n",
        "---\n",
        "\n",
        "### Honest\n",
        "| Metric                                                        | What It Measures                                      |\n",
        "|---------------------------------------------------------------|-------------------------------------------------------|\n",
        "| Was the information extracted from all relevant clauses?      | Are multiple relevant sections included if needed?    |\n",
        "| Was the page number of extracted information correct?         | Are page references accurate?                         |\n",
        "| Was the AI reasoning discussing the relevant clause?          | Is the explanation focused on the right part?         |\n",
        "| Does the information stay within document scope?              | Is the answer limited to the uploaded contract?       |\n",
        "\n",
        "---\n",
        "\n",
        "### Harmless\n",
        "| Metric                                                        | What It Measures                                      |\n",
        "|---------------------------------------------------------------|-------------------------------------------------------|\n",
        "| Were results free from misleading claims?                     | Are there any false or misleading statements?         |\n",
        "| Does the tool avoid generic/non-contract answers?             | Is the answer specific to the contract, not generic?  |\n",
        "| Did the AI avoid illegal or insensitive justifications?       | Are explanations appropriate and lawful?              |\n",
        "| Did the tool prevent false claims about people/entities?      | Are there any incorrect statements about parties?     |\n",
        "| Did the tool context hateful/profane content?                 | Is the output free from inappropriate language?       |\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e9726237",
      "metadata": {
        "id": "e9726237"
      },
      "outputs": [],
      "source": [
        "KEY_TERMS = [\n",
        "    \"Service Warranty\",\n",
        "    \"Limitation of Liability\",\n",
        "    \"Governing Law\",\n",
        "    \"Termination for Cause\",\n",
        "    \"Payment Terms\",\n",
        "    \"Confidentiality Obligations\"\n",
        "]\n",
        "\n",
        "EVALUATION_METRICS = [\n",
        "    \"Was the information extracted as per the question asked in the key term?\",\n",
        "    \"Was the information complete?\",\n",
        "    \"Was the information enough to make a conclusive decision?\",\n",
        "    \"Were associated red flags covered in the extracted output?\",\n",
        "    \"Was the information extracted from all relevant clauses?\",\n",
        "    \"Was the page number of extracted information correct?\",\n",
        "    \"Was the AI reasoning discussing the relevant clause?\",\n",
        "    \"Does the information stay within document scope?\",\n",
        "    \"Were results free from misleading claims?\",\n",
        "    \"Does the tool avoid generic/non-contract answers?\",\n",
        "    \"Did the AI avoid illegal or insensitive justifications?\",\n",
        "    \"Did the tool prevent false claims about people/entities?\",\n",
        "    \"Did the tool context hateful/profane content?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c53a6a43",
      "metadata": {
        "id": "c53a6a43"
      },
      "source": [
        "## 📂 Step 4: Extract Text from Documents\n",
        "\n",
        "To analyze uploaded files, we first extract their text using the `extract_text_from_file` function.\n",
        "\n",
        "It supports:\n",
        "\n",
        "- **PDF** (`.pdf`) via `PyPDFLoader`\n",
        "- **Word** (`.docx`, `.doc`) via `Docx2txtLoader`\n",
        "- **Text** (`.txt`) via `TextLoader`\n",
        "- **CSV** (`.csv`) via `pandas`\n",
        "\n",
        "The function returns:\n",
        "- `text`: Complete extracted content\n",
        "- `docs`: Structured data (useful for page references)\n",
        "\n",
        "✅ This ensures consistent input for the LLM across all major file types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7afea5dd",
      "metadata": {
        "id": "7afea5dd"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_file(file_path):\n",
        "    # Get the file extension and convert it to lowercase\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    # Handle PDF files\n",
        "    if ext == \".pdf\":\n",
        "        loader = PyPDFLoader(file_path)  # Use PyPDFLoader to read PDF\n",
        "        docs = loader.load()  # Load document into LangChain Document objects\n",
        "        text = \"\\n\".join([doc.page_content for doc in docs])  # Combine all page content\n",
        "\n",
        "    # Handle Word documents (.docx, .doc)\n",
        "    elif ext in [\".docx\", \".doc\"]:\n",
        "        loader = Docx2txtLoader(file_path)  # Use Docx2txtLoader for Word files\n",
        "        docs = loader.load()\n",
        "        text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Handle plain text files\n",
        "    elif ext in [\".txt\"]:\n",
        "        loader = TextLoader(file_path)  # Use TextLoader for .txt files\n",
        "        docs = loader.load()\n",
        "        text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Handle CSV files\n",
        "    elif ext == \".csv\":\n",
        "        df = pd.read_csv(file_path)  # Read CSV using pandas\n",
        "        text = df.to_string(index=False)  # Convert DataFrame to plain string\n",
        "        # Wrap in a dummy doc-like object to keep consistent structure\n",
        "        docs = [type('Doc', (object,), {'page_content': text})()]\n",
        "\n",
        "    # Unsupported file types\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type\")\n",
        "\n",
        "    # Return both raw text and structured docs for further processing\n",
        "    return text, docs  # docs may include page-level details\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63eb97e5",
      "metadata": {
        "id": "63eb97e5"
      },
      "source": [
        "## 🔐 Step 5: Set Up OpenAI Client\n",
        "\n",
        "To use GPT models, you must have your own OpenAI API key.\n",
        "\n",
        "**Important:**\n",
        "- You cannot proceed without a valid API key.\n",
        "- Keep your key secure and never share it or commit it to public repositories.\n",
        "\n",
        "> 🔎 **Note:** If you don’t have an API key, follow this guide to get one:  \n",
        "> [How to get your own OpenAI API key (Medium article)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "730cfdf1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "collapsed": true,
        "id": "730cfdf1",
        "outputId": "6235b6ee-1758-4c5b-877c-231f5c445893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Invalid OpenAI API key or connection error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: Insert Y************Here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "OpenAI API key check failed. Please provide a valid key.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-3850031415.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Minimal API call to check if the key is valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ OpenAI API key is valid.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/models.py\u001b[0m in \u001b[0;36mlist\u001b[0;34m(self, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m         return self._get_api_list(\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0;34m\"/models\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mget_api_list\u001b[0;34m(self, path, model, page, body, options, method)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFinalRequestOptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1305\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_api_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request_api_list\u001b[0;34m(self, model, page, options)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: Insert Y************Here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-3850031415.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"❌ Invalid OpenAI API key or connection error:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OpenAI API key check failed. Please provide a valid key.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: OpenAI API key check failed. Please provide a valid key."
          ]
        }
      ],
      "source": [
        "api_key = 'Insert Your API Key Here'\n",
        "\n",
        "try:\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    # Minimal API call to check if the key is valid\n",
        "    client.models.list()\n",
        "    print(\"✅ OpenAI API key is valid.\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Invalid OpenAI API key or connection error:\", e)\n",
        "    raise RuntimeError(\"OpenAI API key check failed. Please provide a valid key.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca46ca6",
      "metadata": {
        "id": "2ca46ca6"
      },
      "source": [
        "## 🧠 Step 6: Extract Key Terms from the Document\n",
        "\n",
        "The `extract_key_terms(text, key_terms)` function is used to pull out and summarize the most important legal clauses from your uploaded document.\n",
        "\n",
        "It takes two inputs:\n",
        "\n",
        "- `text`: The full contract content, extracted earlier using `extract_text_from_file()`\n",
        "- `key_terms`: A list of specific legal terms you defined earlier (e.g. \"Payment Terms\", \"Governing Law\")\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What It Does:\n",
        "\n",
        "Once the document is uploaded and text is extracted, this function:\n",
        "\n",
        "1. Loops through each key term from the list.\n",
        "2. For each term:\n",
        "   - It sends a carefully crafted prompt (along with the full text) to the OpenAI model (GPT-4).\n",
        "   - It asks the model to find the clause, summarize it in simple language, and return it in a structured **JSON format**.\n",
        "3. Then, it:\n",
        "   - Parses the JSON response from the model.\n",
        "   - Extracts the **summary** and **page number** (if mentioned).\n",
        "   - Stores the result in a dictionary.\n",
        "\n",
        "---\n",
        "\n",
        "### 📤 Output:\n",
        "\n",
        "Returns a dictionary that looks like this:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"Payment Terms\": {\n",
        "    \"Summary\": \"Payments must be made within 30 days of invoice.\",\n",
        "    \"page_number\": \"5\"\n",
        "  },\n",
        "  \"Confidentiality Obligations\": {\n",
        "    \"Summary\": \"Parties must keep all shared data confidential.\",\n",
        "    \"page_number\": \"7\"\n",
        "  },\n",
        "  ...\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "35324c05",
      "metadata": {
        "id": "35324c05"
      },
      "outputs": [],
      "source": [
        "def extract_key_terms(text, key_terms):\n",
        "    results = {}\n",
        "    for term in key_terms:\n",
        "        prompt = (\n",
        "            f\"You are a legal document analysis assistant.\\n\"\n",
        "            f\"Find the'{term}' in the contract and provide me the value only.\\n\\n\"\n",
        "            f\"Instructions:\\n\"\n",
        "            f\"1. If found, provide a brief summary in simple language and to the point\\n\"\n",
        "            f\"2. Include the section title and page number if available\\n\"\n",
        "            f\"3. Quote only the most important sentence from the actual clause\\n\"\n",
        "            f\"4. If not found, respond with: 'Not found.'\\n\\n\"\n",
        "            f\"Format your response in JSON format:\\n\"\n",
        "            f\"**Section:** [Title] (Page [number])\\n\"\n",
        "            f\"**Summary:** [Brief explanation in plain English]\\n\"\n",
        "            f\"**Key Quote:** \\\"[Most relevant sentence]\\\"\\n\\n\"\n",
        "            f\"Document:\\n{text}...\"\n",
        "        )\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal contract analysis assistant. Provide clear, concise explanations that non-lawyers can understand.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        answer = completion.choices[0].message.content\n",
        "        print(\"****************LLM Answer*******************\")\n",
        "        print(answer)\n",
        "\n",
        "        # Parse JSON response to extract summary\n",
        "        import json\n",
        "        import re\n",
        "        try:\n",
        "            # Extract JSON from response if it's wrapped in ```json blocks\n",
        "            json_match = re.search(r'```json\\s*(.*?)\\s*```', answer, re.DOTALL)\n",
        "            if json_match:\n",
        "                json_str = json_match.group(1)\n",
        "            else:\n",
        "                json_str = answer\n",
        "\n",
        "            parsed_response = json.loads(json_str)\n",
        "            summary = parsed_response.get(\"Summary\", \"Not found\")\n",
        "\n",
        "            # Extract page number from Section field\n",
        "            page_number = None\n",
        "            section = parsed_response.get(\"Section\", \"\")\n",
        "            if \"Page\" in section:\n",
        "                page_match = re.search(r'Page (\\d+)', section)\n",
        "                if page_match:\n",
        "                    page_number = page_match.group(1)\n",
        "        except json.JSONDecodeError:\n",
        "            summary = \"Not found\"\n",
        "            page_number = None\n",
        "\n",
        "        results[term] = {\"Summary\": summary, \"page_number\": page_number}\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a418c07c",
      "metadata": {
        "id": "a418c07c"
      },
      "source": [
        "## ✅ Step 7: Judge the LLM's Response\n",
        "\n",
        "The `judge_llm()` function is responsible for **evaluating the quality of each extracted answer** provided by the LLM for the key terms.\n",
        "\n",
        "---\n",
        "\n",
        "### 📥 It Takes:\n",
        "- `text`: The contract text (already extracted earlier)\n",
        "- `key_terms`: The list of legal terms you’re looking for\n",
        "- `extract_key_terms_response`: The previous step’s output (summaries + page numbers)\n",
        "- `metrics`: A list of evaluation metrics (e.g. accuracy, completeness, clarity)\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 What It Does:\n",
        "\n",
        "1. Loops over each key term.\n",
        "2. For every term and metric:\n",
        "   - Sends a prompt to GPT to **evaluate the LLM's extracted answer**.\n",
        "   - The model gives a **score between 0 to 5** and a **short justification**.\n",
        "3. Interprets the score:\n",
        "   - **Score > 3** → ✅ `LLM_Judge_Response = True` (Pass)\n",
        "   - **Score ≤ 3** → ❌ `LLM_Judge_Response = False` (Fail)\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 Why This Step Matters:\n",
        "\n",
        "This step **objectively measures** how well the AI extracted each legal term, giving you confidence in the quality of analysis.\n",
        "\n",
        "It adds a **scoring + explanation layer**, making your contract analyzer smarter and more reliable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7abb7fde",
      "metadata": {
        "id": "7abb7fde"
      },
      "outputs": [],
      "source": [
        "def judge_llm(key_terms, extract_key_terms_response, metrics):\n",
        "    results = []  # List to store evaluation results for each key term\n",
        "\n",
        "    for term in key_terms:\n",
        "        # Get the LLM's extracted answer for the current key term\n",
        "        llm_answer = extract_key_terms_response.get(term, {}).get(\"Summary\", \"Not found\")\n",
        "\n",
        "        for metric in metrics:\n",
        "            # Construct the evaluation prompt to ask the LLM to judge its own answer\n",
        "            prompt = (\n",
        "                f\"You are an expert contract lawyer. Carefully analyze the following extracted answer for the key term '{term}' \"\n",
        "                f\"using ALL of the following evaluation metrics:\\n\"\n",
        "                f\"{metrics}\\n\"\n",
        "                f\"Extracted Answer: {llm_answer}\\n\"\n",
        "                \"Scoring Instructions:\\n\"\n",
        "                \"- Use a score from 0 to 5, where:\\n\"\n",
        "                \"    0 = The key term is not found, not addressed, or the answer is completely missing/irrelevant.\\n\"\n",
        "                \"    1 = Very poor: answer is mostly missing, incorrect, or fails almost all metrics.\\n\"\n",
        "                \"    2 = Poor: answer is incomplete, incorrect, or fails most metrics.\\n\"\n",
        "                \"    3 = Fair: answer is partially correct, covers some metrics but has notable gaps or errors.\\n\"\n",
        "                \"    4 = Good: answer is mostly correct, covers most metrics, but could be improved.\\n\"\n",
        "                \"    5 = Excellent: answer is fully correct, complete, and meets all metrics.\\n\"\n",
        "                \"- If the extracted answer is 'Not found.' or does not address the key term at all, you MUST give a score of 0.\\n\"\n",
        "                \"- Carefully consider each metric before assigning a score. Do NOT skip intermediate scores (2, 3) if appropriate.\\n\"\n",
        "                \"- Optimize your evaluation for accuracy and completeness.\\n\"\n",
        "                \"Based on all the metrics above, provide:\\n\"\n",
        "                \"- A single overall score from 0 (not found) to 5 (excellent)\\n\"\n",
        "                \"- A short justification (1-2 sentences) to the point for your overall score\\n\"\n",
        "                \"Respond in the format: Score: <number>\\nJustification: <text>\"\n",
        "            )\n",
        "\n",
        "            # Call the LLM to get its evaluation response\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a contract evaluation expert.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            content = completion.choices[0].message.content\n",
        "            print(\"*********JUDGE EVULATION ANSWER\")\n",
        "            print(content)\n",
        "\n",
        "            import re  # Use regex to extract structured score and justification\n",
        "\n",
        "            # Extract score from the response\n",
        "            score_match = re.search(r\"Score:\\s*(\\d+)\", content)\n",
        "            score = int(score_match.group(1)) if score_match else None\n",
        "\n",
        "            # Extract justification from the response\n",
        "            justification_match = re.search(r\"Justification:\\s*(.*)\", content, re.DOTALL)\n",
        "            justification = justification_match.group(1).strip() if justification_match else content\n",
        "\n",
        "            # Determine pass/fail status based on score threshold\n",
        "            pass_fail = True if score is not None and score > 3 else False\n",
        "\n",
        "            # Append results for this term + metric\n",
        "            results.append({\n",
        "                \"key_term_name\": term,\n",
        "                \"llm_extracted_ans_from_doc\": llm_answer,\n",
        "                \"evulation_metric_name\": metric,\n",
        "                \"LLM_Judge_Response\": pass_fail,\n",
        "                \"justification\": justification\n",
        "            })\n",
        "\n",
        "    return results  # Return all evaluation results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebeeb4ba",
      "metadata": {
        "id": "ebeeb4ba"
      },
      "source": [
        "\n",
        "## 📌 Function: process_documents_with_progress\n",
        "\n",
        "Runs a full contract analysis pipeline with live progress updates using Gradio.\n",
        "\n",
        "🧾 What it does:\n",
        "-----------------\n",
        "- Extracts raw text from an uploaded contract file.\n",
        "- Identifies and extracts key terms from the contract using predefined keywords.\n",
        "- Uses a language model to evaluate the quality of the extracted key term answers against defined metrics.\n",
        "- Formats the results into separate DataFrames for display and review.\n",
        "\n",
        "📥 Parameters:\n",
        "-----------------\n",
        "- `contract_file`: Uploaded contract file (e.g. PDF, DOCX).\n",
        "- `progress` (optional): Gradio progress handler to display status updates in the UI.\n",
        "\n",
        "📤 Returns:\n",
        "-----------------\n",
        "- `text`: Extracted text content from the document.\n",
        "- `df1`: Evaluation results for the first group of metrics.\n",
        "- `df2`: Evaluation results for the second group of metrics.\n",
        "- `df3`: Evaluation results for the third group of metrics.\n",
        "- `df`: Full evaluation DataFrame.\n",
        "\n",
        "🛠️ Internal Flow:\n",
        "-----------------\n",
        "1. Extracts text from the document.\n",
        "2. Passes extracted text to `extract_key_terms()` to isolate relevant terms.\n",
        "3. Sends key term results to `judge_llm()` for scoring against evaluation metrics.\n",
        "4. Cleans and organizes the judged results.\n",
        "5. Splits final results into groups for easier UI display.\n",
        "\n",
        "⚠️ Notes:\n",
        "-----------------\n",
        "- The function depends on earlier steps (`extract_text_from_file`, `extract_key_terms`, and `judge_llm`).\n",
        "- It takes both `text` and `key_terms` as input for evaluation.\n",
        "- It uses the key terms to extract specific content, wraps it in JSON, then evaluates and returns a structured summary.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9a7cdc85",
      "metadata": {
        "id": "9a7cdc85"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import time\n",
        "\n",
        "\n",
        "def process_documents_with_progress(contract_file, progress=gr.Progress()):\n",
        "    \"\"\"\n",
        "    Process documents with progress updates\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Extract text from contract file\n",
        "        progress(0.1, desc=\"📄 Extracting text from contract file...\")\n",
        "        text, docs = extract_text_from_file(contract_file)\n",
        "        progress(0.2, desc=\"✅ Text extraction completed\")\n",
        "\n",
        "        # Step 2: Extract key terms\n",
        "        progress(0.3, desc=\"🔍 Extracting key terms from contract...\")\n",
        "        key_term_results = extract_key_terms(text, KEY_TERMS)\n",
        "        progress(0.5, desc=\"✅ Key terms extraction completed\")\n",
        "\n",
        "        # Step 3: Judge each key term\n",
        "        progress(0.6, desc=\"⚖️ Evaluating key terms with LLM judge...\")\n",
        "        evals = judge_llm(\n",
        "            KEY_TERMS,\n",
        "            key_term_results,\n",
        "            EVALUATION_METRICS\n",
        "        )\n",
        "        progress(0.8, desc=\"✅ Evaluation completed\")\n",
        "\n",
        "        # Step 4: Format results\n",
        "        progress(0.9, desc=\"📊 Formatting results for display...\")\n",
        "\n",
        "        # Format each evaluation result for display\n",
        "        for e in evals:\n",
        "            term = e[\"key_term_name\"]\n",
        "            llm_ans = e[\"llm_extracted_ans_from_doc\"]\n",
        "            # Extract only the text after 'Text:'\n",
        "            if llm_ans:\n",
        "                text_match = re.search(r'Text:\\s*(.*)', llm_ans, re.DOTALL)\n",
        "                e[\"llm_extracted_ans_from_doc\"] = text_match.group(1).strip() if text_match else llm_ans\n",
        "                # # Extract page number from LLM answer\n",
        "                # page_match = re.search(r'Page:\\s*(\\d+)', llm_ans)\n",
        "                # e[\"llm_page_number\"] = page_match.group(1) if page_match else \"Not found\"\n",
        "            else:\n",
        "                e[\"llm_page_number\"] = \"Not found\"\n",
        "\n",
        "        # Prepare DataFrame with new columns in the correct order\n",
        "        df = pd.DataFrame(evals)\n",
        "        display_cols = [\n",
        "            \"key_term_name\",\n",
        "            \"llm_extracted_ans_from_doc\",\n",
        "            # \"llm_page_number\",\n",
        "            \"evulation_metric_name\",\n",
        "            \"LLM_Judge_Response\",\n",
        "            \"justification\"\n",
        "        ]\n",
        "        df = df[display_cols]\n",
        "\n",
        "        # Split DataFrame into three based on metric index\n",
        "        metric_groups = [EVALUATION_METRICS[:4], EVALUATION_METRICS[4:8], EVALUATION_METRICS[8:]]\n",
        "        df1 = df[df[\"evulation_metric_name\"].isin(metric_groups[0])].reset_index(drop=True)\n",
        "        df2 = df[df[\"evulation_metric_name\"].isin(metric_groups[1])].reset_index(drop=True)\n",
        "        df3 = df[df[\"evulation_metric_name\"].isin(metric_groups[2])].reset_index(drop=True)\n",
        "\n",
        "        progress(1.0, desc=\"🎉 Processing completed successfully!\")\n",
        "\n",
        "        return text, df1, df2, df3, df\n",
        "\n",
        "    except Exception as e:\n",
        "        progress(1.0, desc=f\"❌ Error occurred: {str(e)}\")\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "240b4303",
      "metadata": {
        "id": "240b4303"
      },
      "source": [
        "## 📌 Gradio UI: LLM Contract Judge\n",
        "\n",
        "🧾 What it does:\n",
        "-----------------\n",
        "A web UI to upload a contract, extract key terms, evaluate them using an LLM, and view or download the results.\n",
        "\n",
        "📥 Inputs:\n",
        "-----------------\n",
        "- `contract_file`: Upload contract (PDF, DOCX, TXT)\n",
        "- `start_btn`: Starts the evaluation process\n",
        "- `download_btn`: Downloads combined results as a CSV\n",
        "\n",
        "📤 Outputs:\n",
        "-----------------\n",
        "- `progress_text`: Shows current processing status\n",
        "- `extracted_text`: Displays the raw extracted contract text\n",
        "- `results_table1/2/3`: Show evaluation results grouped by helpful, honest, and harmless metrics\n",
        "- `download_file`: Final downloadable CSV file of all results\n",
        "\n",
        "🛠️ Internal Flow:\n",
        "-----------------\n",
        "1. User uploads a file and clicks \"Start Evaluating\".\n",
        "2. `run_and_return_tables()` calls `process_documents_with_progress()`.\n",
        "3. Results are returned to the UI and shown in 3 separate tabs.\n",
        "4. User can click \"Download\" to export all tables as a CSV.\n",
        "\n",
        "🎯 Key Features:\n",
        "-----------------\n",
        "- Live progress updates using Gradio's `progress` utility\n",
        "- State handling to preserve processed DataFrames\n",
        "- Easy CSV export after evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e524fd68",
      "metadata": {
        "id": "e524fd68"
      },
      "source": [
        "# When you run the last cell in your notebook, you’ll see a message like the one shown in the image below. Click on the \"Running on local URL\" link—you will be redirected to a new screen where you can interact with the LLM Contract Judge app.\n",
        "\n",
        "![Gradio Local URL Example](Images//img-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d86132ee",
      "metadata": {
        "id": "d86132ee"
      },
      "source": [
        "# Once you are done with the lab, you will see a UI something like this below in the image:\n",
        "\n",
        "![Gradio Local URL Example](Images/img-2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "024957e8",
      "metadata": {
        "id": "024957e8"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Create the main Gradio interface using Blocks\n",
        "with gr.Blocks() as demo:\n",
        "    # Title Markdown\n",
        "    gr.Markdown(\"# 📄 LLM Contract Judge\\nUpload a contract, extract key terms, and evaluate with LLM.\")\n",
        "\n",
        "    # File upload component in a horizontal row layout\n",
        "    with gr.Row():\n",
        "        contract_file = gr.File(label=\"Upload Contract (PDF, DOCX, TXT)\")\n",
        "\n",
        "    # Button to trigger the evaluation process\n",
        "    start_btn = gr.Button(\"🚀 Start Evaluating\", variant=\"primary\")\n",
        "\n",
        "    # A non-editable textbox to show progress or status updates\n",
        "    progress_text = gr.Textbox(\n",
        "        label=\"Processing Status\",\n",
        "        value=\"Ready to start evaluation...\",\n",
        "        interactive=False\n",
        "    )\n",
        "\n",
        "    # Output box to display raw extracted contract text\n",
        "    extracted_text = gr.Textbox(label=\"Extracted Contract Text\", lines=10, interactive=False)\n",
        "\n",
        "    # Tabbed interface for displaying different metric evaluation results\n",
        "    with gr.Tabs():\n",
        "        # Helpful Metrics tab\n",
        "        with gr.TabItem(\"Helpful Metrics\"):\n",
        "            results_table1 = gr.Dataframe(headers=[\n",
        "                \"key_term_name\",\n",
        "                \"llm_extracted_ans_from_doc\",\n",
        "                \"evulation_metric_name\",\n",
        "                \"LLM_Judge_Response\",\n",
        "                \"justification\"\n",
        "            ], label=\"Evaluation Results (Helpful Metrics)\")\n",
        "\n",
        "        # Honest Metrics tab\n",
        "        with gr.TabItem(\"Honest Metrics\"):\n",
        "            results_table2 = gr.Dataframe(headers=[\n",
        "                \"key_term_name\",\n",
        "                \"llm_extracted_ans_from_doc\",\n",
        "                \"evulation_metric_name\",\n",
        "                \"LLM_Judge_Response\",\n",
        "                \"justification\"\n",
        "            ], label=\"Evaluation Results (Honest Metrics)\")\n",
        "\n",
        "        # Harmless Metrics tab\n",
        "        with gr.TabItem(\"Harmless Metrics\"):\n",
        "            results_table3 = gr.Dataframe(headers=[\n",
        "                \"key_term_name\",\n",
        "                \"llm_extracted_ans_from_doc\",\n",
        "                \"evulation_metric_name\",\n",
        "                \"LLM_Judge_Response\",\n",
        "                \"justification\"\n",
        "            ], label=\"Evaluation Results (Harmless Metrics)\")\n",
        "\n",
        "    # Button to download evaluation results as a CSV\n",
        "    download_btn = gr.Button(\"📥 Download All Results as CSV\")\n",
        "\n",
        "    # File component to show the downloadable CSV file\n",
        "    download_file = gr.File(label=\"Download CSV\")\n",
        "\n",
        "    # State variables to store DataFrames for use during download\n",
        "    state_df1 = gr.State()\n",
        "    state_df2 = gr.State()\n",
        "    state_df3 = gr.State()\n",
        "    state_df_all = gr.State()\n",
        "\n",
        "    # Main function to process contract and return data for all tables\n",
        "    def run_and_return_tables(contract_file, progress=gr.Progress()):\n",
        "        if not contract_file:\n",
        "            # If file not uploaded, return error message and clear outputs\n",
        "            return (\n",
        "                \"Please upload a contract file first.\",\n",
        "                gr.update(value=None),\n",
        "                gr.update(value=None),\n",
        "                gr.update(value=None),\n",
        "                gr.update(value=None),\n",
        "                None, None, None, None\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            # Update UI to show progress\n",
        "            progress_text = \"🔄 Starting document processing...\"\n",
        "\n",
        "            # Function processes the document and returns the text and 3 metrics tables\n",
        "            text, df1, df2, df3, df_all = process_documents_with_progress(contract_file, progress)\n",
        "\n",
        "            return (\n",
        "                text,\n",
        "                gr.update(value=df1),\n",
        "                gr.update(value=df2),\n",
        "                gr.update(value=df3),\n",
        "                df1, df2, df3, df_all\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            # Return error details in case of failure\n",
        "            error_msg = f\"❌ Error during processing: {str(e)}\"\n",
        "            return (\n",
        "                error_msg,\n",
        "                gr.update(value=None),\n",
        "                gr.update(value=None),\n",
        "                gr.update(value=None),\n",
        "                gr.update(value=None),\n",
        "                None, None, None\n",
        "            )\n",
        "\n",
        "    # Function to generate and return the downloadable CSV file from results\n",
        "    def download_csv(contract_file, df_all):\n",
        "        if df_all is None:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Define the display columns in the same format as original\n",
        "            display_cols = [\n",
        "                \"key_term_name\",\n",
        "                \"llm_extracted_ans_from_doc\",\n",
        "                # \"llm_page_number\",\n",
        "                \"evulation_metric_name\",\n",
        "                \"LLM_Judge_Response\",\n",
        "                \"justification\"\n",
        "            ]\n",
        "\n",
        "            # Filter to only include the specified columns\n",
        "            final_df = df_all[display_cols]\n",
        "\n",
        "            # Write to a temporary file and return the path\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\", mode=\"w\", encoding=\"utf-8\") as tmp:\n",
        "                final_df.to_csv(tmp, index=False)\n",
        "                tmp_path = tmp.name\n",
        "\n",
        "            return tmp_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in download_csv: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    # Trigger processing function when 'Start Evaluating' is clicked\n",
        "    start_btn.click(\n",
        "        run_and_return_tables,\n",
        "        inputs=[contract_file],\n",
        "        outputs=[extracted_text, results_table1, results_table2, results_table3, state_df1, state_df2, state_df3, state_df_all]\n",
        "    )\n",
        "\n",
        "    # Trigger CSV download when 'Download' is clicked\n",
        "    download_btn.click(\n",
        "        download_csv,\n",
        "        inputs=[contract_file, state_df_all],\n",
        "        outputs=download_file\n",
        "    )\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
