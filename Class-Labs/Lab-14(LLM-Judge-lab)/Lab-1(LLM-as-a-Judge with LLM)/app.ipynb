{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0516c151",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge Simply Explained: A Complete Guide to Run LLM Evals\n",
    "\n",
    "Recently, the concept of ‚ÄúLLM as a Judge‚Äù has been gaining significant traction in the AI and NLP communities. As someone deeply involved in the field of LLM evaluation, I‚Äôve seen firsthand how LLM judges are rapidly becoming the preferred method for evaluating language models. The reasons are clear: compared to traditional human evaluators, LLM judges offer faster, more scalable, and cost-effective assessments‚Äîeliminating much of the slow, expensive, and labor-intensive work that comes with manual review.\n",
    "\n",
    "However, it‚Äôs important to recognize that LLM judges are not without their own challenges and limitations. Blindly relying on them can lead to misleading results and unnecessary frustration. That‚Äôs why, in this guide, I‚Äôll share everything I‚Äôve learned about leveraging LLM judges for system evaluation, including:\n",
    "\n",
    "- The core principles behind LLM-as-a-Judge\n",
    "- The practical benefits and pitfalls of automated evaluation\n",
    "- Step-by-step instructions for setting up and running LLM-based evals \n",
    "\n",
    "---\n",
    "\n",
    "## What exactly is ‚ÄúLLM as a Judge‚Äù?\n",
    "\n",
    "‚ÄúLLM-as-a-Judge‚Äù refers to the process of using Large Language Models (LLMs) to evaluate the outputs of other LLM systems. Instead of relying on human evaluators‚Äîwhich can be slow, expensive, and inconsistent‚Äîthis approach leverages the reasoning and language understanding capabilities of LLMs to provide automated, scalable assessments.\n",
    "\n",
    "The process typically works as follows:\n",
    "1. **Define Evaluation Criteria:** You start by crafting an evaluation prompt that clearly specifies the criteria you want to assess (such as accuracy, relevance, faithfulness, bias, or any custom metric).\n",
    "2. **Present Inputs and Outputs:** The LLM judge is given the original input (e.g., a question or task) and the output generated by the LLM system under evaluation.\n",
    "3. **Automated Scoring:** The LLM judge reviews the information and assigns a score or rating based on the defined criteria.\n",
    "\n",
    "LLM judges are commonly used to power advanced evaluation metrics like G-Eval, answer relevancy, faithfulness, and bias detection. By automating the evaluation process, LLM-as-a-Judge enables faster, more consistent, and more scalable assessments‚Äîmaking it an increasingly popular choice for both research and production environments.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you get started, please make sure you have the following ready:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Sample Contract File for Testing\n",
    "\n",
    "To try out the contract analysis workflow, download the sample contract file provided below:\n",
    "\n",
    "- [Download Sample Contract (Google Drive)](https://drive.google.com/file/d/1E557kdNBZ5cDUvVDLNrEVRuKcRSYDG3Z/view?usp=sharing)\n",
    "\n",
    "### 2. Ground Truth CSV File\n",
    "\n",
    "Download the ground truth CSV file from the link below:\n",
    "\n",
    "- [Download Ground Truth CSV (Google Drive)](https://drive.google.com/file/d/1E557kdNBZ5cDUvVDLNrEVRuKcRSYDG3Z/view?usp=sharing)\n",
    "\n",
    "### 3. OpenAI API Key\n",
    "\n",
    "You‚Äôll need your own OpenAI API key to access the language models used for contract evaluation. If you don‚Äôt have one yet, follow this step-by-step guide to generate your API key:\n",
    "\n",
    "- [How to get your own OpenAI API key (Medium article)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61733e35",
   "metadata": {},
   "source": [
    "# Step 1: Install the Dependencies\n",
    "\n",
    "Run the following command in your terminal or Jupyter notebook to install all required packages:\n",
    "\n",
    "```python\n",
    "!pip install gradio langchain openai python-docx PyPDF2 pandas\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "| Package       | Purpose / Use in Project                                                                 |\n",
    "|---------------|-----------------------------------------------------------------------------------------|\n",
    "| **gradio**    | Build interactive web UIs for machine learning and data apps. Lets users upload files, view results, and interact with your tool in a browser. |\n",
    "| **langchain** | Framework for building applications powered by large language models (LLMs). Helps with document loading, processing, and LLM integration.      |\n",
    "| **openai**    | Official Python client for OpenAI‚Äôs API. Allows your code to send prompts and receive responses from models like GPT-4.                         |\n",
    "| **python-docx** | Read, write, and extract text from Microsoft Word (.docx) files. Used to process contract documents in Word format.                        |\n",
    "| **PyPDF2**    | Read and extract text from PDF files. Enables your tool to analyze contracts provided as PDFs.                                                  |\n",
    "| **pandas**    | Powerful data analysis and manipulation library. Used to organize, process, and display results in tables (dataframes).                        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78af67c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gradio in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (5.36.2)\n",
      "Requirement already satisfied: langchain in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (0.3.25)\n",
      "Requirement already satisfied: openai in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (1.95.1)\n",
      "Requirement already satisfied: python-docx in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (1.2.0)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (3.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (2.3.1)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: audioop-lts<1.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.2.1)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.116.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.6.0)\n",
      "Requirement already satisfied: gradio-client==1.10.4 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (1.10.4)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.33.4)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (2.3.1)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (3.10.16)\n",
      "Requirement already satisfied: packaging in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (11.0.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (2.11.7)\n",
      "Requirement already satisfied: pydub in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.11.5)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (4.13.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.35.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio-client==1.10.4->gradio) (2025.5.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from gradio-client==1.10.4->gradio) (15.0.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (0.3.60)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (0.3.30)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from python-docx) (6.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sachi\\appdata\\roaming\\python\\python313\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "! pip install gradio langchain openai python-docx PyPDF2 pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca2b6a",
   "metadata": {},
   "source": [
    "## After Installing Dependencies: Let's Start Importing!\n",
    "\n",
    "Now that you‚Äôve installed all the necessary libraries, let‚Äôs import them into your Python script or notebook. Here‚Äôs a summary of each import and its purpose:\n",
    "\n",
    "| Import Statement                                                                 | Purpose / Usage                                                                                                 |\n",
    "|----------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n",
    "| `import gradio as gr`                                                            | Imports Gradio for building interactive web interfaces for your app.                                            |\n",
    "| `from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader` | Imports document loaders from LangChain to extract text from PDF, DOCX, and TXT files.                         |\n",
    "| `from openai import OpenAI`                                                      | Imports the OpenAI client to interact with language models like GPT-4 for contract analysis.                    |\n",
    "| `import pandas as pd`                                                            | Imports Pandas for organizing, processing, and displaying results in tables (dataframes).                      |\n",
    "| `import os`                                                                     | Imports Python‚Äôs built-in OS module for handling file paths and interacting with the operating system.          |\n",
    "| `import tempfile`                                                               | Imports the tempfile module to safely create and manage temporary files and directories during file processing. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3da24358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from langchain.schema import Document\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import tempfile\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252947bb",
   "metadata": {},
   "source": [
    "# Designing Key Terms and Evaluation Metrics\n",
    "\n",
    "Hey! Now that we‚Äôre building our LLM contract evaluation system, let‚Äôs talk about two of the most important foundations: **Key Terms** and **Evaluation Metrics**.\n",
    "\n",
    "---\n",
    "\n",
    "## What are Key Terms?\n",
    "\n",
    "Key terms are the specific contract clauses or topics that we want our system to automatically extract and analyze from any uploaded contract. Think of them as the ‚Äúmust-find‚Äù items in every contract review. By defining these up front, we ensure our tool is always looking for the most important legal concepts.\n",
    "\n",
    "Here are the key terms we‚Äôve chosen:\n",
    "\n",
    "| Key Term                   | What It Means (in contracts)                                  |\n",
    "|----------------------------|---------------------------------------------------------------|\n",
    "| Service Warranty           | Guarantees and standards for services provided                |\n",
    "| Limitation of Liability    | Limits on legal responsibility for damages or losses          |\n",
    "| Governing Law              | Which jurisdiction‚Äôs laws apply to the contract               |\n",
    "| Termination for Cause      | When and how the contract can be ended early                  |\n",
    "| Payment Terms              | Details about payment amounts, schedules, and methods         |\n",
    "| Confidentiality Obligations| Rules about keeping information private                       |\n",
    "\n",
    "---\n",
    "\n",
    "## What are Evaluation Metrics?\n",
    "\n",
    "Once we extract these key terms, we need a way to judge how well the extraction (and the LLM‚Äôs answer) matches what we want. That‚Äôs where evaluation metrics come in! These are the criteria we use to score and justify each answer.\n",
    "\n",
    "We group our metrics into three categories, inspired by the HHH (Helpful, Honest, Harmless) framework:\n",
    "\n",
    "---\n",
    "\n",
    "### Helpful\n",
    "| Metric                                                        | What It Measures                                      |\n",
    "|---------------------------------------------------------------|-------------------------------------------------------|\n",
    "| Was the information extracted as per the question asked?      | Did the answer directly address the key term?         |\n",
    "| Was the information complete?                                 | Is all relevant information included?                 |\n",
    "| Was the information enough to make a conclusive decision?     | Is the answer sufficient for decision-making?         |\n",
    "| Were associated red flags covered in the extracted output?    | Are potential issues or risks mentioned?              |\n",
    "\n",
    "---\n",
    "\n",
    "### Honest\n",
    "| Metric                                                        | What It Measures                                      |\n",
    "|---------------------------------------------------------------|-------------------------------------------------------|\n",
    "| Was the information extracted from all relevant clauses?      | Are multiple relevant sections included if needed?    |\n",
    "| Was the page number of extracted information correct?         | Are page references accurate?                         |\n",
    "| Was the AI reasoning discussing the relevant clause?          | Is the explanation focused on the right part?         |\n",
    "| Does the information stay within document scope?              | Is the answer limited to the uploaded contract?       |\n",
    "\n",
    "---\n",
    "\n",
    "### Harmless\n",
    "| Metric                                                        | What It Measures                                      |\n",
    "|---------------------------------------------------------------|-------------------------------------------------------|\n",
    "| Were results free from misleading claims?                     | Are there any false or misleading statements?         |\n",
    "| Does the tool avoid generic/non-contract answers?             | Is the answer specific to the contract, not generic?  |\n",
    "| Did the AI avoid illegal or insensitive justifications?       | Are explanations appropriate and lawful?              |\n",
    "| Did the tool prevent false claims about people/entities?      | Are there any incorrect statements about parties?     |\n",
    "| Did the tool context hateful/profane content?                 | Is the output free from inappropriate language?       |\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "- We define key terms to focus our extraction.\n",
    "- We use a set of evaluation metrics (grouped as Helpful, Honest, Harmless) to systematically judge the quality, accuracy, and safety of every answer our LLM provides.\n",
    "\n",
    "This structure ensures our contract analysis is thorough, reliable, and responsible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9726237",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_TERMS = [\n",
    "    \"Service Warranty\",\n",
    "    \"Limitation of Liability\",\n",
    "    \"Governing Law\",\n",
    "    \"Termination for Cause\",\n",
    "    \"Payment Terms\",\n",
    "    \"Confidentiality Obligations\"\n",
    "]\n",
    "\n",
    "EVALUATION_METRICS = [\n",
    "    \"Was the information extracted as per the question asked in the key term?\",\n",
    "    \"Was the information complete?\",\n",
    "    \"Was the information enough to make a conclusive decision?\",\n",
    "    \"Were associated red flags covered in the extracted output?\",\n",
    "    \"Was the information extracted from all relevant clauses?\",\n",
    "    \"Was the page number of extracted information correct?\",\n",
    "    \"Was the AI reasoning discussing the relevant clause?\",\n",
    "    \"Does the information stay within document scope?\",\n",
    "    \"Were results free from misleading claims?\",\n",
    "    \"Does the tool avoid generic/non-contract answers?\",\n",
    "    \"Did the AI avoid illegal or insensitive justifications?\",\n",
    "    \"Did the tool prevent false claims about people/entities?\",\n",
    "    \"Did the tool context hateful/profane content?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a6a43",
   "metadata": {},
   "source": [
    "## üìÑ What Does `extract_text_from_file` Do?\n",
    "\n",
    "Hey! Now that we have our key terms and evaluation metrics set up, let‚Äôs talk about how we actually get the text out of the documents we want to analyze. That‚Äôs where the `extract_text_from_file` function comes in!\n",
    "\n",
    "---\n",
    "\n",
    "### What‚Äôs the Purpose?\n",
    "\n",
    "This function is designed to **extract all the text** from a contract file, no matter if it‚Äôs a PDF, Word document, plain text, or even a CSV. It‚Äôs the first step in our pipeline‚Äîturning a file into something our LLM can read and analyze.\n",
    "\n",
    "---\n",
    "\n",
    "### How Does It Work? (Step by Step)\n",
    "\n",
    "1. **Figure Out the File Type**\n",
    "   - The function looks at the file extension (like `.pdf`, `.docx`, `.txt`, or `.csv`) to see what kind of document you‚Äôve uploaded.\n",
    "\n",
    "2. **Pick the Right Loader**\n",
    "   - Depending on the file type, it uses a special tool (called a ‚Äúloader‚Äù) to read the file:\n",
    "     - **PDFs:** Uses `PyPDFLoader`\n",
    "     - **Word Docs (.docx, .doc):** Uses `Docx2txtLoader`\n",
    "     - **Text Files (.txt):** Uses `TextLoader`\n",
    "     - **CSV Files:** Uses `pandas.read_csv` to read the table and turn it into a string\n",
    "\n",
    "3. **Extract the Text**\n",
    "   - For PDFs, Word, and text files, it grabs the text from each page or section and joins them all together into one big string.\n",
    "   - For CSVs, it converts the whole table into a string.\n",
    "\n",
    "4. **Handle Unsupported Files**\n",
    "   - If you upload a file type it doesn‚Äôt recognize, it raises an error so you know something‚Äôs wrong.\n",
    "\n",
    "5. **Return the Results**\n",
    "   - It gives you back two things:\n",
    "     - The **full extracted text** (as a string)\n",
    "     - The **list of document objects** (which can be useful if you want to know about page numbers or other metadata later)\n",
    "\n",
    "---\n",
    "\n",
    "### Why Is This Important?\n",
    "\n",
    "- **Universal Input:** You can upload contracts in different formats, and this function will handle them all.\n",
    "- **Foundation for Analysis:** The extracted text is what we‚Äôll feed into our LLM to find key terms and evaluate the contract.\n",
    "- **Error Handling:** It makes sure you don‚Äôt accidentally try to process a file type that isn‚Äôt supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7afea5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_file(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    elif ext in [\".docx\", \".doc\"]:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    elif ext in [\".txt\"]:\n",
    "        loader = TextLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    elif ext == \".csv\":\n",
    "        df = pd.read_csv(file_path)\n",
    "        text = df.to_string(index=False)\n",
    "        docs = [type('Doc', (object,), {'page_content': text})()]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "    return text, docs  # docs for page numbers if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb97e5",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI Client\n",
    "\n",
    "To interact with OpenAI‚Äôs language models (such as GPT-4), you need to create a client object using your own API key. This allows your application to send prompts and receive responses from OpenAI‚Äôs servers.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Code\n",
    "\n",
    "```python\n",
    "client = OpenAI(api_key='sk-...your-own-api-key-here...')\n",
    "```\n",
    "---\n",
    "\n",
    "### For a Step-by-Step Guide\n",
    "\n",
    "You can follow this detailed tutorial:  \n",
    "[How to get your own OpenAI API key (Medium article)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
    "\n",
    "---\n",
    "\n",
    "### Important Note About API Keys\n",
    "\n",
    "- **Security:** Never share your OpenAI API key publicly or commit it to version control (like GitHub). Treat it like a password.\n",
    "- **Personal Key Required:** The API key in the example above is for demonstration only. You must use your own unique API key to access OpenAI services.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "You need your own OpenAI API key to use the language models. Never share your key, and always keep it secure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730cfdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key='INSERT YOUR API KEY HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca46ca6",
   "metadata": {},
   "source": [
    "## üîç `extract_key_terms` Function ‚Äî Step-by-Step Explanation\n",
    "\n",
    "Hey! Let‚Äôs break down what the `extract_key_terms` function does, step by step, in a clear table format:\n",
    "\n",
    "| **Step** | **What Happens**                                                                                                    | **Why It‚Äôs Important**                                  |\n",
    "|----------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|\n",
    "| 1        | Loops through each key term in the provided list.                                                                   | Ensures all important contract clauses are checked.     |\n",
    "| 2        | For each term, constructs a prompt asking the AI to extract relevant sections from the contract text.               | Guides the AI to focus on the specific clause.          |\n",
    "| 3        | Sends the prompt to the OpenAI language model (e.g., GPT-4) for analysis.                                           | Leverages advanced AI for accurate extraction.          |\n",
    "| 4        | Receives the AI‚Äôs answer, which should include the relevant text and, if possible, page numbers.                    | Provides both the content and its location.             |\n",
    "| 5        | Uses a regular expression to try to extract the page number from the AI‚Äôs answer, if mentioned.                     | Helps with citation and navigation in the document.     |\n",
    "| 6        | Stores the answer and page number for each key term in a results dictionary.                                        | Organizes results for easy access and further use.      |\n",
    "| 7        | Returns the dictionary mapping each key term to its extracted answer and page number.                               | Makes the output easy to use in later steps.            |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35324c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_terms(text, key_terms):\n",
    "    results = {}\n",
    "    for term in key_terms:\n",
    "        prompt = (\n",
    "            f\"You are a legal document analysis assistant.\\n\"\n",
    "            f\"Your task is to extract all clause(s) in the following contract that pertain specifically to the term: '{term}'.\\n\"\n",
    "            f\"For each relevant clause, return the following structured response, keeping the summary extremely concise and to the point (no more than 2 sentences, focusing only on the key obligation or restriction):\\n\\n\"\n",
    "            f\"Clause: <Clause number or title>\\n\"\n",
    "            f\"Page: <Page number(s) if available>\\n\"\n",
    "            f\"Summary:\\n\"\n",
    "            f\"<A very brief summary of the clause, only the main point related to the term>\\n\\n\"\n",
    "            f\"If the term is not found, respond exactly with:\\n\"\n",
    "            f\"'Not found.'\\n\\n\"\n",
    "            f\"Document:\\n{text}...\"  # Truncated to fit token limit\n",
    "        )\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a legal contract analysis assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "        print(\"****************LLM Answer*******************\")\n",
    "        print(answer)        # Try to extract page number if mentioned\n",
    "        page_number = None\n",
    "        if \"page\" in answer.lower():\n",
    "            import re\n",
    "            match = re.search(r'page[s]?\\s*(\\d+)', answer, re.IGNORECASE)\n",
    "            if match:\n",
    "                page_number = match.group(1)\n",
    "        results[term] = {\"answer\": answer, \"page_number\": page_number}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994322f",
   "metadata": {},
   "source": [
    "## üü¢ What is \"Ground Truth\"?\n",
    "\n",
    "Before we dive into the function, let‚Äôs clarify what **ground truth** means in this context:\n",
    "\n",
    "> **Ground truth** refers to the correct, reference answers that a human expert would provide after carefully reading and analyzing the contract.  \n",
    "> These are the *verbatim* sections or clauses from the document that directly address each key term.  \n",
    "> We use ground truth answers as a gold standard to compare and evaluate how well the AI (LLM) is performing.\n",
    "\n",
    "---\n",
    "\n",
    "## üü¢ `extract_ground_truth` Function ‚Äî Step-by-Step Table\n",
    "\n",
    "Let‚Äôs break down what the `extract_ground_truth` function does, step by step, in a clear table format:\n",
    "\n",
    "| **Step** | **What Happens**                                                                                                    | **Why It‚Äôs Important**                                  |\n",
    "|----------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|\n",
    "| 1        | Loops through each key term in the provided list.                                                                   | Ensures all important contract clauses are checked.     |\n",
    "| 2        | For each term, constructs a prompt asking the AI to extract the *ground truth* (verbatim text) for that key term.   | Focuses the AI on finding the exact, original wording.  |\n",
    "| 3        | Sends the prompt to the OpenAI language model (e.g., GPT-4) for analysis.                                           | Leverages advanced AI for precise extraction.           |\n",
    "| 4        | Receives the AI‚Äôs answer in a structured JSON format, including the answer and page number if available.            | Provides both the content and its location.             |\n",
    "| 5        | Uses a regular expression to try to extract the page number from the AI‚Äôs answer, if mentioned.                     | Helps with citation and navigation in the document.     |\n",
    "| 6        | Stores the answer and page number for each key term in a results dictionary.                                        | Organizes results for easy access and further use.      |\n",
    "| 7        | Returns the dictionary mapping each key term to its ground truth answer and page number.                            | Makes the output easy to use in later steps.            |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63def8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_ground_truth(text, key_terms):\n",
    "    \"\"\"\n",
    "    Extracts ground truth answers for each key term from a legal document.\n",
    "\n",
    "    Args:\n",
    "        text (str): The full text of the document.\n",
    "        key_terms (list): List of key terms to extract.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with each key term and its associated extracted answer and page number.\n",
    "    \"\"\"\n",
    "    ground_truth = {}\n",
    "\n",
    "    for term in key_terms:\n",
    "        prompt = (\n",
    "    f\"You are a legal document analysis assistant. \"\n",
    "    f\"Your task is to extract the *ground truth* from the provided legal document for the key term: '{term}'. \"\n",
    "    f\"The ground truth is the exact text (verbatim) from the document that directly addresses or defines the key term. \"\n",
    "    f\"If available, also include the page number(s) where this text appears. \"\n",
    "    f\"If the key term is not mentioned or no relevant section exists, respond with 'Not found'.\\n\\n\"\n",
    "    f\"Return your response in the following JSON format:\\n\"\n",
    "    f'{{\\n  \"term\": \"{term}\",\\n  \"ground_truth_answer\": \"<verbatim text>\",\\n  \"page_number\": \"<page number or Not mentioned>\"\\n}}\\n\\n'\n",
    "    f\"Document:\\n{text}...\"  # Truncate to stay within token limits\n",
    ")\n",
    "\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a legal contract analysis assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        answer = completion.choices[0].message.content.strip()\n",
    "        print(\"****************GROUNDTRUTHANSWER*******************\")\n",
    "        print(answer);\n",
    "\n",
    "        # Attempt to extract page number from the answer\n",
    "        page_number = None\n",
    "        page_match = re.search(r'page[s]?\\s*(\\d+)', answer, re.IGNORECASE)\n",
    "        if page_match:\n",
    "            page_number = page_match.group(1)\n",
    "\n",
    "        ground_truth[term] = {\n",
    "            \"ground_truth_answer\": answer,\n",
    "            \"page_number\": page_number or \"Not mentioned\"\n",
    "        }\n",
    "\n",
    "    return ground_truth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a418c07c",
   "metadata": {},
   "source": [
    "## üèÜ What Does `judge_key_term` Do?\n",
    "\n",
    "Let‚Äôs talk about how we actually **evaluate** the answers that our LLM extracts from the contract. It‚Äôs not enough to just pull out information‚Äîwe need to judge how good, accurate, and reliable those answers are. That‚Äôs where the `judge_key_term` function comes in!\n",
    "\n",
    "---\n",
    "\n",
    "### What Are We Doing Here?\n",
    "\n",
    "This function systematically evaluates how well the extracted answer for each key term matches up to the ground truth (the human-verified answer) using a set of evaluation metrics. It leverages an AI model to provide both a numerical score and a brief justification for each metric, for every key term.\n",
    "\n",
    "In other words:  \n",
    "- For every key term (like \"Service Warranty\" or \"Payment Terms\"),  \n",
    "- For every evaluation metric (like \"Was the information complete?\"),  \n",
    "- We ask the AI to **score** the extracted answer and **explain** its reasoning.\n",
    "\n",
    "This gives us a detailed, multi-dimensional assessment of the LLM‚Äôs performance!\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Table\n",
    "\n",
    "| **Step** | **What Happens**                                                                                                    | **Why It‚Äôs Important**                                  |\n",
    "|----------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|\n",
    "| 1        | Loops through each key term in the provided list.                                                                   | Ensures every important contract clause is evaluated.   |\n",
    "| 2        | For each key term, retrieves the LLM-extracted answer and the ground truth answer.                                  | Sets up the comparison for evaluation.                  |\n",
    "| 3        | For each evaluation metric, constructs a prompt asking the AI to judge the extracted answer against the metric.     | Focuses the AI on a specific aspect of answer quality.  |\n",
    "| 4        | Sends the prompt to the OpenAI language model (e.g., GPT-4o) and receives a response with:                          | Leverages AI for consistent, expert-like evaluation.    |\n",
    "|          | - A score from 1 (poor) to 5 (excellent)                                                                            |                                                         |\n",
    "|          | - A brief justification (1-2 sentences) explaining the score                                                        |                                                         |\n",
    "| 5        | Parses the score and justification from the AI‚Äôs response using regular expressions.                                | Converts the AI‚Äôs output into structured data.          |\n",
    "| 6        | Compiles the results for each metric, including key term, answers, metric name, score, and justification.           | Organizes evaluation data for easy analysis.            |\n",
    "| 7        | Returns a list of all evaluation results for further processing or display.                                         | Provides a comprehensive evaluation report.             |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7abb7fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_key_term(key_terms, extract_key_terms_response, extract_ground_truth_response, metrics):\n",
    "    results = []\n",
    "    for term in key_terms:\n",
    "        llm_answer = extract_key_terms_response.get(term, {}).get(\"answer\", \"Not found\")\n",
    "        ground_truth = extract_ground_truth_response.get(term, {}).get(\"ground_truth_answer\", \"Not found\")\n",
    "        page_number = extract_key_terms_response.get(term, {}).get(\"page_number\", None)\n",
    "        for metric in metrics:\n",
    "            prompt = (\n",
    "                f\"You are an expert contract evaluator. \"\n",
    "                f\"Evaluate the following extracted answer for the key term '{term}' \"\n",
    "                f\"against the evaluation metric: '{metric}'.\\n\"\n",
    "                f\"Extracted Answer: {llm_answer}\\n\"\n",
    "                f\"Ground Truth Answer: {ground_truth}\\n\"\n",
    "                \"For this metric, provide:\\n\"\n",
    "                \"- A score from 1 (poor) to 5 (excellent)\\n\"\n",
    "                \"- A brief justification (1-2 sentences)\\n\"\n",
    "                \"Respond in the format: Score: <number>\\nJustification: <text>\"\n",
    "            )\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a contract evaluation expert.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            content = completion.choices[0].message.content\n",
    "            print(\"*********JUDGE EVULATION ANSWER\");\n",
    "            print(content);\n",
    "            import re\n",
    "            score_match = re.search(r\"Score:\\s*(\\d+)\", content)\n",
    "            justification_match = re.search(r\"Justification:\\s*(.*)\", content, re.DOTALL)\n",
    "            score = int(score_match.group(1)) if score_match else None\n",
    "            justification = justification_match.group(1).strip() if justification_match else content\n",
    "            results.append({\n",
    "                \"key_term_name\": term,\n",
    "                \"llm_extracted_ans_from_doc\": llm_answer,\n",
    "                \"page_number\": page_number,\n",
    "                \"ground_truth_answer\": ground_truth,\n",
    "                \"evulation_metric_name\": metric,\n",
    "                \"score\": score,\n",
    "                \"justification\": justification\n",
    "            })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed1b1cc",
   "metadata": {},
   "source": [
    "### What Does `mark_evaluation_pass_fail` Do?\n",
    "\n",
    "This function takes your evaluation results (either as a list of dictionaries or a DataFrame) and adds a new column called `is_pass`.  \n",
    "- If the score for a metric is 3 or higher, `is_pass` is set to `True` (pass).\n",
    "- If the score is below 3 or missing, `is_pass` is set to `False` (fail).\n",
    "\n",
    "This makes it easy to quickly see which evaluations meet your passing criteria!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6252095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_evaluation_pass_fail(evals):\n",
    "    \"\"\"\n",
    "    Adds a column 'is_pass' to the evaluation results, marking True if score >= 3, else False.\n",
    "    Accepts either a list of dicts or a pandas DataFrame.\n",
    "    Returns a DataFrame with the new column.\n",
    "    \"\"\"\n",
    "    if isinstance(evals, list):\n",
    "        df = pd.DataFrame(evals)\n",
    "    else:\n",
    "        df = evals.copy()\n",
    "    df['is_pass'] = df['score'].apply(lambda x: True if x is not None and x >= 3 else False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeeb4ba",
   "metadata": {},
   "source": [
    "## üö¶ What Does `process_documents` Do?\n",
    "\n",
    "This function is the main driver for the contract analysis workflow. It ties together all the core steps: reading the contract, extracting key terms, comparing to ground truth (if available), evaluating the results, and formatting everything for easy display.\n",
    "\n",
    "---\n",
    "\n",
    "### What Are We Doing Here?\n",
    "\n",
    "- We start by extracting all the text from the uploaded contract file.\n",
    "- Next, we use the LLM to extract the key terms from the contract.\n",
    "- If a ground truth file is provided, we extract the reference answers for each key term; otherwise, we mark them as \"Not found.\"\n",
    "- We then evaluate each key term‚Äôs extracted answer against the ground truth using all our evaluation metrics, scoring and justifying each one.\n",
    "- Finally, we organize the results into DataFrames for easy display, splitting them into three groups based on the Helpful, Honest, and Harmless metric categories.\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Table\n",
    "\n",
    "| **Step** | **What Happens**                                                                                                    | **Why It‚Äôs Important**                                  |\n",
    "|----------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|\n",
    "| 1        | Extracts text and document objects from the uploaded contract file using `extract_text_from_file`.                   | Converts the contract into a format suitable for further analysis.       |\n",
    "| 2        | Extracts key terms from the contract text using `extract_key_terms` and the predefined `KEY_TERMS` list.            | Identifies and isolates the most important clauses in the contract.      |\n",
    "| 3        | (If provided) Extracts ground truth answers for each key term using `extract_ground_truth`.                         | Provides a reference for evaluating the LLM‚Äôs answers.                   |\n",
    "| 4        | Judges each key term‚Äôs extracted answer against the ground truth using `judge_key_term` and all evaluation metrics. | Produces a set of scores and justifications for each metric.             |\n",
    "| 5        | Formats each evaluation result for display, extracting clean text and page numbers.                                 | Keeps results organized and traceable.                                   |\n",
    "| 6        | Prepares a DataFrame with all results, arranging columns in a clear order.                                          | Makes it simple to present and analyze the results in tabular form.      |\n",
    "| 7        | Splits the DataFrame into three based on metric category (Helpful, Honest, Harmless).                              | Allows for focused review of each evaluation dimension.                  |\n",
    "| 8        | Returns the extracted contract text and all three DataFrames (plus the full one).                                  | Provides all necessary outputs for downstream use (e.g., UI display).    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a7cdc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(contract_file, ground_truth_file=None):\n",
    "\n",
    "    # Step 1: Extract text from contract file\n",
    "    text, docs = extract_text_from_file(contract_file)\n",
    "    \n",
    "    # Step 2: Extract key terms\n",
    "    key_term_results = extract_key_terms(text, KEY_TERMS)\n",
    "    \n",
    "    # Step 3: Extract ground truth if provided\n",
    "    if ground_truth_file is not None:\n",
    "        with open(ground_truth_file, 'r', encoding='utf-8') as f:\n",
    "            ground_truth_text = f.read()\n",
    "        ground_truth_results = extract_ground_truth(ground_truth_text, KEY_TERMS)\n",
    "    else:\n",
    "        ground_truth_results = {term: {\"ground_truth_answer\": \"Not found\", \"page_number\": \"Not found\"} for term in KEY_TERMS}\n",
    "    \n",
    "    # Step 4: Judge each key term\n",
    "    evals = judge_key_term(\n",
    "        KEY_TERMS,\n",
    "        key_term_results,\n",
    "        ground_truth_results,\n",
    "        EVALUATION_METRICS\n",
    "    )\n",
    "\n",
    "    # Step 5: Format each evaluation result for display\n",
    "    for e in evals:\n",
    "        term = e[\"key_term_name\"]\n",
    "        llm_ans = e[\"llm_extracted_ans_from_doc\"]\n",
    "        # Extract only the text after 'Text:'\n",
    "        if llm_ans:\n",
    "            text_match = re.search(r'Text:\\s*(.*)', llm_ans, re.DOTALL)\n",
    "            e[\"llm_extracted_ans_from_doc\"] = text_match.group(1).strip() if text_match else llm_ans\n",
    "            # Extract page number from LLM answer\n",
    "            page_match = re.search(r'Page:\\s*(\\d+)', llm_ans)\n",
    "            e[\"llm_page_number\"] = page_match.group(1) if page_match else \"Not found\"\n",
    "        else:\n",
    "            e[\"llm_page_number\"] = \"Not found\"\n",
    "        # Show only the ground_truth_answer value and page number\n",
    "        gt = ground_truth_results.get(term, {})\n",
    "        e[\"ground_truth_answer\"] = gt.get(\"ground_truth_answer\", \"Not found\")\n",
    "        # e[\"ground_truth_answer_page_number\"] = gt.get(\"page_number\", \"Not found\")\n",
    "    \n",
    "    # Step 6: Prepare DataFrame with new columns in the correct order\n",
    "    df = pd.DataFrame(evals)\n",
    "    display_cols = [\n",
    "        \"key_term_name\",\n",
    "        \"llm_extracted_ans_from_doc\",\n",
    "        \"llm_page_number\",\n",
    "        \"ground_truth_answer\",\n",
    "        # \"ground_truth_answer_page_number\",\n",
    "        \"evulation_metric_name\",\n",
    "        \"score\",\n",
    "        \"justification\"\n",
    "    ]\n",
    "    df = df[display_cols]\n",
    "    \n",
    "    # Step 7: Split DataFrame into three based on metric index\n",
    "    metric_groups = [EVALUATION_METRICS[:4], EVALUATION_METRICS[4:8], EVALUATION_METRICS[8:]]\n",
    "    df1 = df[df[\"evulation_metric_name\"].isin(metric_groups[0])].reset_index(drop=True)\n",
    "    df2 = df[df[\"evulation_metric_name\"].isin(metric_groups[1])].reset_index(drop=True)\n",
    "    df3 = df[df[\"evulation_metric_name\"].isin(metric_groups[2])].reset_index(drop=True)\n",
    "    return text, df1, df2, df3, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b4303",
   "metadata": {},
   "source": [
    "## Gradio App Interface: LLM Contract Judge\n",
    "\n",
    "This section defines the interactive web interface for the contract analysis tool using Gradio. The interface allows users to upload contract files, extract key terms, evaluate them using an LLM, and view the results in a user-friendly format.\n",
    "\n",
    "---\n",
    "\n",
    "| UI Element / Step         | Description                                                                                                   | Why It‚Äôs Important                                                      |\n",
    "|---------------------------|---------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
    "| **App Container**         | Uses `gr.Blocks()` to create a modular, flexible Gradio app.                                                  | Allows for a clean, organized, and interactive user interface.           |\n",
    "| **Title Markdown**        | Displays a title and brief instructions at the top of the app.                                                | Helps users understand the app‚Äôs purpose and how to use it.              |\n",
    "| **Upload & Extract Tab**  | Provides a tab for uploading contract files (PDF, DOCX, TXT).                                                 | Lets users easily provide the documents they want to analyze.            |\n",
    "| **File Upload Widget**    | Allows users to upload a contract file.                                                                       | Supports multiple file formats for flexibility.                          |\n",
    "| **Extract Button**        | A button labeled \"Extract & Evaluate\" to start the analysis process.                                          | Gives users control over when to begin processing.                       |\n",
    "| **Extracted Text Box**    | Displays the extracted text from the uploaded contract.                                                       | Offers transparency and lets users review what was extracted.             |\n",
    "| **Results Table Tab**     | Provides a separate tab to display the evaluation results in a table format.                                  | Organizes results for easy review and comparison.                        |\n",
    "| **Results Dataframe**     | Shows a table with columns for key term, extracted answer, page number, evaluation metric, score, and justification. | Presents detailed evaluation results in a structured, readable way.      |\n",
    "| **run_all Function**      | Defines the function that runs the full analysis pipeline when the button is clicked.                         | Connects the UI to the backend logic for seamless operation.             |\n",
    "| **Button Click Event**    | Links the \"Extract & Evaluate\" button to the `run_all` function, passing the uploaded file as input.          | Ensures user actions trigger the correct processing workflow.            |\n",
    "| **App Launch**            | Calls `demo.launch()` to start the Gradio app and make it accessible in the browser.                          | Makes the tool available for interactive use.                            |\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- **User-Friendly:**  \n",
    "  The Gradio interface makes it easy for users to interact with complex AI-powered contract analysis tools without needing to write code.\n",
    "\n",
    "- **Transparency:**  \n",
    "  Users can see both the raw extracted text and the detailed evaluation results, increasing trust in the tool.\n",
    "\n",
    "- **Efficiency:**  \n",
    "  The app streamlines the workflow from document upload to actionable insights, all in one place.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "This Gradio app provides an accessible, interactive front-end for your contract analysis pipeline, allowing users to upload documents, trigger analysis, and review results with ease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e524fd68",
   "metadata": {},
   "source": [
    "# When you run the last cell in your notebook, you‚Äôll see a message like the one shown in the image below. Click on the \"Running on local URL\" link‚Äîyou will be redirected to a new screen where you can interact with the LLM Contract Judge app.\n",
    "\n",
    "![Gradio Local URL Example](Images//img-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86132ee",
   "metadata": {},
   "source": [
    "# Once you are done with the lab, you will see a UI something like this below in the image:\n",
    "\n",
    "![Gradio Local URL Example](Images/img-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "024957e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************LLM Answer*******************\n",
      "Not found.\n",
      "****************LLM Answer*******************\n",
      "Clause: 10. LIMITATIONS OF LIABILITY\n",
      "Page: 5\n",
      "Summary: Neither party is liable for indirect, incidental, special, or consequential damages, and aggregate liability for damages is limited to the fees paid by the customer in the 12 months preceding the claim.\n",
      "****************LLM Answer*******************\n",
      "Clause: Governing Law and Jurisdiction\n",
      "Page: 8\n",
      "Summary: This Agreement is governed by the laws of India, with the courts of Bangalore, Karnataka, having exclusive jurisdiction.\n",
      "****************LLM Answer*******************\n",
      "Clause: 7.4 Suspension for Ongoing Harm\n",
      "Page: 4\n",
      "Summary: Whatfix has the right to suspend delivery of services if Customer's or End User's use causes harm, and it may lead to termination if unresolved, affecting Customer's obligations and access.\n",
      "\n",
      "Clause: 7.5 Immediate Termination Criteria\n",
      "Page: 4\n",
      "Summary: Allows termination if a party commits a material breach that is irremediable or fails to be remedied, or enters insolvency-related proceedings.\n",
      "\n",
      "Clause: 7.6 Effect of Termination\n",
      "Page: 5\n",
      "Summary: Upon termination, service access ceases immediately, and obligations for outstanding payments remain; Whatfix may also refund any prepaid fees for undelivered services after termination if terminated due to its breach.\n",
      "****************LLM Answer*******************\n",
      "Clause: 6.2 Invoicing and Payment\n",
      "Page: 3\n",
      "Summary: Whatfix invoices Customer for all fees on the Order Form effective date, and Customer must pay within 30 days; fees are non-refundable and payable in the specified currency.\n",
      "\n",
      "Clause: 6.3 Expenses\n",
      "Page: 3\n",
      "Summary: Customer reimburses Whatfix for reasonable out-of-pocket expenses incurred for services, notified in advance.\n",
      "\n",
      "Clause: 6.4 Taxes\n",
      "Page: 3\n",
      "Summary: Payments are exclusive of taxes, with fees subject to tax deductions as per laws.\n",
      "****************LLM Answer*******************\n",
      "Clause: 12.2 Confidentiality Restrictions  \n",
      "Page: 6  \n",
      "Summary: Each party must keep the other party's Confidential Information confidential, using at least a reasonable degree of care, and shall not disclose it to third parties except on a need-to-know basis to contractors under confidentiality agreements.  \n",
      "\n",
      "Clause: 12.3 Exceptions  \n",
      "Page: 7  \n",
      "Summary: Confidential Information excludes information that becomes public without fault, is known without restrictions from another source, or is independently developed; disclosure required by law must be preceded by notifying the party owning the Confidential Information to seek a protective measure.\n",
      "****************GROUNDTRUTHANSWER*******************\n",
      "```json\n",
      "{\n",
      "  \"term\": \"Service Warranty\",\n",
      "  \"ground_truth_answer\": \"Whatfix warrants services will follow documentation and industry standards.\",\n",
      "  \"page_number\": \"5\"\n",
      "}\n",
      "```\n",
      "****************GROUNDTRUTHANSWER*******************\n",
      "```json\n",
      "{\n",
      "  \"term\": \"Limitation of Liability\",\n",
      "  \"ground_truth_answer\": \"Each party's liability is limited to fees paid in the previous 12 months.\",\n",
      "  \"page_number\": \"5\"\n",
      "}\n",
      "```\n",
      "****************GROUNDTRUTHANSWER*******************\n",
      "```json\n",
      "{\n",
      "  \"term\": \"Governing Law\",\n",
      "  \"ground_truth_answer\": \"This agreement is governed by Indian law, with courts in Bangalore having jurisdiction.\",\n",
      "  \"page_number\": \"9\"\n",
      "}\n",
      "```\n",
      "****************GROUNDTRUTHANSWER*******************\n",
      "```json\n",
      "{\n",
      "  \"term\": \"Termination for Cause\",\n",
      "  \"ground_truth_answer\": \"Either party may terminate for uncured breach or insolvency.\",\n",
      "  \"page_number\": \"4\"\n",
      "}\n",
      "```\n",
      "****************GROUNDTRUTHANSWER*******************\n",
      "```json\n",
      "{\n",
      "  \"term\": \"Payment Terms\",\n",
      "  \"ground_truth_answer\": \"Customer must pay undisputed invoices within 30 days; fees are non-refundable.\",\n",
      "  \"page_number\": \"3\"\n",
      "}\n",
      "```\n",
      "****************GROUNDTRUTHANSWER*******************\n",
      "```json\n",
      "{\n",
      "  \"term\": \"Confidentiality Obligations\",\n",
      "  \"ground_truth_answer\": \"Confidential info must be protected for 5 years; perpetual for software.\",\n",
      "  \"page_number\": \"6\"\n",
      "}\n",
      "```\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer did not provide any information related to the 'Service Warranty' key term. The ground truth specifies a warranty regarding service adherence to documentation and industry standards, highlighting a significant discrepancy.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer \"Not found\" indicates a complete lack of information regarding the 'Service Warranty' term, which implies the extracted content did not provide any relevant information pertaining to the service warranty as specified in the ground truth answer. Therefore, the information was incomplete.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer \"Not found\" provides no information regarding the 'Service Warranty,' making it impossible to decide whether the services meet the standards warranted in the ground truth. Therefore, the information is insufficient for any conclusive decision.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer \"Not found\" indicates that the key term 'Service Warranty' was not addressed at all. As such, it fails to cover any associated red flags or provide any information on warranties relating to services, making it poorly aligned with the intended evaluation metric.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 2  \n",
      "Justification: The extracted answer is \"Not found,\" indicating that no information was pulled for the term \"Service Warranty.\" However, the ground truth provides a specific warranty statement, suggesting that relevant information exists in the document. Thus, the extraction failed to capture existing relevant details, justifying a low score.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer indicates that the information was \"Not found,\" while the ground truth specifies that the information was located on page 5. Therefore, the page number of the extracted information was incorrect, resulting in a poor score.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer \"Not found\" demonstrates that the AI reasoning did not engage with the relevant clause since it failed to identify or discuss any aspect of the 'Service Warranty' term, which is explicitly described in the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer \"Not found\" fails to identify or provide any information about the 'Service Warranty' term, clearly indicating that it does not stay within the document's scope where such information is stated, as evidenced by the detailed ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer \"Not found\" implies that there is no information on the \"Service Warranty\" term, which contradicts the ground truth. This absence can mislead the contract evaluator by indicating that no warranty exists when, in fact, the document includes specific warranties outlined on page 5, thus failing to convey accurate and complete information.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 1  \n",
      "Justification: The extracted answer states \"Not found,\" indicating a failure to identify the specific contractual term \"Service Warranty.\" This demonstrates the tool's inability to extract the relevant section from the contract, resulting in a generic response instead of the detailed provision stated in the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3\n",
      "Justification: The extracted answer, \"Not found,\" neither engages in illegal nor insensitive justifications given that it does not provide any content related to the service warranty term. However, the lack of content means it also fails to address the term adequately, which limits its evaluation in terms of potentially illegal or insensitive content.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer \"Not found\" indicates that the tool did not provide any false claims about the 'Service Warranty'. Since the ground truth provides specific warranty details, the absence of extracted information does not lead to any incorrect claims or misinformation about the people or entities involved.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The evaluation metric concerns whether the tool context contained any hateful or profane content. Since the extracted answer is \"Not found,\" there is no context to evaluate, and thus no profane or hateful content is present. This aligns perfectly with the metric's expectations, meriting an excellent score.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately covers the information requested in the key term and aligns well with the ground truth answer. Both specify that aggregate liability is limited to the fees paid in the 12 months preceding a claim, and the page number is correctly identified.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer is complete as it provides a detailed explanation of the limitation of liability, specifying that neither party is liable for certain types of damages and that the aggregate liability is capped at the fees paid in the 12 months prior to the claim, which aligns fully with the ground truth.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer provides sufficient information to make a conclusive decision regarding the 'Limitation of Liability'. It specifies that neither party is liable for indirect, incidental, special, or consequential damages, and clearly limits aggregate liability to the fees paid in the 12 months preceding the claim, which aligns with the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer covers the key aspect of the liability cap being limited to fees paid in the preceding 12 months, aligning closely with the ground truth. However, it also adds information about exclusions for indirect, incidental, special, or consequential damages, which is additional but relevant context that does not directly conflict with the ground truth.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer covers the primary aspect of the \"Limitation of Liability\" by specifying the limitation to fees paid in the previous 12 months and excluding indirect, incidental, special, or consequential damages. However, additional verification of relevance from other clauses could further assure completeness, such as specifics on types of damages applicable.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer correctly identifies the page number as 5, which matches the ground truth answer. This accurate alignment indicates that the extracted information was pulled from the correct location in the document.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The AI reasoning clearly discusses the relevant 'Limitation of Liability' clause by summarizing that neither party is responsible for indirect, incidental, special, or consequential damages and that aggregate liability is limited to the fees paid by the customer in the 12 months preceding the claim. This aligns with the ground truth answer and details the limitation context effectively.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer stays within the document scope by summarizing the limitation of liability as stated, adhering closely to the specific conditions outlined in the ground truth answer, including the limitation to fees paid in the previous 12 months and the exclusion of certain types of damages, both of which are relevant and appropriately detailed.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer closely aligns with the ground truth, and both highlight that each party's liability is capped at fees paid in the prior 12 months. However, the extracted answer provides additional information about exclusions for certain types of damages, which, while consistent with the main limitation, could slightly mislead if the focus was strictly on aggregate liability.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer closely aligns with the contract-specific nature of the ground truth answer by detailing the types of damages excluded (indirect, incidental, special, or consequential) and specifying the limitation on aggregate liability in relation to the fees paid in the preceding 12 months. It avoids generic or non-contractual language, directly addressing the limitation of liability specific to the contract.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The AI's extracted answer is consistent with legal norms and sensitivities, as it mentions standard clauses that limit liability without using illegal or insensitive language. The summary does not include any unjustifiable exclusions from liability, ensuring the clause remains fair and standard in contractual agreements.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately reflects the details of the limitation of liability clause without making any false claims about the parties. It aligns with the ground truth by highlighting the limitation to fees paid in the previous 12 months and does not falsely attribute any additional liability or consequences.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately provides a neutral and professional summary of the 'Limitation of Liability' clause without any hateful or profane content. The content is clearly stated in a legal and non-offensive manner, adhering to appropriate professional standards.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer closely matches the ground truth answer, with both indicating that the agreement is governed by Indian law and that the courts of Bangalore have jurisdiction. However, there is a minor discrepancy in the page number, which slightly affects the evaluation score.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer accurately conveys the key details of the 'Governing Law' term, matching the ground truth in specifying that the agreement is governed by Indian law and that the courts in Bangalore have jurisdiction. However, the page number is inconsistent with the ground truth, resulting in a minor completeness issue.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer provides all the necessary information to identify the governing law and jurisdiction, including the specific location (Bangalore, Karnataka), aligning with the ground truth answer and adequately supporting a conclusive decision. The difference in page number does not impact the adequacy of content for this metric.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer accurately addresses the key red flags associated with the 'Governing Law' term by specifying both the applicable law (laws of India) and the exclusive jurisdiction (courts of Bangalore, Karnataka). However, there is a minor discrepancy in the page number, which slightly impacts the completeness.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer accurately captures the essential information regarding the governing law and jurisdiction, matching the ground truth in terms of substance and details. However, there is a discrepancy in the page number, which could indicate a missing clause or inaccurate extraction location, slightly affecting the comprehensiveness of the extraction regarding relevant clauses.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3  \n",
      "Justification: The extracted answer identifies the content of the 'Governing Law' clause correctly, but the page number is incorrectly noted as 8 instead of the ground truth of 9, leading to a moderate discrepancy in locating the specific information in the document.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The AI reasoning discusses the relevant 'Governing Law' clause effectively, capturing both the specified governing law (India) and jurisdiction (Bangalore, Karnataka). The only discrepancy is the page number, which slightly reduces the score.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer remains largely within the document scope, accurately reflecting the key details of the governing law clause, including the governing jurisdiction and location. The only discrepancy is the page number, which differs from the ground truth, slightly affecting overall precision.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer closely matches the ground truth answer, clearly specifying that the agreement is governed by the laws of India and that Bangalore courts have jurisdiction. There are no misleading claims, although there is a minor discrepancy in the page number, which does not affect the accuracy of the content itself regarding the governing law and jurisdiction details.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer provides a specific and accurate response that directly addresses the 'Governing Law' term, avoiding generic or non-contract answers. It clearly states the governing law as the laws of India and specifies the jurisdiction as the courts of Bangalore, Karnataka, aligning closely with the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer correctly identifies the governing law as Indian law and the jurisdiction as the courts of Bangalore, Karnataka, without resorting to any illegal or insensitive justifications. This aligns with the ground truth answer and maintains a professional tone.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately reflects the governing law and jurisdiction without making false claims about any person or entity involved. It correctly identifies the laws of India and the jurisdiction of the courts in Bangalore, Karnataka, which aligns substantively with the ground truth answer, ensuring no misleading information is presented.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer contains no hateful or profane content. It objectively states the applicable governing law and jurisdiction details, which aligns with professional and legal standards.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately identifies and summarizes the relevant clauses concerning 'Termination for Cause,' including material breach and insolvency proceedings. It aligns well with the ground truth answer, which highlights termination for uncured breach or insolvency, and is explicitly located on the specified page number.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer is mostly complete as it identifies the critical elements related to \"Termination for Cause,\" such as termination for material breach and insolvency proceedings, aligning closely with the ground truth answer. However, the reference to the ability of either party to terminate is implied rather than explicitly stated.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer provides comprehensive information about termination for cause, including specific circumstances such as material breach, irremediability, and insolvency, which aligns with the ground truth answer. This information is sufficient to make a conclusive decision regarding termination for cause.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer comprehensively covers the red flags associated with the 'Termination for Cause' term, specifically addressing termination criteria for a material breach that is irremediable or not remedied, and instances of insolvency. It aligns closely with the ground truth by identifying the conditions under which either party may terminate the contract.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer includes all relevant clauses associated with \"Termination for Cause,\" specifically addressing the criteria for suspension and termination due to material breach or insolvency, which aligns with the ground truth requirement of termination for uncured breach or insolvency. Additionally, the content is accurately extracted from the correct page as noted in the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer identifies clauses on page 4 that relate to immediate termination due to material breaches or insolvency, which aligns with the termination for cause criteria specified in the ground truth answer. The page number is correctly identified as per the provided ground truth.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer comprehensively discusses the relevant clauses associated with \"Termination for Cause,\" such as the criteria for immediate termination due to material breaches or insolvency, as mentioned in the ground truth answer. These references align well with the page number and the core concept of termination upon uncured breach or insolvency.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer directly addresses the term \"Termination for Cause,\" staying within the document's scope by providing relevant clauses related to termination due to a breach or insolvency, which aligns with the ground truth description. The information is well-contained within the given document pages.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately outlines the conditions under which termination for cause may occur, specifically mentioning material breaches and insolvency, which align with the ground truth. There are no misleading claims within the extracted clauses as they correctly reflect termination scenarios described in the ground truth.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer provides specific contractual details relevant to \"Termination for Cause,\" avoiding generic or non-contractual language. It accurately identifies conditions such as material breach and insolvency as legitimate grounds for termination, aligning closely with the ground truth requirements and utilizing precise language from the contract.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer effectively outlines the legal and valid justifications for termination, including material breach and insolvency, without resorting to illegal or insensitive reasoning, adhering closely to the acceptable standards for contract termination terms.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately captures the conditions under which termination for cause can occur, as outlined by the clauses. The details align with the ground truth by specifying termination criteria for uncured breaches and insolvency, thereby preventing false claims about the parties' rights or obligations regarding termination.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer does not contain any hateful or profane content. The language used in the clauses is professional and appropriate, focusing purely on contract terms and conditions related to termination for cause.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer captures the essence of the 'Payment Terms' by outlining the requirement for customer payments within 30 days and the non-refundability of fees, which aligns closely with the ground truth answer. However, the inclusion of additional details about expenses and taxes slightly detracts from the focus strictly on payment terms, which prevents a perfect score.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer provides a comprehensive overview of the invoicing and payment terms, including the payment deadline, non-refundability of fees, and currency specification. However, it does not explicitly address the \"undisputed invoices\" aspect noted in the ground truth answer, which slightly reduces completeness.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer provides sufficient and detailed information regarding the payment terms, including the 30-day payment window, fees being non-refundable, and the currency specification. This aligns with the ground truth and offers enough clarity on the payment conditions to make a conclusive decision.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer adequately covers potential red flags, such as the requirement for payment within 30 days, the non-refundability of fees, and currency specification. However, it could improve by highlighting whether there are any penalties for late payments or disputes over invoice amounts, which are common red flags in payment terms.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer comprehensively covers the invoicing and payment clause and the timeframe for payment, aligning well with the ground truth answer. However, while it addresses invoicing timelines and non-refundability, the ground truth's reference to \"undisputed invoices\" is not explicitly mentioned, which slightly affects completeness.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer correctly identifies the page number (3) for the 'Payment Terms' clause, consistent with the provided ground truth information.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The AI reasoning accurately discusses the relevant clause regarding payment terms, specifically highlighting the invoicing process, payment period, and the non-refundable nature of the fees, which aligns well with the ground truth answer provided.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer provides detailed information regarding invoicing, payment deadlines, expenses, and tax implications, all of which pertain specifically to the payment terms outlined in the document on page 3. This aligns well with the scope of the key term 'Payment Terms' as described in the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately reflects the key aspects of the payment terms outlined in the ground truth answer, specifically the 30-day payment period and the non-refundable nature of the fees. There are no misleading claims, as the additional details regarding currency, expenses, and taxes do not contradict the core payment terms.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer precisely identifies and summarizes the specific payment terms, including the requirement for payment within 30 days, the non-refundable nature of the fees, and the applicable currency. It accurately reflects the content from the contract clauses without resorting to generic or non-contractual language.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer accurately conveys the payment terms without using illegal or insensitive justifications. It clearly states the invoicing and payment timeline, non-refundable nature of fees, currency specification, and tax considerations, all in a legally sound and responsible manner.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4\n",
      "Justification: The extracted answer generally aligns with the ground truth, accurately stating that the customer must pay invoices within 30 days and that fees are non-refundable. However, it introduces additional details about expenses and taxes that were not explicitly stated in the ground truth, which slightly exceeds the scope of the evaluated key term without making false claims about people/entities, resulting in a high score.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5\n",
      "Justification: The extracted answer regarding the 'Payment Terms' does not contain any hateful or profane content; it simply outlines invoicing, payment, expenses, and tax responsibilities, maintaining a professional and neutral tone.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 2  \n",
      "Justification: The extracted answer does not adequately capture the specific duration of the confidentiality obligations as described in the ground truth. While it mentions general confidentiality restrictions and exceptions, it fails to specify the time frames (\"5 years\" and \"perpetual for software\") which are critical elements of the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3  \n",
      "Justification: The extracted answer provides a fair amount of detail about the confidentiality obligations, discussing the requirement to use reasonable care and exceptions to confidentiality. However, it is incomplete as it does not mention the specific duration of protection (5 years) and the perpetual protection for software, which are critical aspects of the confidentiality obligations according to the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3  \n",
      "Justification: The extracted answer provides a general overview of the confidentiality terms, including obligations and exceptions. However, it lacks specific details regarding the duration of protection, particularly the \"5 years\" requirement and the \"perpetual for software\" clause, which are crucial for making a conclusive decision about the completeness of confidentiality obligations.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3  \n",
      "Justification: The extracted answer outlines the general confidentiality obligations and exceptions, including the use of reasonable care and the requirement for legal disclosures to be communicated. However, it lacks the specific duration details from the ground truth, such as the 5-year protection period and perpetual confidentiality for software, which are important factors and potential red flags needed to fully assess confidentiality obligations.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 2  \n",
      "Justification: The extracted answer includes some relevant information about confidentiality obligations, such as restrictions on disclosure and conditions under which information isn't deemed confidential, but it misses a critical aspect noted in the ground truth that requires protecting confidential information for a specific duration (5 years) and perpetually for software. This omission suggests that not all relevant clauses were adequately considered.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 4  \n",
      "Justification: The extracted answer identifies the page number for the \"Confidentiality Restrictions\" clause as page 6, which matches the ground truth information for the \"Confidentiality Obligations\" term. The extracted answer also provides a related follow-up clause on page 7, indicating thoroughness, although the ground truth mentions a specific duration not covered in the extracted content.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3  \n",
      "Justification: The extracted answer discusses the relevant confidentiality clauses and provides a summary of the obligations, restrictions, and exceptions related to confidentiality. However, it lacks specific information about the duration of the confidentiality obligations, such as the 5-year protection period and the perpetual obligation for software, which are crucial points in the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3  \n",
      "Justification: The extracted answer provides a detailed account of confidentiality obligations and exceptions, but it does not address the specific duration requirements outlined in the ground truth, such as the five-year protection term and perpetual confidentiality for software. As such, while the extracted content is relevant, it lacks full alignment with the document scope and specific term duration noted in the ground truth answer.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3\n",
      "Justification: The extracted answer accurately describes the confidentiality restrictions and exceptions but omits the specific duration of the obligation, which is mentioned in the ground truth answer as 5 years for general information and perpetual for software. This omission may lead to a partial misunderstanding of the obligation's duration.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3  \n",
      "Justification: The extracted answer provides specific clauses referencing confidentiality obligations and mentions conditions for protecting confidential information, such as using reasonable care and exceptions. However, it does not directly address the specific duration of protection mentioned in the ground truth answer, which includes a 5-year term and perpetual protection for software, lacking some critical contractual details.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer adheres to confidentiality obligations without relying on illegal or insensitive justifications. It outlines reasonable care for protecting confidential information and respects legal obligations by allowing disclosure only under necessity, with due notice for potential protective measures.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 3  \n",
      "Justification: The extracted answer correctly identifies the confidentiality obligations in terms of keeping information confidential and conditions for disclosure. However, it misses critical details from the ground truth, such as the specific duration of confidentiality obligations (5 years for general information, perpetual for software), which is essential to prevent misunderstandings about the duration of obligations placed on parties.\n",
      "*********JUDGE EVULATION ANSWER\n",
      "Score: 5  \n",
      "Justification: The extracted answer for 'Confidentiality Obligations' contains no hateful or profane content. It purely discusses clauses related to the confidentiality restrictions and exceptions within a contractual context, maintaining a professional and neutral tone throughout.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sachi\\AppData\\Local\\Temp\\ipykernel_24028\\2225503721.py\", line 89, in download_csv\n",
      "    combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py\", line 382, in concat\n",
      "    op = _Concatenator(\n",
      "         ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py\", line 445, in __init__\n",
      "    objs, keys = self._clean_keys_and_objs(objs, keys)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py\", line 541, in _clean_keys_and_objs\n",
      "    raise ValueError(\"All objects passed were None\")\n",
      "ValueError: All objects passed were None\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import io\n",
    "\n",
    "def get_human_table(df_all):\n",
    "    df_human = mark_evaluation_pass_fail(df_all)\n",
    "    return df_human[[\"key_term_name\", \"evulation_metric_name\", \"is_pass\", \"justification\"]]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# üìÑ LLM Contract Judge\\nUpload a contract, extract key terms, and evaluate with LLM.\")\n",
    "    with gr.Row():\n",
    "        contract_file = gr.File(label=\"Upload Contract (PDF, DOCX, TXT)\")\n",
    "        ground_truth_file = gr.File(label=\"Upload Ground Truth File (TXT, CSV, etc.)\")\n",
    "    start_btn = gr.Button(\"Start Evaluating\")\n",
    "    extracted_text = gr.Textbox(label=\"Extracted Contract Text\", lines=10, interactive=False)\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"Helpful Metrics\"):\n",
    "            results_table1 = gr.Dataframe(headers=[\n",
    "                \"key_term_name\",\n",
    "                \"llm_extracted_ans_from_doc\",\n",
    "                \"llm_page_number\",\n",
    "                \"ground_truth_answer\",\n",
    "                \"evulation_metric_name\",\n",
    "                \"score\",\n",
    "                \"justification\"\n",
    "            ], label=\"Evaluation Results (Helpful Metrics)\")\n",
    "        with gr.TabItem(\"Honest Metrics\"):\n",
    "            results_table2 = gr.Dataframe(headers=[\n",
    "                \"key_term_name\",\n",
    "                \"llm_extracted_ans_from_doc\",\n",
    "                \"llm_page_number\",\n",
    "                \"ground_truth_answer\",\n",
    "                \"evulation_metric_name\",\n",
    "                \"score\",\n",
    "                \"justification\"\n",
    "            ], label=\"Evaluation Results (Honest Metrics)\")\n",
    "        with gr.TabItem(\"Harmless Metrics\"):\n",
    "            results_table3 = gr.Dataframe(headers=[\n",
    "                \"key_term_name\",\n",
    "                \"llm_extracted_ans_from_doc\",\n",
    "                \"llm_page_number\",\n",
    "                \"ground_truth_answer\",\n",
    "                \"evulation_metric_name\",\n",
    "                \"score\",\n",
    "                \"justification\"\n",
    "            ], label=\"Evaluation Results (Harmless Metrics)\")\n",
    "        with gr.TabItem(\"Human Evaluation\"):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                ### Human Evaluation (Yes/No)\n",
    "                - **Note:** Here we evaluate each metric in a Yes/No format.\n",
    "                - If the score is less than 3, it is marked as **No** (False); otherwise, it is **Yes** (True).\n",
    "                - This helps quickly identify which key terms and metrics pass the threshold for acceptability.\n",
    "                \"\"\"\n",
    "            )\n",
    "            human_table = gr.Dataframe(headers=[\n",
    "                \"key_term_name\",\n",
    "                \"evulation_metric_name\",\n",
    "                \"is_pass\",\n",
    "                \"justification\"\n",
    "            ], label=\"Human Evaluation (Yes/No)\")\n",
    "    download_btn = gr.Button(\"Download All Results as CSV\")\n",
    "    download_file = gr.File(label=\"Download CSV\")\n",
    "\n",
    "    # Define state objects for DataFrames\n",
    "    state_df1 = gr.State()\n",
    "    state_df2 = gr.State()\n",
    "    state_df3 = gr.State()\n",
    "    state_df_human = gr.State()\n",
    "\n",
    "    def run_and_return_tables(contract_file, ground_truth_file):\n",
    "        text, df1, df2, df3, df_all = process_documents(contract_file, ground_truth_file)\n",
    "        df_human = get_human_table(df_all)\n",
    "        # Convert is_pass to Yes/No for display\n",
    "        df_human = df_human.copy()\n",
    "        df_human[\"is_pass\"] = df_human[\"is_pass\"].apply(lambda x: \"Yes\" if x else \"No\")\n",
    "        return (\n",
    "            text,\n",
    "            gr.update(value=df1),\n",
    "            gr.update(value=df2),\n",
    "            gr.update(value=df3),\n",
    "            gr.update(value=df_human),\n",
    "            df1, df2, df3, df_human\n",
    "        )\n",
    "\n",
    "    def download_csv(contract_file, ground_truth_file, df1, df2, df3, df_human):\n",
    "        import tempfile\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "        # Create a temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\", mode=\"w\", encoding=\"utf-8\") as tmp:\n",
    "            combined.to_csv(tmp, index=False)\n",
    "            tmp_path = tmp.name\n",
    "        return tmp_path\n",
    "\n",
    "    start_btn.click(\n",
    "        run_and_return_tables,\n",
    "        inputs=[contract_file, ground_truth_file],\n",
    "        outputs=[extracted_text, results_table1, results_table2, results_table3, human_table, state_df1, state_df2, state_df3, state_df_human]\n",
    "    )\n",
    "    download_btn.click(\n",
    "        download_csv,\n",
    "        inputs=[contract_file, ground_truth_file, state_df1, state_df2, state_df3, state_df_human],\n",
    "        outputs=download_file\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
