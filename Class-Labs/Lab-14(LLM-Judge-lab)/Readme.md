# LLM Judge Lab : Evaluating Contracts with Language Models

## Introduction

This repository contains hands-on labs designed to explore the use of Large Language Models (LLMs) as automated judges for contract evaluation. Through practical exercises, you will learn how to leverage LLMs and advanced AI services to assess legal clauses, compare outputs, and build robust evaluation pipelines.

## Lab Overviews

### Lab 1: LLM as a Judge with Advanced Azure AI Evaluation

- **Objective:** Enhance contract evaluation by integrating Azure AIâ€™s advanced evaluation tools.


### Lab 2: LLM as a Judge with LLM

- **Objective:** Use a standard LLM to evaluate contract clauses against ground truth data.


### Lab 3: LLM vs. Human Judgment

- **Objective:** Compare the effectiveness of LLM-based evaluation with human evaluation to determine which is more reliable or insightful for contract clause assessment.
