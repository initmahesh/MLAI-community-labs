# LLM Judge Lab : Evaluating Contracts with Language Models

## Introduction

This repository contains hands-on labs designed to explore the use of Large Language Models (LLMs) as automated judges for contract evaluation. Through practical exercises, you will learn how to leverage LLMs and advanced AI services to assess legal clauses, compare outputs, and build robust evaluation pipelines.

## Lab Overviews

### Lab 1: LLM as a Judge with LLM

- **Objective:** Use a standard LLM to evaluate contract clauses against ground truth data.
- **Activities:**
  - Load and analyze contract clauses.
  - Use an LLM to judge the validity of each clause.
  - Compare LLM outputs with ground truth for accuracy.

### Lab 2: LLM as a Judge with Advanced Azure AI Evaluation

- **Objective:** Enhance contract evaluation by integrating Azure AI’s advanced evaluation tools.
- **Activities:**
  - Utilize Azure AI services for more sophisticated clause analysis.
  - Automate evaluation workflows.
  - Benchmark Azure AI’s performance against standard LLMs.
