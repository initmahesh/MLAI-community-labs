# LLM Judge Evaluation Lab

This repository contains a Jupyter notebook for evaluating LLM (Large Language Model) performance as a judge.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/initmahesh/MLAI-community-labs/blob/main/Class-Labs/Lab-LLM-AS-A-JUDGE/Azure_ai_judgelab/LLm_Judgle_Eval.ipynb)

## Description

This lab explores the use of Large Language Models (LLMs) as judges for evaluating various tasks and responses. The notebook contains experiments and evaluations to assess LLM performance in judgment tasks.

## Getting Started

Click the "Open in Colab" button above to open the notebook in Google Colab and start experimenting.
