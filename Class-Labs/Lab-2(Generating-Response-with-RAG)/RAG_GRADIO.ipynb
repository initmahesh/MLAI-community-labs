{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxBjBBZ95hPf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG Archiecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gdown\n",
        "from IPython.display import Image\n",
        "\n",
        "# URL format to download from Google Drive\n",
        "file_id = \"1v9eDyAFqlUv-7_SRdpVp9iOeNMEmBcxs\"\n",
        "url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "# Download the image\n",
        "gdown.download(url, \"image.png\", quiet=False)\n",
        "\n",
        "# Display the image\n",
        "Image(\"image.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YD8h_ygqDGLy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers faiss-cpu sentence-transformers langchain==0.0.354 pypdf openai==1.3.9 python-dotenv==1.0.0 PyMuPDF==1.24.2 tqdm tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM5WwtcJDk6e",
        "outputId": "8bccba88-c325-4138-9fa4-c21021b8c04a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.5.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.5)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.4.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.4.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart==0.0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.3)\n",
            "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.2)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nb6r3eX5s0m"
      },
      "source": [
        "## 1. Import Libraries and Load Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Q6AJm_5TDK_G"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import faiss\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import fitz\n",
        "import tiktoken\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "token_encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "knowledge_base = []\n",
        "embed_index = None\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN-21UNu53Ie"
      },
      "source": [
        "### 1. gradio: This library is used for building interactive web interfaces (UI). It helps us create a user-friendly way to interact with the RAG pipeline.\n",
        "\n",
        "### 2. faiss: Used for fast similarity search in vector spaces. We will use it to retrieve the most relevant text chunks based on a query.\n",
        "\n",
        "### 3. numpy: A fundamental package for numerical computing in Python, used to handle vectors for similarity search.\n",
        "\n",
        "### 4. openai: This is used to interact with OpenAI’s models, particularly for generating answers.\n",
        "\n",
        "### 5. sentence_transformers: Used for encoding text into vectors using pre-trained sentence embeddings.\n",
        "\n",
        "### 6. RecursiveCharacterTextSplitter: A helper for breaking down long text into smaller, manageable chunks (with adjustable size and overlap).\n",
        "\n",
        "### 7. fitz: The PyMuPDF library, used to extract text from PDF documents.\n",
        "tiktoken: Tokenizer for OpenAI models.\n",
        "\n",
        "### 8. load_dotenv: Loads environment variables from a .env file (for API keys and configuration)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt3BmDYK6Pel"
      },
      "source": [
        "## 2. Global Variables for Chunk Size and Overlap\n",
        "\n",
        "chunk_size and chunk_overlap: These variables define the size of each chunk and the amount of overlap between consecutive chunks when splitting the PDF text. Adjusting these will affect the granularity of the chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "P5XpZQzA6SBK"
      },
      "outputs": [],
      "source": [
        "# Global variables for chunk size and overlap\n",
        "chunk_size = 500\n",
        "chunk_overlap = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fwC2IX66dg7"
      },
      "source": [
        "## 3. Load PDF and Split into Chunks\n",
        "\n",
        "### load_pdf function:\n",
        "\n",
        "This function extracts text from each page of the uploaded PDF. It splits the text into chunks based on the chunk_size and chunk_overlap parameters. Each chunk is added to the knowledge_base.\n",
        "\n",
        "### RecursiveCharacterTextSplitter:\n",
        "This class is responsible for breaking down the large text into smaller, manageable pieces.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_aZ2mJyv6eqI"
      },
      "outputs": [],
      "source": [
        "def load_pdf(pdf_path, chunk_size, chunk_overlap):\n",
        "    global knowledge_base\n",
        "    knowledge_base = []\n",
        "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    file = fitz.open(pdf_path)\n",
        "    for page in file:\n",
        "        text = page.get_text()\n",
        "        chunks = recursive_splitter.split_text(text)\n",
        "        knowledge_base.extend(chunks)\n",
        "    return knowledge_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqCrvfAu60ft"
      },
      "source": [
        "## 4. Display Chunks\n",
        "\n",
        "### display_chunks function:\n",
        "\n",
        "Displays the first num_chunks from the knowledge_base. This is the text the model will use to answer the user's question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "znTHPjWn63i7"
      },
      "outputs": [],
      "source": [
        "# Display the first N chunks\n",
        "def display_chunks(num_chunks):\n",
        "    return \"\\n\\n\".join(knowledge_base[:num_chunks])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbfaIHUS7GSM"
      },
      "source": [
        "## 5. Create Embeddings for Chunks\n",
        "\n",
        "### create_embeddings:\n",
        "This function converts each chunk in the knowledge_base into a vector representation using the SentenceTransformer model. These vectors are then indexed using FAISS for efficient similarity search.\n",
        "\n",
        "### faiss.IndexFlatL2:\n",
        "FAISS’s index structure that allows fast retrieval of vectors based on similarity.\n",
        "\n",
        "### faiss.normalize_L2:\n",
        "Normalizes vectors for cosine similarity-based search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "AaQVJC7z7SKm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_embeddings():\n",
        "    global embed_index\n",
        "    vectors = encoder.encode(knowledge_base)\n",
        "    vector_dimension = vectors.shape[1]\n",
        "    embed_index = faiss.IndexFlatL2(vector_dimension)\n",
        "    faiss.normalize_L2(vectors)\n",
        "    embed_index.add(vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LDtOk_x7bfj"
      },
      "source": [
        "## 6. Search for Relevant Chunks\n",
        "\n",
        "### answer_question:\n",
        "\n",
        "This function takes the user's question, converts it into a vector, and performs a similarity search on the embed_index to retrieve the most relevant chunks from the knowledge base.\n",
        "\n",
        "### FAISS Search:\n",
        "\n",
        "The search method retrieves the top results_len most similar chunks from the knowledge base, which are then returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "eCp3Fk347kjH"
      },
      "outputs": [],
      "source": [
        "def answer_question(question, results_len):\n",
        "    global embed_index\n",
        "    if embed_index is None:\n",
        "        return {\"error\": \"Embeddings not created. Please upload a PDF and retrieve chunks first.\"}\n",
        "\n",
        "    answer = []\n",
        "    search_vector = encoder.encode(question)\n",
        "    _vector = np.array([search_vector])\n",
        "    faiss.normalize_L2(_vector)\n",
        "    distances, retrieved_idxs = embed_index.search(_vector, results_len)\n",
        "\n",
        "    for i in range(len(retrieved_idxs.ravel())):\n",
        "        answer.append(knowledge_base[retrieved_idxs.ravel()[i]])\n",
        "\n",
        "    return \"\\n\\n\".join(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZyPuq3Q7qUD"
      },
      "source": [
        "## 7. Generate Answer Using OpenAI\n",
        "\n",
        "### CallOpenAI:\n",
        "\n",
        "This function sends the retrieved chunks and the user's question to the OpenAI model to generate a response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jgtmMCyz8Ljw"
      },
      "outputs": [],
      "source": [
        "def CallOpenAI(user, system):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsYsVENe77m7"
      },
      "source": [
        "## 8. Gradio Functions for User Interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "OEbOiV8h75lw"
      },
      "outputs": [],
      "source": [
        "def retrieve_chunks(pdf_file, chunk_size, chunk_overlap, num_chunks):\n",
        "    load_pdf(pdf_file.name, chunk_size, chunk_overlap)\n",
        "    create_embeddings()\n",
        "    knowledge_base_text = display_chunks(num_chunks)\n",
        "    return knowledge_base_text, gr.update(visible=True), gr.update(visible=True)\n",
        "\n",
        "def update_chunks(chunk_size, chunk_overlap, num_chunks):\n",
        "    knowledge_base_text = display_chunks(num_chunks)\n",
        "    return knowledge_base_text\n",
        "\n",
        "def retrieve_relevant_chunks(question):\n",
        "    relevant_chunks = answer_question(question, 3)\n",
        "    return relevant_chunks, gr.update(visible=True)\n",
        "\n",
        "def get_llm_answer(question, relevant_chunks):\n",
        "    full_prompt = relevant_chunks + \"\\n\\n\" + question\n",
        "    answer = CallOpenAI(question, relevant_chunks)\n",
        "    return answer\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Enhanced RAG Pipeline Demonstration\")\n",
        "\n",
        "    pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
        "    chunk_size_slider = gr.Slider(minimum=100, maximum=1000, step=50, value=500, label=\"Adjust Chunk Size\")\n",
        "    chunk_overlap_slider = gr.Slider(minimum=0, maximum=500, step=10, value=50, label=\"Adjust Chunk Overlap\")\n",
        "    num_chunks_input = gr.Number(label=\"Number of Chunks to Display\", value=5)\n",
        "\n",
        "    retrieve_chunks_button = gr.Button(\"Retrieve Chunks\")\n",
        "    chunks_output = gr.Textbox(label=\"Retrieved Chunks\", interactive=False)\n",
        "\n",
        "    question_input = gr.Textbox(label=\"Ask a question:\", visible=False)\n",
        "    retrieve_relevant_button = gr.Button(\"Retrieve Relevant Chunks\")\n",
        "    relevant_chunks_output = gr.Textbox(label=\"Relevant Passages (RAG)\", interactive=False)\n",
        "\n",
        "    get_answer_button = gr.Button(\"Get Answer from LLM\", visible=False)\n",
        "    final_answer_output = gr.Textbox(label=\"Answer from OpenAI\", interactive=False)\n",
        "\n",
        "    retrieve_chunks_button.click(\n",
        "        retrieve_chunks,\n",
        "        inputs=[pdf_input, chunk_size_slider, chunk_overlap_slider, num_chunks_input],\n",
        "        outputs=[chunks_output, question_input, retrieve_relevant_button]\n",
        "    )\n",
        "\n",
        "    chunk_size_slider.change(\n",
        "        update_chunks,\n",
        "        inputs=[chunk_size_slider, chunk_overlap_slider, num_chunks_input],\n",
        "        outputs=chunks_output\n",
        "    )\n",
        "\n",
        "    chunk_overlap_slider.change(\n",
        "        update_chunks,\n",
        "        inputs=[chunk_size_slider, chunk_overlap_slider, num_chunks_input],\n",
        "        outputs=chunks_output\n",
        "    )\n",
        "\n",
        "    num_chunks_input.change(\n",
        "        update_chunks,\n",
        "        inputs=[chunk_size_slider, chunk_overlap_slider, num_chunks_input],\n",
        "        outputs=chunks_output\n",
        "    )\n",
        "\n",
        "    retrieve_relevant_button.click(\n",
        "        retrieve_relevant_chunks,\n",
        "        inputs=question_input,\n",
        "        outputs=[relevant_chunks_output, get_answer_button]\n",
        "    )\n",
        "\n",
        "    get_answer_button.click(\n",
        "        get_llm_answer,\n",
        "        inputs=[question_input, relevant_chunks_output],\n",
        "        outputs=final_answer_output\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGuVh8B26iGE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "UKf6o5aPDM_E",
        "outputId": "70cd69fa-9aa9-44be-8928-558344b1270f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e530b1def6e9320694.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://e530b1def6e9320694.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "UI Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import faiss\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import fitz\n",
        "import tiktoken\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "token_encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "knowledge_base = []\n",
        "embed_index = None\n",
        "client = OpenAI()\n",
        "\n",
        "# Global variables for chunk size and overlap\n",
        "chunk_size = 500\n",
        "chunk_overlap = 50\n",
        "\n",
        "# Load PDF and process it with adjustable chunk size and overlap\n",
        "def load_pdf(pdf_path, chunk_size, chunk_overlap):\n",
        "    global knowledge_base\n",
        "    knowledge_base = []\n",
        "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    file = fitz.open(pdf_path)\n",
        "    for page in file:\n",
        "        text = page.get_text()\n",
        "        chunks = recursive_splitter.split_text(text)\n",
        "        knowledge_base.extend(chunks)\n",
        "    return knowledge_base\n",
        "\n",
        "# Display the first N chunks\n",
        "def display_chunks(num_chunks):\n",
        "    return \"\\n\\n\".join(knowledge_base[:num_chunks])\n",
        "\n",
        "# Create embeddings\n",
        "def create_embeddings():\n",
        "    global embed_index\n",
        "    vectors = encoder.encode(knowledge_base)\n",
        "    vector_dimension = vectors.shape[1]\n",
        "    embed_index = faiss.IndexFlatL2(vector_dimension)\n",
        "    faiss.normalize_L2(vectors)\n",
        "    embed_index.add(vectors)\n",
        "\n",
        "# Search for the answer\n",
        "def answer_question(question, results_len):\n",
        "    global embed_index\n",
        "    if embed_index is None:\n",
        "        return {\"error\": \"Embeddings not created. Please upload a PDF and retrieve chunks first.\"}\n",
        "    \n",
        "    answer = []\n",
        "    search_vector = encoder.encode(question)\n",
        "    _vector = np.array([search_vector])\n",
        "    faiss.normalize_L2(_vector)\n",
        "    distances, retrieved_idxs = embed_index.search(_vector, results_len)\n",
        "\n",
        "    for i in range(len(retrieved_idxs.ravel())):\n",
        "        answer.append(knowledge_base[retrieved_idxs.ravel()[i]])\n",
        "\n",
        "    return \"\\n\\n\".join(answer)\n",
        "\n",
        "# OpenAI API call\n",
        "def CallOpenAI(user, system):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Gradio functions\n",
        "def retrieve_chunks(pdf_file, chunk_size, chunk_overlap, num_chunks):\n",
        "    load_pdf(pdf_file.name, chunk_size, chunk_overlap)\n",
        "    create_embeddings()\n",
        "    knowledge_base_text = display_chunks(num_chunks)\n",
        "    return knowledge_base_text, gr.update(visible=True), gr.update(visible=True)\n",
        "\n",
        "def update_chunks(chunk_size, chunk_overlap, num_chunks):\n",
        "    knowledge_base_text = display_chunks(num_chunks)\n",
        "    return knowledge_base_text\n",
        "\n",
        "def retrieve_relevant_chunks(question, results_len):\n",
        "    relevant_chunks = answer_question(question, results_len)\n",
        "    return relevant_chunks, gr.update(visible=True)\n",
        "\n",
        "def get_llm_answer(question, relevant_chunks):\n",
        "    full_prompt = relevant_chunks + \"\\n\\n\" + question\n",
        "    answer = CallOpenAI(question, relevant_chunks)\n",
        "    return answer\n",
        "\n",
        "# Gradio Interface with Separate Sections\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Enhanced RAG Pipeline Demonstration\")\n",
        "\n",
        "    # Section 1: PDF Upload and Chunk Retrieval\n",
        "    with gr.Tab(\"Upload and Retrieve Chunks\"):\n",
        "        pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
        "        chunk_size_slider = gr.Slider(minimum=100, maximum=1000, step=50, value=500, label=\"Adjust Chunk Size\")\n",
        "        chunk_overlap_slider = gr.Slider(minimum=0, maximum=500, step=10, value=50, label=\"Adjust Chunk Overlap\")\n",
        "        num_chunks_input = gr.Number(label=\"Number of Chunks to Display\", value=5)\n",
        "        retrieve_chunks_button = gr.Button(\"Retrieve Chunks\")\n",
        "        chunks_output = gr.Textbox(label=\"Retrieved Chunks\", interactive=False)\n",
        "\n",
        "    # Section 2: Question Input and Relevant Chunks Retrieval\n",
        "    with gr.Tab(\"Ask a Question\"):\n",
        "        question_input = gr.Textbox(label=\"Enter your question:\")\n",
        "        num_relevant_chunks_input = gr.Slider(minimum=1, maximum=10, step=1, value=3, label=\"Number of Relevant Chunks\")\n",
        "        retrieve_relevant_button = gr.Button(\"Retrieve Relevant Chunks\")\n",
        "        relevant_chunks_output = gr.Textbox(label=\"Relevant Passages (RAG)\", interactive=False)\n",
        "\n",
        "    # Section 3: Get Answer from LLM\n",
        "    with gr.Tab(\"Get Answer\"):\n",
        "        get_answer_button = gr.Button(\"Get Answer from LLM\")\n",
        "        final_answer_output = gr.Textbox(label=\"Answer from OpenAI\", interactive=False)\n",
        "\n",
        "    # Step 1: Retrieve chunks with adjustable chunk size and overlap\n",
        "    retrieve_chunks_button.click(\n",
        "        retrieve_chunks,\n",
        "        inputs=[pdf_input, chunk_size_slider, chunk_overlap_slider, num_chunks_input],\n",
        "        outputs=[chunks_output, question_input, retrieve_relevant_button]\n",
        "    )\n",
        "\n",
        "    # Step 2: Update displayed chunks based on user input\n",
        "    chunk_size_slider.change(\n",
        "        update_chunks,\n",
        "        inputs=[chunk_size_slider, chunk_overlap_slider, num_chunks_input],\n",
        "        outputs=chunks_output\n",
        "    )\n",
        "\n",
        "    chunk_overlap_slider.change(\n",
        "        update_chunks,\n",
        "        inputs=[chunk_size_slider, chunk_overlap_slider, num_chunks_input],\n",
        "        outputs=chunks_output\n",
        "    )\n",
        "\n",
        "    num_chunks_input.change(\n",
        "        update_chunks,\n",
        "        inputs=[chunk_size_slider, chunk_overlap_slider, num_chunks_input],\n",
        "        outputs=chunks_output\n",
        "    )\n",
        "\n",
        "    # Step 3: Retrieve relevant chunks\n",
        "    retrieve_relevant_button.click(\n",
        "        retrieve_relevant_chunks,\n",
        "        inputs=[question_input, num_relevant_chunks_input],\n",
        "        outputs=[relevant_chunks_output, get_answer_button]\n",
        "    )\n",
        "\n",
        "    # Step 4: Get answer from LLM\n",
        "    get_answer_button.click(\n",
        "        get_llm_answer,\n",
        "        inputs=[question_input, relevant_chunks_output],\n",
        "        outputs=final_answer_output\n",
        "    )\n",
        "\n",
        "# Launch the Gradio app\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
