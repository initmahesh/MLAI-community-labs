# AI Evaluation Course Labs - Introduction

## Overview

This repository contains a curated set of hands-on labs focused on building and evaluating contract analysis systems using Azure AI and Large Language Models (LLMs). Each lab is designed to help you progressively develop skills in contract clause evaluation, LLM-based judgment, continuous monitoring, and comparing human vs. machine insights.

## Lab Overviews

### Lab 1: LLM as Judge

* **Objective:** Use a standard LLM to evaluate contract clauses against ground truth data.

### Lab 2: Vibe Coding Your First Agent Using Cursar

* **Objective:** * Build your first AI agent using the Cursar framework in a hands-on environment.

### Lab 3: Building AI Evaluators

* **Objective:** Construct evaluation systems using LLMs to assess the quality, accuracy, and performance of contract analysis.

### Lab 4: Continuous Monitoring, Alerting and Monitoring Your AI Agents

* **Objective:** Learn how to continuously monitor and alert on performance, behavior, and anomalies in deployed LLM agents using Azure AI Studio or similar tools.

### Lab 5: Evaluating LLM vs Human Judgement â€“ Judging the Judge

* **Objective:** Compare the effectiveness of LLM-based evaluation with human evaluation to determine which is more reliable or insightful for contract clause assessment.




