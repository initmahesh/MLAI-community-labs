# Lab 1: LLM-as-a-Judge - A Complete Guide to Run LLM Evals

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sachin0034/MLAI-community-labs/blob/main/AI-evaluation-course-labs/Lab-1(LLM_as_judge)/LLM_As_A_Judge.ipynb)
---

## üß† Overview

**LLM-as-a-Judge** is a modern evaluation technique where a large language model (LLM) such as GPT is used to **judge or assess the quality** of outputs generated by other AI systems. This lab will guide you step-by-step on how to set up and use LLMs as evaluators.

This approach is scalable, cost-effective, and especially useful when human evaluation isn't feasible.

---

## üìò What You Will Learn

In this lab, you will understand how a Large Language Model (LLM) can **act as a judge** to evaluate the quality of AI-generated outputs. Specifically, you will learn:

- ‚úÖ How to define **key terms** and **evaluation metrics** for a specific task (e.g., contract analysis)
- ‚úÖ How to **upload a contract document** and extract relevant information using an LLM
- ‚úÖ How the LLM uses the extracted information to **evaluate responses** based on the predefined metrics

By the end of this lab, you‚Äôll have a working understanding of how to build an LLM-as-a-judge pipeline using real-world documents like contracts.

---

## üñºÔ∏è Output Preview

Here's a preview of the LLM-as-a-Judge interface and evaluation output you will build in this lab:

![LLM-as-a-Judge Output Screenshot](images/img-1.png)
