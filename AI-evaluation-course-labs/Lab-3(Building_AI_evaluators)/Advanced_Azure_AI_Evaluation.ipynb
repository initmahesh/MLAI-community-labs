{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rei_fV8bttWq"
      },
      "source": [
        "# Azure AI Evaluation\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sachin0034/MLAI-community-labs/blob/main/AI-evaluation-course-labs/Lab-3(Building_AI_evaluators)/Advanced_Azure_AI_Evaluation.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "In this lab, we will learn about **Azure AI Evaluation**, which refers to a set of tools and services provided by Microsoft Azure designed to assess and monitor the performance, safety, and quality of artificial intelligence applications—particularly generative AI and machine learning models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD1cPgrIt4Zz"
      },
      "source": [
        "# Get Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70avKV8luGW5"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before you begin, please make sure you have the following ready so your experience in the lab is smooth and successful:\n",
        "\n",
        "\n",
        "> **Note Azure Subscription Requirement:**  \n",
        "> To run this lab and access Azure OpenAI services, you must have an **Azure account with Premium (Pay-as-you-go or Enterprise) subscription**.  \n",
        "> Free-tier or trial accounts **do not** provide access to Azure OpenAI resources.\n",
        "\n",
        "\n",
        "### 1. Sample Contract File for Testing\n",
        "\n",
        "\n",
        "- **Download the sample contract here:**  \n",
        "  [Download Sample Contract (Google Drive)](https://drive.google.com/file/d/1E557kdNBZ5cDUvVDLNrEVRuKcRSYDG3Z/view?usp=sharing)\n",
        "\n",
        "\n",
        "### 2. OpenAI API Key\n",
        "\n",
        "An OpenAI API key is required for accessing the language models used in contract evaluation.\n",
        "\n",
        "- **Don’t have an OpenAI API key? Get one in a few minutes by following this step-by-step guide:**  \n",
        "  [How to get your own OpenAI API key (Medium article)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV7pOTlXvR_i"
      },
      "source": [
        "# Step 1: Install Dependencies and Their Descriptions\n",
        "\n",
        "To begin working with Azure AI Evaluation and related tools, install all required Python packages\n",
        "\n",
        "\n",
        "```| Package Name             | Description                                                                                         |\n",
        "|-------------------------|-----------------------------------------------------------------------------------------------------|\n",
        "| **langchain**           | Framework to build applications powered by language models, helping with prompt management and chaining AI responses.\n",
        "| **pypdf**               | Library to read and manipulate PDF files, useful for processing contract documents in PDF format.     \n",
        "| **docx2txt**            | Simple tool to extract text from Microsoft Word (.docx) files, enabling text extraction from contracts.\n",
        "| **pandas**              | Data analysis and manipulation library, useful for handling structured evaluation results and datasets.\n",
        "| **openai**              | Python client for OpenAI API, allowing access to OpenAI language models for text generation and evaluation.\n",
        "| **gradio**              | Easy-to-use library to build interactive UIs for machine learning demos, helpful for building evaluation interfaces.\n",
        "| **azure-ai-generative** | Azure SDK to use Azure’s generative AI capabilities including chat and text generation.                 \n",
        "| **langchain-community** | Community-maintained extensions and tools for LangChain to support additional AI workflow features.   \n",
        "| **azure-ai-evaluation** | Azure AI Evaluation SDK providing built-in evaluators to measure AI-generated content quality and safety.\n",
        "| **azure-ai-projects**   | SDK to manage Azure AI Foundry projects and execute evaluations, enabling integration with Azure's AI management tools.\n",
        "| **semantic-kernel**     | Framework for building AI apps with semantic memory and prompt orchestration, enhancing complex AI workflows.\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Installing these packages equips your Azure AI contract evaluation environment with capabilities for document processing, language model access, interactive interfaces, and quality evaluation tools.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p2iyXfvAjAZH",
        "outputId": "138a9ac3-8f7c-4ead-d714-18537b1d2e9c"
      },
      "outputs": [],
      "source": [
        "# Install Dependencies\n",
        "! pip install langchain pypdf docx2txt pandas openai gradio azure-ai-generative langchain-community azure-ai-evaluation azure-ai-projects semantic-kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFe_KJjivxlO"
      },
      "source": [
        "# Step 2 : Imports and Configuration\n",
        "\n",
        "This code cell sets up the necessary imports and configurations for processing contract documents, interacting with OpenAI models, and using Azure AI Evaluation tools.\n",
        "\n",
        "```\n",
        "| Import / Library                  | Purpose                                                                                       |\n",
        "|---------------------------------|------------------------------------------------------------------------------------------------|\n",
        "| `os`, `io`, `tempfile`, `re`, `json`, `time` | Standard Python libraries for file handling, string processing, JSON manipulation, and timing.\n",
        "| `PyPDFLoader`, `Docx2txtLoader`, `TextLoader` | LangChain loaders to read and extract text from PDF, DOCX, and text files.                    \n",
        "| `RecursiveCharacterTextSplitter` | Splits large documents into smaller chunks for better processing by AI models.                 \n",
        "| `Document`                      | LangChain object representing textual documents with associated metadata.                      \n",
        "| `pandas`                       | For organizing data and evaluation results in tabular format.                                 \n",
        "| `gradio`                       | To build interactive user interfaces and demos.                                              \n",
        "| `OpenAI` and error classes     | OpenAI Python SDK for making API calls and handling errors like rate limits or connection issues.\n",
        "| `GroundednessEvaluator`, `CoherenceEvaluator`, `RelevanceEvaluator`, `FluencyEvaluator` | Azure AI Evaluation SDK classes to measure AI-generated content’s quality on various dimensions.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "This setup prepares the environment to load contract documents, process them, generate AI responses, and evaluate those responses using Azure AI Evaluation’s built-in metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sVm26qb5jJma"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import tempfile\n",
        "import re\n",
        "import json\n",
        "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pandas as pd\n",
        "from langchain.schema import Document\n",
        "from openai import OpenAI\n",
        "from openai import APIError, APIConnectionError, RateLimitError\n",
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "# Azure AI Evaluation imports\n",
        "from azure.ai.evaluation import GroundednessEvaluator, CoherenceEvaluator, RelevanceEvaluator, FluencyEvaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iALEswltyjmZ"
      },
      "source": [
        "## Step 3 : OpenAI and Azure AI Configuration\n",
        "\n",
        "### OpenAI Client Initialization\n",
        "\n",
        "> 1. **OpenAI API Key**  \n",
        ">    To use the OpenAI client, you need an API key.  \n",
        ">    👉 Follow this guide to get your API key:  \n",
        ">    [How to get your own OpenAI API key (Medium)](https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
        "\n",
        "---\n",
        "\n",
        "### Azure AI Project Configuration\n",
        "\n",
        "The `azure_ai_project` dictionary holds metadata for identifying and accessing a specific Azure AI project. This includes:\n",
        "\n",
        "- `subscription_id`: The unique identifier of your Azure subscription.\n",
        "- `resource_group_name`: The name of the resource group that contains your Azure resources.\n",
        "- `project_name`: The name of the Azure AI project or workspace.\n",
        "\n",
        "> **To configure the Azure AI project:**  \n",
        "> 👉 Use this official Microsoft guide to create an Azure AI project:  \n",
        "> [Create Azure AI Projects (Microsoft Docs)](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/create-projects?tabs=ai-foundry&pivots=fdp-project)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Azure OpenAI Model Configuration\n",
        "\n",
        "The `model_config` dictionary contains connection details required to interact with an Azure-hosted OpenAI deployment.\n",
        "\n",
        "- `azure_endpoint`: The base URL of the Azure OpenAI resource.\n",
        "- `api_key`: The API key for authentication against the Azure OpenAI endpoint.\n",
        "- `azure_deployment`: The specific deployment name of the OpenAI model within Azure.\n",
        "- `api_version`: The version of the Azure OpenAI API to be used (e.g., `2024-02-15-preview`).\n",
        "\n",
        "> **To configure the Azure OpenAI model deployment:**  \n",
        "> 👉 Follow this official Microsoft guide to create and deploy an Azure OpenAI resource:  \n",
        "> [Create and Deploy Azure OpenAI Resources (Microsoft Docs)](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/create-resource?pivots=web-portal)\n",
        "\n",
        "---\n",
        "\n",
        "> ❗ **Important:**  \n",
        "> You **must** have both your **OpenAI API key** and your **Azure AI project + model configuration** completed.  \n",
        "> Without these keys and setup, the application **will not run** and you **cannot proceed** further.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALjteZupjOBD"
      },
      "outputs": [],
      "source": [
        "# Initialize OpenAI client (replace with your own API key securely)\n",
        "# Follow this link to get your api key : https://medium.com/@lorenzozar/how-to-get-your-own-openai-api-key-f4d44e60c327)\n",
        "client = OpenAI(api_key='YOUR_OPENAI_API_KEY')\n",
        "\n",
        "\n",
        "# Initialize Azure AI project configuration (replace placeholders with your own values)\n",
        "# Follow this link create a azure ai project  : https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/create-projects?tabs=ai-foundry&pivots=fdp-project\n",
        "azure_ai_project = {\n",
        "    \"subscription_id\": \"your-subscription-id\",\n",
        "    \"resource_group_name\": \"your-resource-group-name\",\n",
        "    \"project_name\": \"your-project-name\",\n",
        "}\n",
        "\n",
        "\n",
        "# Azure OpenAI model configuration (replace placeholders with your own values)\n",
        "# Follow this to create and deploy an azure open ai project : https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/create-resource?pivots=web-portal\n",
        "model_config = {\n",
        "    \"azure_endpoint\": \"your-azure-endpoint\",\n",
        "    \"api_key\": \"YOUR_AZURE_API_KEY\",\n",
        "    \"azure_deployment\": \"your-deployment-name\",\n",
        "    \"api_version\": \"your-api-version\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR0ZtpwBy3IQ"
      },
      "source": [
        "### Step 4 : Azure Evaluator Initialization\n",
        "\n",
        "Initializes multiple evaluation modules from Azure AI to assess different quality aspects of text generation (e.g., model outputs or document parsing).\n",
        "\n",
        "- **`GroundednessEvaluator`**: Measures how well the generated content is grounded in the source material.\n",
        "- **`CoherenceEvaluator`**: Evaluates logical flow and consistency between sentences.\n",
        "- **`RelevanceEvaluator`**: Checks how relevant the output is to the input or expected context.\n",
        "- **`FluencyEvaluator`**: Assesses the grammatical and linguistic quality of the content.\n",
        "\n",
        "Each evaluator is configured using the `model_config` dictionary, which must contain Azure OpenAI endpoint, API key, deployment name, and API version.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Key Terms to Extract\n",
        "\n",
        "Defines a list of key legal or contractual terms to identify and extract from the contract.\n",
        "\n",
        "- **`\"Product Name\"`**: The name of the product being described or licensed.\n",
        "- **`\"Limitation of Liability In Months\"`**: The time limit for legal liability in the agreement.\n",
        "- **`\"Governing Law\"`**: Specifies which jurisdiction’s laws apply to the contract."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2mCv9tgAjQkm"
      },
      "outputs": [],
      "source": [
        "# Initialize Azure evaluators\n",
        "groundedness_eval = GroundednessEvaluator(model_config=model_config)\n",
        "coherence_eval = CoherenceEvaluator(model_config=model_config)\n",
        "relevance_eval = RelevanceEvaluator(model_config=model_config)\n",
        "fluency_eval = FluencyEvaluator(model_config=model_config)\n",
        "\n",
        "# Key terms to extract\n",
        "KEY_TERMS = [\n",
        "    \"Product Name\",\n",
        "    \"Limitation of Liability In Months\",\n",
        "    \"Governing Law\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UcGbxHzzQ6a"
      },
      "source": [
        "## Step 5  : File Text Extraction Helper\n",
        "\n",
        "### Function: `extract_text_from_file(file_path)`\n",
        "\n",
        "This helper function handles the extraction of text from various file types, converting them into a consistent structure for downstream processing (e.g., with LangChain or LLM pipelines).\n",
        "\n",
        "#### Parameters\n",
        "\n",
        "- **`file_path`** *(str)*: Path to the input file whose content needs to be extracted.\n",
        "\n",
        "#### Returns\n",
        "\n",
        "- **`text`** *(str)*: Full extracted text from the file, as a single string.\n",
        "- **`docs`** *(list)*: A list of document-like objects, each having a `page_content` attribute for structured processing.\n",
        "\n",
        "---\n",
        "\n",
        "### Supported File Types\n",
        "\n",
        "| File Type | Loader Used         | Description                              |\n",
        "|-----------|---------------------|------------------------------------------|\n",
        "| `.pdf`    | `PyPDFLoader`       | Extracts page-wise text from PDF         |\n",
        "| `.docx`, `.doc` | `Docx2txtLoader` | Reads content from Word documents        |\n",
        "| `.txt`    | `TextLoader`        | Loads plain text from text files         |\n",
        "| `.csv`    | `pandas.read_csv`   | Converts CSV rows into a text string     |\n",
        "\n",
        "\n",
        "\n",
        "> ⚠️ **Warning:** If the file type is not recognized, the function raises a `ValueError`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DAyoPNnajS0s"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_file(file_path):\n",
        "    # Get the file extension and convert it to lowercase\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    # Handle PDF files\n",
        "    if ext == \".pdf\":\n",
        "        loader = PyPDFLoader(file_path)  # Use PyPDFLoader to read PDF\n",
        "        docs = loader.load()  # Load document into LangChain Document objects\n",
        "        text = \"\\n\".join([doc.page_content for doc in docs])  # Combine all page content\n",
        "\n",
        "    # Handle Word documents (.docx, .doc)\n",
        "    elif ext in [\".docx\", \".doc\"]:\n",
        "        loader = Docx2txtLoader(file_path)  # Use Docx2txtLoader for Word files\n",
        "        docs = loader.load()\n",
        "        text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Handle plain text files\n",
        "    elif ext in [\".txt\"]:\n",
        "        loader = TextLoader(file_path)  # Use TextLoader for .txt files\n",
        "        docs = loader.load()\n",
        "        text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Handle CSV files\n",
        "    elif ext == \".csv\":\n",
        "        df = pd.read_csv(file_path)  # Read CSV using pandas\n",
        "        text = df.to_string(index=False)  # Convert DataFrame to plain string\n",
        "        # Wrap in a dummy doc-like object to keep consistent structure\n",
        "        docs = [type('Doc', (object,), {'page_content': text})()]\n",
        "\n",
        "    # Unsupported file types\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type\")\n",
        "\n",
        "    # Return both raw text and structured docs for further processing\n",
        "    return text, docs  # docs may include page-level details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KEWSlk40gDe"
      },
      "source": [
        "## Step 6 : Key Term Extraction with LLM and JSON Parsing\n",
        "\n",
        "### Function: `extract_key_terms(text, key_terms)`\n",
        "\n",
        "This function extracts specific legal or contractual terms from a text document using a large language model (LLM). It prompts the LLM to return structured JSON responses and parses them with resilience against formatting issues.\n",
        "\n",
        "---\n",
        "\n",
        "### 📥 Parameters\n",
        "\n",
        "- **`text`** *(str)*: Full contract or document text to be analyzed.\n",
        "- **`key_terms`** *(list of str)*: List of term names (e.g., `\"Governing Law\"`) to extract from the document.\n",
        "\n",
        "---\n",
        "\n",
        "### 📤 Returns\n",
        "\n",
        "- **`results`** *(dict)*: Dictionary mapping each key term to a dictionary containing:\n",
        "  - `\"Value\"`: The extracted value or `\"Not found\"`.\n",
        "  - `\"page_number\"`: The page number where the term was found (if available).\n",
        "\n",
        "Example:\n",
        "```json\n",
        "{\n",
        "  \"Governing Law\": {\n",
        "    \"Value\": \"California\",\n",
        "    \"page_number\": \"5\"\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hUWx97BPjXAt"
      },
      "outputs": [],
      "source": [
        "def extract_key_terms(text, key_terms):\n",
        "\n",
        "    def safe_json_parse(response_text):\n",
        "        \"\"\"Safely parse JSON from LLM response with fallback strategies.\"\"\"\n",
        "        # Strategy 1: Try direct JSON parsing\n",
        "        try:\n",
        "            return json.loads(response_text.strip())\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "        # Strategy 2: Extract from code blocks\n",
        "        json_patterns = [\n",
        "            r'```json\\s*(.*?)\\s*```',\n",
        "            r'```\\s*(.*?)\\s*```',\n",
        "            r'\\{.*\\}'\n",
        "        ]\n",
        "\n",
        "        for pattern in json_patterns:\n",
        "            json_match = re.search(pattern, response_text, re.DOTALL)\n",
        "            if json_match:\n",
        "                try:\n",
        "                    return json.loads(json_match.group(1).strip() if 'json' in pattern else json_match.group(0))\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "        # Fallback: return default structure\n",
        "        return {\"Value\": \"Not found\", \"Page Number\": None, \"Section\": None}\n",
        "\n",
        "    results = {}\n",
        "    for term in key_terms:\n",
        "        prompt = (\n",
        "            f\"Act as a legal expert. From this contract text, extract the value for '{term}'.\\n\\n\"\n",
        "            f\"Contract Text: {text}\\n\\n\"\n",
        "            f\"Instructions:\\n\"\n",
        "            f\"1. Provide a one-word answer for '{term}' if found\\n\"\n",
        "            f\"2. Include page number if available\\n\"\n",
        "            f\"3. If not found, use 'Not found'\\n\\n\"\n",
        "            f\"Return ONLY valid JSON in this exact format:\\n\"\n",
        "            f'{{\"Value\": \"your_answer\", \"Page Number\": \"page_number_or_null\", \"Section\": \"section_name\"}}'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",  # Fixed model name\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a legal contract analysis assistant. Always return valid JSON only.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.1  # Lower temperature for consistency\n",
        "            )\n",
        "            answer = completion.choices[0].message.content\n",
        "            print(\"****************LLM Answer*******************\")\n",
        "            print(answer)\n",
        "            print(\"*********************************************\")\n",
        "\n",
        "            # Use safe JSON parsing\n",
        "            parsed_response = safe_json_parse(answer)\n",
        "            print(f\"DEBUG: Parsed JSON successfully: {parsed_response}\")\n",
        "\n",
        "            value = parsed_response.get(\"Value\", \"Not found\")\n",
        "            print(f\"DEBUG: Extracted value: {value}\")\n",
        "\n",
        "            # Extract page number with multiple fallback options\n",
        "            page_number = (\n",
        "                parsed_response.get(\"Page Number\") or\n",
        "                parsed_response.get(\"PageNumber\") or\n",
        "                parsed_response.get(\"page_number\")\n",
        "            )\n",
        "            print(f\"DEBUG: Direct page number extraction: {page_number}\")\n",
        "\n",
        "            # If not found, try extracting from Section field\n",
        "            if not page_number:\n",
        "                section = parsed_response.get(\"Section\", \"\")\n",
        "                print(f\"DEBUG: Section field content: {section}\")\n",
        "                if section and \"Page\" in str(section):\n",
        "                    page_match = re.search(r'Page (\\d+)', str(section))\n",
        "                    if page_match:\n",
        "                        page_number = page_match.group(1)\n",
        "                        print(f\"DEBUG: Extracted page number from section: {page_number}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: API call or parsing failed: {e}\")\n",
        "            value = \"Error\"\n",
        "            page_number = None\n",
        "\n",
        "        final_result = {\"Value\": value, \"page_number\": page_number}\n",
        "        print(f\"DEBUG: Final result for term '{term}': {final_result}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        results[term] = final_result\n",
        "\n",
        "    print(f\"DEBUG: All results: {results}\")\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjJj6G3h03BQ"
      },
      "source": [
        "## Step 7 : LLM Response Evaluation with Azure AI\n",
        "\n",
        "### Function: `azure_judge(entry, results_df)`\n",
        "\n",
        "This function evaluates a single LLM-generated response across four evaluation — **groundedness**, **coherence**, **relevance**, and **fluency** — using Azure-based evaluators. It returns an updated results DataFrame with the new evaluation appended.\n",
        "\n",
        "---\n",
        "\n",
        "### 📥 Parameters\n",
        "\n",
        "- **`entry`** *(dict)*: A dictionary containing the following fields:\n",
        "  - `\"Key Term Name\"`: Name of the key term being evaluated.\n",
        "  - `\"context\"`: Background or reference text used for grounding.\n",
        "  - `\"query\"`: The question or prompt issued to the LLM.\n",
        "  - `\"llm_response\"`: The LLM-generated output to be evaluated.\n",
        "\n",
        "- **`results_df`** *(pd.DataFrame)*: The existing DataFrame to which the evaluation result will be appended.\n",
        "\n",
        "---\n",
        "\n",
        "### 📤 Returns\n",
        "\n",
        "- **`pd.DataFrame`**: An updated DataFrame with a new row containing:\n",
        "  - Evaluation scores from all four evaluators\n",
        "  - The query, context, response, and key term name\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PsFL3E6djZaF"
      },
      "outputs": [],
      "source": [
        "def azure_judge(entry, results_df):\n",
        "    \"\"\"Evaluate a single entry using all evaluators and add to results dataframe\"\"\"\n",
        "\n",
        "    # Format inputs for each evaluator - using llm_response instead of ground_truth\n",
        "    groundedness_input = {\n",
        "        \"query\": entry[\"query\"],\n",
        "        \"context\": entry[\"context\"],\n",
        "        \"response\": entry[\"llm_response\"]  # Changed from ground_truth to llm_response\n",
        "    }\n",
        "\n",
        "    coherence_input = {\n",
        "        \"query\": entry[\"query\"],\n",
        "        \"response\": entry[\"llm_response\"]  # Changed from ground_truth to llm_response\n",
        "    }\n",
        "\n",
        "    relevance_input = {\n",
        "        \"query\": entry[\"query\"],\n",
        "        \"response\": entry[\"llm_response\"]  # Changed from ground_truth to llm_response\n",
        "    }\n",
        "\n",
        "    fluency_input = {\n",
        "        \"response\": entry[\"llm_response\"]  # Changed from ground_truth to llm_response\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Get all scores\n",
        "        groundedness_score = groundedness_eval(**groundedness_input)\n",
        "        coherence_score = coherence_eval(**coherence_input)\n",
        "        relevance_score = relevance_eval(**relevance_input)\n",
        "        fluency_score = fluency_eval(**fluency_input)\n",
        "\n",
        "        # Create new row for the dataframe\n",
        "        new_row = {\n",
        "            'Key Term Name': entry['Key Term Name'], # Ensure Key Term Name is passed through\n",
        "            'Context': entry['context'],\n",
        "            'Query': entry['query'],\n",
        "            'LLM Response': entry['llm_response'],\n",
        "            'Groundedness Score': groundedness_score['groundedness'],\n",
        "            'Coherence Score': coherence_score['coherence'],\n",
        "            'Relevance Score': relevance_score['relevance'],\n",
        "            'Fluency Score': fluency_score['fluency']\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating entry: {e}\")\n",
        "        # Create row with error indicators\n",
        "        new_row = {\n",
        "            'Key Term Name': entry['Key Term Name'],\n",
        "            'Context': entry['context'],\n",
        "            'Query': entry['query'],\n",
        "            'LLM Response': entry['llm_response'],\n",
        "            'Groundedness Score': None,\n",
        "            'Coherence Score': None,\n",
        "            'Relevance Score': None,\n",
        "            'Fluency Score': None\n",
        "        }\n",
        "\n",
        "    # Use concat with a pre-defined DataFrame\n",
        "    new_row_df = pd.DataFrame([new_row])\n",
        "    return pd.concat([results_df, new_row_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTsd55ne1Syk"
      },
      "source": [
        "## Step 8 : Main Document Processing Pipeline\n",
        "\n",
        "### Function: `process_document(file)`\n",
        "\n",
        "This core function orchestrates the end-to-end workflow for document ingestion, key term extraction, and quality evaluation of AI-generated outputs using Azure Large Language Model (LLM) evaluators. It is designed to process contractual or related documents, extracting relevant terms and assessing the reliability and quality of the extracted data.\n",
        "\n",
        "---\n",
        "\n",
        "### Parameters\n",
        "\n",
        "- **`file`** *(UploadedFile or equivalent)*:  \n",
        "  The input file to be processed. Supported formats include `.pdf`, `.docx`, `.txt`, and `.csv`.\n",
        "\n",
        "---\n",
        "\n",
        "### Returns\n",
        "\n",
        "A tuple consisting of:\n",
        "\n",
        "1. **Status Message** *(str)* — Indicates success or explains errors encountered during processing.  \n",
        "2. **`extracted_df`** *(pandas.DataFrame)* — A structured table containing extracted key contractual terms, their detected values, and associated page numbers or locations within the document.  \n",
        "3. **`evaluation_summary`** *(pandas.DataFrame)* — A summary table containing quality scores generated by Azure LLM evaluators for each extracted term, measuring attributes such as groundedness, coherence, relevance, and fluency.\n",
        "\n",
        "---\n",
        "\n",
        "### Workflow Overview\n",
        "\n",
        "#### 1. File Validation  \n",
        "The function initially verifies that a valid file has been provided. If no file is detected, it terminates early with a user-friendly prompt requesting file upload.\n",
        "\n",
        "#### 2. Document Text Extraction  \n",
        "The input document is processed using the `extract_text_from_file()` utility, which supports multiple file formats. This step yields:  \n",
        "- A single concatenated string representing the full document text.  \n",
        "- A structured list of LangChain-compatible document objects (`docs`) for downstream NLP processing.\n",
        "\n",
        "> **Supported formats:** PDF, Microsoft Word (.docx), plain text (.txt), and CSV files.\n",
        "\n",
        "#### 3. Key Term Extraction  \n",
        "Utilizing prompt-based Large Language Model (LLM) techniques, the function `extract_key_terms()` is called to accurately identify and extract relevant contractual terms and their corresponding values from the processed text. The output is a dictionary mapping key terms to their detected values along with metadata such as page numbers where the terms were found.\n",
        "\n",
        "> **Example key contractual terms:** `\"Governing Law\"`, `\"Product Name\"`, `\"Termination Clause\"`.\n",
        "\n",
        "#### 4. Azure AI Evaluation  \n",
        "Each extracted key term is then evaluated for quality and reliability:  \n",
        "- An evaluation context is prepared combining the original query, relevant document context, and the LLM’s extracted value.  \n",
        "- The `azure_judge()` function invokes Azure AI evaluators to assess each response across multiple dimensions:  \n",
        "  - **Groundedness:** Degree to which the output is supported by source content.  \n",
        "  - **Coherence:** Logical consistency and clarity of the extracted response.  \n",
        "  - **Relevance:** Pertinence of the response to the specific query or clause.  \n",
        "  - **Fluency:** Language quality, readability, and grammatical correctness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zLBu8zrljj9Y"
      },
      "outputs": [],
      "source": [
        "# Main Processing Function\n",
        "\n",
        "def process_document(file):\n",
        "    \"\"\"Main function to process document and return results\"\"\"\n",
        "\n",
        "    if file is None:\n",
        "        return \"Please upload a document first.\", None, None\n",
        "\n",
        "    try:\n",
        "        # Step 2: Extract text from document\n",
        "        print(\"Step 2: Extracting text from document...\")\n",
        "        document_text, docs = extract_text_from_file(file.name)\n",
        "\n",
        "        # Step 3: Extract key terms using LLM\n",
        "        print(\"Step 3: Extracting key terms...\")\n",
        "        key_term_results = extract_key_terms(document_text, KEY_TERMS)\n",
        "\n",
        "        # Step 4: Prepare data for Azure evaluation\n",
        "        print(\"Step 4: Preparing for Azure evaluation...\")\n",
        "        results_df = pd.DataFrame()\n",
        "\n",
        "        extracted_info = []\n",
        "\n",
        "        for term, result in key_term_results.items():\n",
        "            # Prepare entry for azure evaluation\n",
        "            entry = {\n",
        "                'Key Term Name': term,\n",
        "                'context': document_text[:2000],  # Limit context for API\n",
        "                'query': f\"Extract the {term} from this contract\",\n",
        "                'llm_response': result['Value']\n",
        "            }\n",
        "\n",
        "            # Add to extracted info for display\n",
        "            extracted_info.append({\n",
        "                'Key Term': term,\n",
        "                'Extracted Value': result['Value'],\n",
        "                'Page Number': result['page_number'] if result['page_number'] else 'N/A'\n",
        "            })\n",
        "\n",
        "            # Evaluate with Azure\n",
        "            print(f\"Evaluating {term}...\")\n",
        "            results_df = azure_judge(entry, results_df)\n",
        "\n",
        "        # Create extracted info dataframe for display\n",
        "        extracted_df = pd.DataFrame(extracted_info)\n",
        "\n",
        "        # Create evaluation summary\n",
        "        if not results_df.empty:\n",
        "            evaluation_summary = results_df[['Key Term Name', 'LLM Response', 'Groundedness Score',\n",
        "                                           'Coherence Score', 'Relevance Score', 'Fluency Score']].copy()\n",
        "        else:\n",
        "            evaluation_summary = pd.DataFrame()\n",
        "\n",
        "        return (\n",
        "            f\"✅ Document processed successfully!\\n\\nExtracted {len(key_term_results)} key terms.\",\n",
        "            extracted_df,\n",
        "            evaluation_summary\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error processing document: {str(e)}\", None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqoxZXFY2CYy"
      },
      "source": [
        "## Step 9 : Dashboard Visualization of LLM Evaluation Scores\n",
        "\n",
        "### Function: `create_dashboard(evaluation_df)`\n",
        "\n",
        "This function generates a comprehensive three-panel dashboard using Matplotlib and Seaborn to visualize evaluation metrics of Large Language Model (LLM) outputs. It presents key quality attributes such as groundedness, coherence, relevance, and fluency across extracted terms from document analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### Parameters\n",
        "\n",
        "- **`evaluation_df`** *(pandas.DataFrame)*:  \n",
        "  A DataFrame containing LLM evaluation scores, typically produced by the `process_document()` pipeline. It should include columns representing different quality metrics aligned with specific key terms.\n",
        "\n",
        "---\n",
        "\n",
        "### Returns\n",
        "\n",
        "- **`fig`** *(matplotlib.figure.Figure)*:  \n",
        "  A Matplotlib Figure object that encapsulates the dashboard's visualizations. This figure can be rendered directly in various Python UI frameworks like Streamlit or saved as an image file for reporting purposes.\n",
        "\n",
        "---\n",
        "\n",
        "### Visualization Panels\n",
        "\n",
        "#### 1. Average Evaluation Scores (Bar Chart)  \n",
        "Displays the overall average scores for the four core evaluation metrics (groundedness, coherence, relevance, fluency) aggregated across all key terms and documents.  \n",
        "> **Note:** Scores are expected to range from 0 (lowest) to 5 (highest).\n",
        "\n",
        "#### 2. Performance by Key Term (Bar Chart)  \n",
        "Aggregates and depicts scores per extracted key term, allowing users to identify specific contract clauses or topics where the LLM's performance is stronger or weaker.\n",
        "\n",
        "#### 3. Overall Performance Gauge  \n",
        "A semi-circular gauge chart illustrating the mean evaluation score across all terms and metrics.  \n",
        "- Uses categorical color coding to visually differentiate performance levels:  \n",
        "  - 🟢 Excellent (4.0 – 5.0)  \n",
        "  - 🟠 Good (3.0 – 4.0)  \n",
        "  - 🔴 Fair (2.0 – 3.0)  \n",
        "  - ⚪ Poor (< 2.0)\n",
        "\n",
        "---\n",
        "\n",
        "### Internal Logic and Features\n",
        "\n",
        "- **Data Preparation:**  \n",
        "  - Converts evaluation score columns to numeric format.  \n",
        "  - Filters out records with all missing (`NaN`) values to maintain plot accuracy.\n",
        "\n",
        "- **Styling and Palettes:**  \n",
        "  - Employs Seaborn's `\"husl\"` color palette to ensure vibrant, consistent aesthetics.  \n",
        "  - Custom colors are assigned for the gauge’s performance segments.\n",
        "\n",
        "- **Annotations and Responsiveness:**  \n",
        "  - Bar charts include value labels directly on bars for readability.  \n",
        "  - Axes and tick labels are styled for clear visualization even with multiple terms.\n",
        "\n",
        "- **Gauge Plot Implementation:**  \n",
        "  - Uses polar coordinates to simulate a semi-circular gauge.  \n",
        "  - Divides the gauge into colored bands representing performance categories.  \n",
        "  - A needle indicates the computed mean score dynamically.\n",
        "\n",
        "- **Fallback Behavior:**  \n",
        "  - In the absence of valid data, the function returns a blank figure with a user-friendly message indicating no data is available to plot.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QQ4Ab4g6jnxv"
      },
      "outputs": [],
      "source": [
        "# Dashboard Visualization Function\n",
        "\n",
        "def create_dashboard(evaluation_df):\n",
        "    \"\"\"Create dashboard visualization of evaluation scores\"\"\"\n",
        "\n",
        "    if evaluation_df is None or evaluation_df.empty:\n",
        "        return None\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    # Set style\n",
        "    plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "\n",
        "    # Create figure with subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    fig.suptitle('Document Evaluation Dashboard', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Score columns\n",
        "    score_columns = ['Groundedness Score', 'Coherence Score', 'Relevance Score', 'Fluency Score']\n",
        "\n",
        "    # Filter out None values and convert to numeric\n",
        "    plot_data = evaluation_df.copy()\n",
        "    for col in score_columns:\n",
        "        plot_data[col] = pd.to_numeric(plot_data[col], errors='coerce')\n",
        "\n",
        "    # Remove rows with all NaN scores\n",
        "    plot_data = plot_data.dropna(subset=score_columns, how='all')\n",
        "\n",
        "    if plot_data.empty:\n",
        "        # If no valid data, show message\n",
        "        fig.text(0.5, 0.5, 'No valid evaluation scores available',\n",
        "                ha='center', va='center', fontsize=20)\n",
        "        return fig\n",
        "\n",
        "    # 1. Individual Scores Bar Chart\n",
        "    ax1 = axes[0]\n",
        "    scores_avg = plot_data[score_columns].mean()\n",
        "    bars1 = ax1.bar(range(len(scores_avg)), scores_avg.values,\n",
        "                    color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
        "    ax1.set_xlabel('Evaluation Metrics')\n",
        "    ax1.set_ylabel('Average Score')\n",
        "    ax1.set_title('Average Evaluation Scores')\n",
        "    ax1.set_xticks(range(len(scores_avg)))\n",
        "    ax1.set_xticklabels([col.replace(' Score', '') for col in scores_avg.index], rotation=45)\n",
        "    ax1.set_ylim(0, 5)  # Assuming scores are 0-5\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars1, scores_avg.values):\n",
        "        height = bar.get_height()\n",
        "        if not pd.isna(height):\n",
        "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
        "                    f'{height:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    # 2. Key Terms Performance\n",
        "    ax2 = axes[1]\n",
        "    if len(plot_data) > 0:\n",
        "        term_scores = plot_data.groupby('Key Term Name')[score_columns].mean().mean(axis=1)\n",
        "        bars2 = ax2.bar(range(len(term_scores)), term_scores.values,\n",
        "                       color=['#FFD93D', '#6BCF7F', '#4D96FF'])\n",
        "        ax2.set_xlabel('Key Terms')\n",
        "        ax2.set_ylabel('Average Score')\n",
        "        ax2.set_title('Performance by Key Term')\n",
        "        ax2.set_xticks(range(len(term_scores)))\n",
        "        ax2.set_xticklabels(term_scores.index, rotation=45, ha='right')\n",
        "        ax2.set_ylim(0, 5)\n",
        "\n",
        "        # Add value labels\n",
        "        for bar, value in zip(bars2, term_scores.values):\n",
        "            height = bar.get_height()\n",
        "            if not pd.isna(height):\n",
        "                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
        "                        f'{height:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    # 3. Overall Performance Gauge\n",
        "    ax3 = axes[2]\n",
        "    overall_score = plot_data[score_columns].mean().mean()\n",
        "\n",
        "    # Create a simple gauge chart\n",
        "    theta = np.linspace(0, np.pi, 100)\n",
        "    r = np.ones_like(theta)\n",
        "\n",
        "    # Color based on score\n",
        "    if pd.notna(overall_score):\n",
        "        if overall_score >= 4:\n",
        "            color = '#2ECC71'  # Green\n",
        "            status = 'Excellent'\n",
        "        elif overall_score >= 3:\n",
        "            color = '#F39C12'  # Orange\n",
        "            status = 'Good'\n",
        "        elif overall_score >= 2:\n",
        "            color = '#E74C3C'  # Red\n",
        "            status = 'Fair'\n",
        "        else:\n",
        "            color = '#95A5A6'  # Gray\n",
        "            status = 'Poor'\n",
        "\n",
        "        # Plot gauge\n",
        "        ax3.fill_between(theta, 0, r, alpha=0.3, color='lightgray')\n",
        "        gauge_theta = np.linspace(0, np.pi * (overall_score/5), 50)\n",
        "        gauge_r = np.ones_like(gauge_theta)\n",
        "        ax3.fill_between(gauge_theta, 0, gauge_r, alpha=0.8, color=color)\n",
        "\n",
        "        ax3.set_ylim(0, 1)\n",
        "        ax3.set_xlim(0, np.pi)\n",
        "        ax3.set_title('Overall Performance')\n",
        "        ax3.text(np.pi/2, 0.5, f'{overall_score:.2f}\\n{status}',\n",
        "                ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "        ax3.set_xticks([])\n",
        "        ax3.set_yticks([])\n",
        "        ax3.spines['top'].set_visible(False)\n",
        "        ax3.spines['right'].set_visible(False)\n",
        "        ax3.spines['bottom'].set_visible(False)\n",
        "        ax3.spines['left'].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xZeI90O2bEl"
      },
      "source": [
        "## Step 10 :  Gradio Interface – Legal Document Analyzer\n",
        "\n",
        "### Purpose\n",
        "\n",
        "This section defines the **Gradio user interface (UI)** that enables users to upload legal documents, extract key contractual terms, evaluate their quality using Azure LLM metrics, and visualize the results through an interactive dashboard.\n",
        "\n",
        "---\n",
        "\n",
        "### Function: `gradio_process_wrapper(file)`\n",
        "\n",
        "A wrapper function that connects the Gradio frontend input to the backend processing pipeline.\n",
        "\n",
        "#### Workflow:\n",
        "\n",
        "1. Receives a document upload (`file`), then calls `process_document(file)` to:\n",
        "   - Extract document text and key contractual terms.\n",
        "   - Run quality evaluation metrics on the extracted data.\n",
        "2. If evaluation results are available, invokes `create_dashboard()` to generate a comprehensive visualization of performance.\n",
        "3. Returns the following outputs to the UI:\n",
        "   - Status message indicating success or error.\n",
        "   - Table of extracted key terms with associated values and locations.\n",
        "   - Table of LLM evaluation scores per term.\n",
        "   - Matplotlib figure containing the evaluation dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9i-Br9Ajpag"
      },
      "outputs": [],
      "source": [
        "# Gradio Interface\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def gradio_process_wrapper(file):\n",
        "    \"\"\"Wrapper function for Gradio that processes document and returns dashboard\"\"\"\n",
        "\n",
        "    # Process the document\n",
        "    status, extracted_df, evaluation_df = process_document(file)\n",
        "\n",
        "    # Create dashboard if evaluation data exists\n",
        "    dashboard_plot = None\n",
        "    if evaluation_df is not None and not evaluation_df.empty:\n",
        "        dashboard_plot = create_dashboard(evaluation_df)\n",
        "\n",
        "    return status, extracted_df, evaluation_df, dashboard_plot\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"Legal Document Analyzer\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # 📋 Legal Document Analyzer\n",
        "\n",
        "        Upload a legal document (PDF, DOCX, TXT, CSV) and extract key terms with AI evaluation.\n",
        "\n",
        "        **Key Terms Extracted:**\n",
        "        - Product Name\n",
        "        - Limitation of Liability In Months\n",
        "        - Governing Law\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Upload section\n",
        "            gr.Markdown(\"## 📤 Upload Document\")\n",
        "            file_input = gr.File(\n",
        "                label=\"Choose Document\",\n",
        "                file_types=[\".pdf\", \".docx\", \".doc\", \".txt\", \".csv\"],\n",
        "                type=\"filepath\"\n",
        "            )\n",
        "\n",
        "            process_btn = gr.Button(\n",
        "                \"🚀 Start Evaluating\",\n",
        "                variant=\"primary\",\n",
        "                size=\"lg\"\n",
        "            )\n",
        "\n",
        "            # Status output\n",
        "            status_output = gr.Textbox(\n",
        "                label=\"Status\",\n",
        "                interactive=False,\n",
        "                placeholder=\"Upload a document and click 'Start Evaluating'...\"\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            # Results section\n",
        "            gr.Markdown(\"## 📊 Evaluation Dashboard\")\n",
        "            dashboard_plot = gr.Plot(label=\"Performance Dashboard\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"## 📝 Extracted Information\")\n",
        "            extracted_table = gr.Dataframe(\n",
        "                label=\"Key Terms Extracted\",\n",
        "                interactive=False,\n",
        "                wrap=True\n",
        "            )\n",
        "\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"## 🎯 Evaluation Scores\")\n",
        "            evaluation_table = gr.Dataframe(\n",
        "                label=\"AI Evaluation Results\",\n",
        "                interactive=False,\n",
        "                wrap=True\n",
        "            )\n",
        "\n",
        "    # Event handlers\n",
        "    process_btn.click(\n",
        "        fn=gradio_process_wrapper,\n",
        "        inputs=[file_input],\n",
        "        outputs=[status_output, extracted_table, evaluation_table, dashboard_plot],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    # Example section\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        ## 📋 Evaluation Metrics Explained\n",
        "\n",
        "        - **Groundedness**: How well the response is supported by the context\n",
        "        - **Coherence**: How logical and well-structured the response is\n",
        "        - **Relevance**: How relevant the response is to the query\n",
        "        - **Fluency**: How natural and well-written the response is\n",
        "\n",
        "        *Scores range from 1-5, with 5 being the best.*\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True, share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
